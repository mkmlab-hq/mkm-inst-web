import requests
import json
import time
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import feedparser
import xml.etree.ElementTree as ET

class EnhancedDataCollectors:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def collect_healthcare_news_enhanced(self):
        """í–¥ìƒëœ í—¬ìŠ¤ì¼€ì–´ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        sources = [
            {
                "name": "FierceBiotech",
                "url": "https://www.fiercebiotech.com/",
                "search_url": "https://www.fiercebiotech.com/search?keywords=AI+diagnosis",
                "keywords": ["AI diagnosis", "voice analysis", "facial recognition", "machine learning"]
            },
            {
                "name": "MedCityNews",
                "url": "https://medcitynews.com/",
                "search_url": "https://medcitynews.com/?s=AI+diagnosis",
                "keywords": ["artificial intelligence", "diagnostic tools", "digital health"]
            }
        ]
        
        all_news = []
        
        for source in sources:
            print(f"[í–¥ìƒëœ ë‰´ìŠ¤ ìˆ˜ì§‘] {source['name']} ìˆ˜ì§‘ ì¤‘...")
            
            try:
                # ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘
                response = self.session.get(source['url'], timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                    article_links = []
                    
                    # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                    selectors = [
                        'article a[href*="/"]',
                        '.post-title a',
                        '.entry-title a',
                        'h2 a',
                        'h3 a',
                        '.headline a'
                    ]
                    
                    for selector in selectors:
                        links = soup.select(selector)
                        for link in links[:10]:  # ìµœëŒ€ 10ê°œ
                            href = link.get('href')
                            if href and not href.startswith('#'):
                                if not href.startswith('http'):
                                    href = urljoin(source['url'], href)
                                article_links.append(href)
                    
                    # ì¤‘ë³µ ì œê±°
                    article_links = list(set(article_links))
                    
                    # ê° ê¸°ì‚¬ ìƒì„¸ ìˆ˜ì§‘
                    for link in article_links[:5]:  # ìµœëŒ€ 5ê°œ ê¸°ì‚¬
                        try:
                            article_response = self.session.get(link, timeout=30)
                            if article_response.status_code == 200:
                                article_soup = BeautifulSoup(article_response.content, 'html.parser')
                                
                                # ì œëª© ì¶”ì¶œ
                                title_selectors = ['h1', '.post-title', '.entry-title', '.headline']
                                title = ""
                                for selector in title_selectors:
                                    title_elem = article_soup.select_one(selector)
                                    if title_elem:
                                        title = title_elem.get_text().strip()
                                        break
                                
                                # ë³¸ë¬¸ ì¶”ì¶œ
                                content_selectors = ['.post-content', '.entry-content', '.article-content', 'article']
                                content = ""
                                for selector in content_selectors:
                                    content_elem = article_soup.select_one(selector)
                                    if content_elem:
                                        content = content_elem.get_text().strip()
                                        break
                                
                                # í‚¤ì›Œë“œ í•„í„°ë§
                                full_text = (title + " " + content).lower()
                                if any(keyword.lower() in full_text for keyword in source['keywords']):
                                    news_item = {
                                        "source_type": "news",
                                        "source_name": source['name'],
                                        "title": title,
                                        "content": content[:1000],  # 1000ìë¡œ ì œí•œ
                                        "url": link,
                                        "published_date": datetime.now().strftime("%Y-%m-%d"),
                                        "entities": self.extract_entities(content),
                                        "keywords": source['keywords']
                                    }
                                    all_news.append(news_item)
                                    print(f"  â†’ {title[:50]}...")
                        
                        except Exception as e:
                            print(f"    âš ï¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                            continue
                
                print(f"  {source['name']}: {len([n for n in all_news if n['source_name'] == source['name']])}ê±´ ìˆ˜ì§‘")
                
            except Exception as e:
                print(f"  âš ï¸ {source['name']} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return all_news
    
    def collect_clinical_trials_enhanced(self):
        """í–¥ìƒëœ ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘"""
        keywords = [
            "voice analysis",
            "facial diagnosis", 
            "tongue diagnosis",
            "AI diagnosis",
            "machine learning diagnosis",
            "digital health diagnosis"
        ]
        
        all_trials = []
        
        for keyword in keywords:
            print(f"[í–¥ìƒëœ ì„ìƒì‹œí—˜ ìˆ˜ì§‘] í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # ClinicalTrials.gov API í˜¸ì¶œ
                url = "https://clinicaltrials.gov/api/query/study_fields"
                params = {
                    "expr": keyword,
                    "fields": "NCTId,BriefTitle,OfficialTitle,LeadSponsorName,Phase,Status,StartDate,CompletionDate,EnrollmentCount,StudyType,Condition,InterventionType,InterventionName,Description",
                    "min_rnk": 1,
                    "max_rnk": 20,  # ê° í‚¤ì›Œë“œë‹¹ 20ê°œì”©
                    "fmt": "json"
                }
                
                response = self.session.get(url, params=params, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    studies = data.get('StudyFieldsResponse', {}).get('StudyFields', [])
                    
                    for study in studies:
                        # í•„ìˆ˜ í•„ë“œ í™•ì¸
                        nct_id = study.get('NCTId', [''])[0] if study.get('NCTId') else ''
                        title = study.get('BriefTitle', [''])[0] if study.get('BriefTitle') else ''
                        description = study.get('Description', [''])[0] if study.get('Description') else ''
                        
                        if nct_id and title:  # í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ
                            trial_item = {
                                "source_type": "clinical_trial",
                                "source_name": "ClinicalTrials.gov",
                                "nct_id": nct_id,
                                "title": title,
                                "content": description,
                                "phase": study.get('Phase', [''])[0] if study.get('Phase') else '',
                                "status": study.get('Status', [''])[0] if study.get('Status') else '',
                                "sponsor": study.get('LeadSponsorName', [''])[0] if study.get('LeadSponsorName') else '',
                                "start_date": study.get('StartDate', [''])[0] if study.get('StartDate') else '',
                                "completion_date": study.get('CompletionDate', [''])[0] if study.get('CompletionDate') else '',
                                "enrollment": study.get('EnrollmentCount', [''])[0] if study.get('EnrollmentCount') else '',
                                "condition": study.get('Condition', [''])[0] if study.get('Condition') else '',
                                "intervention": study.get('InterventionName', [''])[0] if study.get('InterventionName') else '',
                                "published_date": study.get('StartDate', [''])[0] if study.get('StartDate') else '',
                                "entities": self.extract_entities(description),
                                "keywords": [keyword],
                                "url": f"https://clinicaltrials.gov/ct2/show/{nct_id}"
                            }
                            all_trials.append(trial_item)
                            print(f"  â†’ {title[:50]}... (Phase: {trial_item['phase']})")
                    
                    print(f"  {keyword}: {len([t for t in all_trials if keyword in t['keywords']])}ê±´ ìˆ˜ì§‘")
                    
            except Exception as e:
                print(f"  âš ï¸ {keyword} ì„ìƒì‹œí—˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return all_trials
    
    def collect_github_projects(self):
        """GitHub ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ ìˆ˜ì§‘"""
        keywords = [
            "voice analysis",
            "facial recognition",
            "tongue diagnosis",
            "AI diagnosis",
            "medical AI",
            "healthcare AI"
        ]
        
        all_projects = []
        
        for keyword in keywords:
            print(f"[GitHub í”„ë¡œì íŠ¸ ìˆ˜ì§‘] í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # GitHub API í˜¸ì¶œ (ê³µê°œ API ì‚¬ìš©)
                url = "https://api.github.com/search/repositories"
                params = {
                    "q": f"{keyword} language:python",
                    "sort": "stars",
                    "order": "desc",
                    "per_page": 10
                }
                
                response = self.session.get(url, params=params, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    repositories = data.get('items', [])
                    
                    for repo in repositories:
                        project_item = {
                            "source_type": "github",
                            "source_name": "GitHub",
                            "title": repo.get('name', ''),
                            "content": repo.get('description', ''),
                            "url": repo.get('html_url', ''),
                            "published_date": repo.get('created_at', ''),
                            "stars": repo.get('stargazers_count', 0),
                            "forks": repo.get('forks_count', 0),
                            "language": repo.get('language', ''),
                            "entities": self.extract_entities(repo.get('description', '')),
                            "keywords": [keyword],
                            "owner": repo.get('owner', {}).get('login', '')
                        }
                        all_projects.append(project_item)
                        print(f"  â†’ {repo.get('name', '')} (â­ {repo.get('stargazers_count', 0)})")
                    
                    print(f"  {keyword}: {len([p for p in all_projects if keyword in p['keywords']])}ê±´ ìˆ˜ì§‘")
                    
            except Exception as e:
                print(f"  âš ï¸ {keyword} GitHub ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return all_projects
    
    def collect_research_papers_enhanced(self):
        """í–¥ìƒëœ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ (arXiv, PubMed ë“±)"""
        keywords = [
            "voice analysis health",
            "facial diagnosis AI",
            "tongue diagnosis deep learning",
            "multimodal medical diagnosis"
        ]
        
        all_papers = []
        
        for keyword in keywords:
            print(f"[í–¥ìƒëœ ë…¼ë¬¸ ìˆ˜ì§‘] í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # arXiv API í˜¸ì¶œ
                url = "http://export.arxiv.org/api/query"
                params = {
                    "search_query": f"all:{keyword}",
                    "start": 0,
                    "max_results": 10,
                    "sortBy": "submittedDate",
                    "sortOrder": "descending"
                }
                
                response = self.session.get(url, params=params, timeout=30)
                if response.status_code == 200:
                    # XML íŒŒì‹±
                    root = ET.fromstring(response.content)
                    
                    for entry in root.findall('.//{http://www.w3.org/2005/Atom}entry'):
                        title_elem = entry.find('.//{http://www.w3.org/2005/Atom}title')
                        summary_elem = entry.find('.//{http://www.w3.org/2005/Atom}summary')
                        published_elem = entry.find('.//{http://www.w3.org/2005/Atom}published')
                        id_elem = entry.find('.//{http://www.w3.org/2005/Atom}id')
                        
                        if title_elem is not None and summary_elem is not None:
                            paper_item = {
                                "source_type": "research_paper",
                                "source_name": "arXiv",
                                "title": title_elem.text.strip(),
                                "content": summary_elem.text.strip(),
                                "url": id_elem.text if id_elem is not None else "",
                                "published_date": published_elem.text[:10] if published_elem is not None else "",
                                "entities": self.extract_entities(summary_elem.text),
                                "keywords": [keyword],
                                "arxiv_id": id_elem.text.split('/')[-1] if id_elem is not None else ""
                            }
                            all_papers.append(paper_item)
                            print(f"  â†’ {title_elem.text[:50]}...")
                    
                    print(f"  {keyword}: {len([p for p in all_papers if keyword in p['keywords']])}ê±´ ìˆ˜ì§‘")
                    
            except Exception as e:
                print(f"  âš ï¸ {keyword} ë…¼ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return all_papers
    
    def extract_entities(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì—”í‹°í‹° ì¶”ì¶œ"""
        if not text:
            return []
        
        entities = []
        
        # ê¸°ì—…ëª… íŒ¨í„´
        company_pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Inc|Corp|LLC|Ltd|Company|Technologies|Systems|Solutions|AI|Health|Medical)\b'
        companies = re.findall(company_pattern, text)
        entities.extend(companies)
        
        # ê¸°ìˆ ëª… íŒ¨í„´
        tech_pattern = r'\b(?:AI|ML|Deep Learning|Machine Learning|Neural Network|Computer Vision|NLP|Voice Analysis|Facial Recognition|Natural Language Processing|Computer Vision|Image Processing)\b'
        tech_terms = re.findall(tech_pattern, text, re.IGNORECASE)
        entities.extend(tech_terms)
        
        # ê¸°ê´€ëª… íŒ¨í„´
        org_pattern = r'\b(?:FDA|NIH|WHO|CDC|Stanford|MIT|Harvard|Google|Microsoft|Apple|Amazon|OpenAI|DeepMind|IBM|Intel)\b'
        orgs = re.findall(org_pattern, text, re.IGNORECASE)
        entities.extend(orgs)
        
        # ì˜í•™ ìš©ì–´ íŒ¨í„´
        medical_pattern = r'\b(?:Diagnosis|Treatment|Clinical|Medical|Healthcare|Patient|Disease|Symptom|Therapy|Medicine|Traditional|TCM|Acupuncture|Herbal)\b'
        medical_terms = re.findall(medical_pattern, text, re.IGNORECASE)
        entities.extend(medical_terms)
        
        return list(set(entities))  # ì¤‘ë³µ ì œê±°

if __name__ == "__main__":
    collectors = EnhancedDataCollectors()
    
    print("ğŸš€ í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # 1. í—¬ìŠ¤ì¼€ì–´ ë‰´ìŠ¤ ìˆ˜ì§‘
    print("\nğŸ“° í—¬ìŠ¤ì¼€ì–´ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    news_data = collectors.collect_healthcare_news_enhanced()
    print(f"   â†’ ì´ {len(news_data)}ê±´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # 2. ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘
    print("\nğŸ¥ ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    trial_data = collectors.collect_clinical_trials_enhanced()
    print(f"   â†’ ì´ {len(trial_data)}ê±´ì˜ ì„ìƒì‹œí—˜ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # 3. GitHub í”„ë¡œì íŠ¸ ìˆ˜ì§‘
    print("\nğŸ’» GitHub í”„ë¡œì íŠ¸ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    github_data = collectors.collect_github_projects()
    print(f"   â†’ ì´ {len(github_data)}ê±´ì˜ GitHub í”„ë¡œì íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # 4. ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘
    print("\nğŸ“š ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    paper_data = collectors.collect_research_papers_enhanced()
    print(f"   â†’ ì´ {len(paper_data)}ê±´ì˜ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # ê²°ê³¼ ì €ì¥
    all_data = {
        "news": news_data,
        "clinical_trials": trial_data,
        "github_projects": github_data,
        "research_papers": paper_data
    }
    
    with open("enhanced_collected_data.json", "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    total_items = sum(len(items) for items in all_data.values())
    print(f"\nâœ… í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"   ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {total_items}ê±´")
    print(f"   ì €ì¥ íŒŒì¼: enhanced_collected_data.json") 