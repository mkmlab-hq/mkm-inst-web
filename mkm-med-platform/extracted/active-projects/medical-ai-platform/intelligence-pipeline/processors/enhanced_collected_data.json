{
  "news": [],
  "clinical_trials": [],
  "github_projects": [
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "funNLP",
      "content": "ä¸­è‹±æ–‡æ•æ„Ÿè¯ã€è¯­è¨€æ£€æµ‹ã€ä¸­å¤–æ‰‹æœº/ç”µè¯å½’å±åœ°/è¿è¥å•†æŸ¥è¯¢ã€åå­—æ¨æ–­æ€§åˆ«ã€æ‰‹æœºå·æŠ½å–ã€èº«ä»½è¯æŠ½å–ã€é‚®ç®±æŠ½å–ã€ä¸­æ—¥æ–‡äººååº“ã€ä¸­æ–‡ç¼©å†™åº“ã€æ‹†å­—è¯å…¸ã€è¯æ±‡æƒ…æ„Ÿå€¼ã€åœç”¨è¯ã€ååŠ¨è¯è¡¨ã€æš´æè¯è¡¨ã€ç¹ç®€ä½“è½¬æ¢ã€è‹±æ–‡æ¨¡æ‹Ÿä¸­æ–‡å‘éŸ³ã€æ±ªå³°æ­Œè¯ç”Ÿæˆå™¨ã€èŒä¸šåç§°è¯åº“ã€åŒä¹‰è¯åº“ã€åä¹‰è¯åº“ã€å¦å®šè¯åº“ã€æ±½è½¦å“ç‰Œè¯åº“ã€æ±½è½¦é›¶ä»¶è¯åº“ã€è¿ç»­è‹±æ–‡åˆ‡å‰²ã€å„ç§ä¸­æ–‡è¯å‘é‡ã€å…¬å¸åå­—å¤§å…¨ã€å¤è¯—è¯åº“ã€ITè¯åº“ã€è´¢ç»è¯åº“ã€æˆè¯­è¯åº“ã€åœ°åè¯åº“ã€å†å²åäººè¯åº“ã€è¯—è¯è¯åº“ã€åŒ»å­¦è¯åº“ã€é¥®é£Ÿè¯åº“ã€æ³•å¾‹è¯åº“ã€æ±½è½¦è¯åº“ã€åŠ¨ç‰©è¯åº“ã€ä¸­æ–‡èŠå¤©è¯­æ–™ã€ä¸­æ–‡è°£è¨€æ•°æ®ã€ç™¾åº¦ä¸­æ–‡é—®ç­”æ•°æ®é›†ã€å¥å­ç›¸ä¼¼åº¦åŒ¹é…ç®—æ³•é›†åˆã€bertèµ„æºã€æ–‡æœ¬ç”Ÿæˆ&æ‘˜è¦ç›¸å…³å·¥å…·ã€cocoNLPä¿¡æ¯æŠ½å–å·¥å…·ã€å›½å†…ç”µè¯å·ç æ­£åˆ™åŒ¹é…ã€æ¸…åå¤§å­¦XLORE:ä¸­è‹±æ–‡è·¨è¯­è¨€ç™¾ç§‘çŸ¥è¯†å›¾è°±ã€æ¸…åå¤§å­¦äººå·¥æ™ºèƒ½æŠ€æœ¯ç³»åˆ—æŠ¥å‘Šã€è‡ªç„¶è¯­è¨€ç”Ÿæˆã€NLUå¤ªéš¾äº†ç³»åˆ—ã€è‡ªåŠ¨å¯¹è”æ•°æ®åŠæœºå™¨äººã€ç”¨æˆ·åé»‘åå•åˆ—è¡¨ã€ç½ªåæ³•åŠ¡åè¯åŠåˆ†ç±»æ¨¡å‹ã€å¾®ä¿¡å…¬ä¼—å·è¯­æ–™ã€cs224næ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†è¯¾ç¨‹ã€ä¸­æ–‡æ‰‹å†™æ±‰å­—è¯†åˆ«ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç† è¯­æ–™/æ•°æ®é›†ã€å˜é‡å‘½åç¥å™¨ã€åˆ†è¯è¯­æ–™åº“+ä»£ç ã€ä»»åŠ¡å‹å¯¹è¯è‹±æ–‡æ•°æ®é›†ã€ASR è¯­éŸ³æ•°æ®é›† + åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€ç¬‘å£°æ£€æµ‹å™¨ã€Microsoftå¤šè¯­è¨€æ•°å­—/å•ä½/å¦‚æ—¥æœŸæ—¶é—´è¯†åˆ«åŒ…ã€ä¸­åæ–°åå­—å…¸æ•°æ®åº“åŠapi(åŒ…æ‹¬å¸¸ç”¨æ­‡åè¯­ã€æˆè¯­ã€è¯è¯­å’Œæ±‰å­—)ã€æ–‡æ¡£å›¾è°±è‡ªåŠ¨ç”Ÿæˆã€SpaCy ä¸­æ–‡æ¨¡å‹ã€Common Voiceè¯­éŸ³è¯†åˆ«æ•°æ®é›†æ–°ç‰ˆã€ç¥ç»ç½‘ç»œå…³ç³»æŠ½å–ã€åŸºäºbertçš„å‘½åå®ä½“è¯†åˆ«ã€å…³é”®è¯(Keyphrase)æŠ½å–åŒ…pkeã€åŸºäºåŒ»ç–—é¢†åŸŸçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿã€åŸºäºä¾å­˜å¥æ³•ä¸è¯­ä¹‰è§’è‰²æ ‡æ³¨çš„äº‹ä»¶ä¸‰å…ƒç»„æŠ½å–ã€ä¾å­˜å¥æ³•åˆ†æ4ä¸‡å¥é«˜è´¨é‡æ ‡æ³¨æ•°æ®ã€cnocrï¼šç”¨æ¥åšä¸­æ–‡OCRçš„Python3åŒ…ã€ä¸­æ–‡äººç‰©å…³ç³»çŸ¥è¯†å›¾è°±é¡¹ç›®ã€ä¸­æ–‡nlpç«èµ›é¡¹ç›®åŠä»£ç æ±‡æ€»ã€ä¸­æ–‡å­—ç¬¦æ•°æ®ã€speech-aligner: ä»â€œäººå£°è¯­éŸ³â€åŠå…¶â€œè¯­è¨€æ–‡æœ¬â€äº§ç”ŸéŸ³ç´ çº§åˆ«æ—¶é—´å¯¹é½æ ‡æ³¨çš„å·¥å…·ã€AmpliGraph: çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ (Python)åº“ï¼šçŸ¥è¯†å›¾è°±æ¦‚å¿µé“¾æ¥é¢„æµ‹ã€Scattertext æ–‡æœ¬å¯è§†åŒ–(python)ã€è¯­è¨€/çŸ¥è¯†è¡¨ç¤ºå·¥å…·ï¼šBERT & ERNIEã€ä¸­æ–‡å¯¹æ¯”è‹±æ–‡è‡ªç„¶è¯­è¨€å¤„ç†NLPçš„åŒºåˆ«ç»¼è¿°ã€Synonymsä¸­æ–‡è¿‘ä¹‰è¯å·¥å…·åŒ…ã€HarvestTexté¢†åŸŸè‡ªé€‚åº”æ–‡æœ¬æŒ–æ˜å·¥å…·ï¼ˆæ–°è¯å‘ç°-æƒ…æ„Ÿåˆ†æ-å®ä½“é“¾æ¥ç­‰ï¼‰ã€word2wordï¼š(Python)æ–¹ä¾¿æ˜“ç”¨çš„å¤šè¯­è¨€è¯-è¯å¯¹é›†ï¼š62ç§è¯­è¨€/3,564ä¸ªå¤šè¯­è¨€å¯¹ã€è¯­éŸ³è¯†åˆ«è¯­æ–™ç”Ÿæˆå·¥å…·ï¼šä»å…·æœ‰éŸ³é¢‘/å­—å¹•çš„åœ¨çº¿è§†é¢‘åˆ›å»ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)è¯­æ–™åº“ã€æ„å»ºåŒ»ç–—å®ä½“è¯†åˆ«çš„æ¨¡å‹ï¼ˆåŒ…å«è¯å…¸å’Œè¯­æ–™æ ‡æ³¨ï¼‰ã€å•æ–‡æ¡£éç›‘ç£çš„å…³é”®è¯æŠ½å–ã€Kashgariä¸­ä½¿ç”¨gpt-2è¯­è¨€æ¨¡å‹ã€å¼€æºçš„é‡‘èæŠ•èµ„æ•°æ®æå–å·¥å…·ã€æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦åº“TextTeaser: ä»…æ”¯æŒè‹±æ–‡ã€äººæ°‘æ—¥æŠ¥è¯­æ–™å¤„ç†å·¥å…·é›†ã€ä¸€äº›å…³äºè‡ªç„¶è¯­è¨€çš„åŸºæœ¬æ¨¡å‹ã€åŸºäº14Wæ­Œæ›²çŸ¥è¯†åº“çš„é—®ç­”å°è¯•--åŠŸèƒ½åŒ…æ‹¬æ­Œè¯æ¥é¾™andå·²çŸ¥æ­Œè¯æ‰¾æ­Œæ›²ä»¥åŠæ­Œæ›²æ­Œæ‰‹æ­Œè¯ä¸‰è§’å…³ç³»çš„é—®ç­”ã€åŸºäºSiamese bilstmæ¨¡å‹çš„ç›¸ä¼¼å¥å­åˆ¤å®šæ¨¡å‹å¹¶æä¾›è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ã€ç”¨Transformerç¼–è§£ç æ¨¡å‹å®ç°çš„æ ¹æ®Hacker Newsæ–‡ç« æ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆè¯„è®ºã€ç”¨BERTè¿›è¡Œåºåˆ—æ ‡è®°å’Œæ–‡æœ¬åˆ†ç±»çš„æ¨¡æ¿ä»£ç ã€LitBankï¼šNLPæ•°æ®é›†â€”â€”æ”¯æŒè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—äººæ–‡å­¦ç§‘ä»»åŠ¡çš„100éƒ¨å¸¦æ ‡è®°è‹±æ–‡å°è¯´è¯­æ–™ã€ç™¾åº¦å¼€æºçš„åŸºå‡†ä¿¡æ¯æŠ½å–ç³»ç»Ÿã€è™šå‡æ–°é—»æ•°æ®é›†ã€Facebook: LAMAè¯­è¨€æ¨¡å‹åˆ†æï¼Œæä¾›Transformer-XL/BERT/ELMo/GPTé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€è®¿é—®æ¥å£ã€CommonsenseQAï¼šé¢å‘å¸¸è¯†çš„è‹±æ–‡QAæŒ‘æˆ˜ã€ä¸­æ–‡çŸ¥è¯†å›¾è°±èµ„æ–™ã€æ•°æ®åŠå·¥å…·ã€å„å¤§å…¬å¸å†…éƒ¨é‡Œå¤§ç‰›åˆ†äº«çš„æŠ€æœ¯æ–‡æ¡£ PDF æˆ–è€… PPTã€è‡ªç„¶è¯­è¨€ç”ŸæˆSQLè¯­å¥ï¼ˆè‹±æ–‡ï¼‰ã€ä¸­æ–‡NLPæ•°æ®å¢å¼ºï¼ˆEDAï¼‰å·¥å…·ã€è‹±æ–‡NLPæ•°æ®å¢å¼ºå·¥å…· ã€åŸºäºåŒ»è¯çŸ¥è¯†å›¾è°±çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€äº¬ä¸œå•†å“çŸ¥è¯†å›¾è°±ã€åŸºäºmongodbå­˜å‚¨çš„å†›äº‹é¢†åŸŸçŸ¥è¯†å›¾è°±é—®ç­”é¡¹ç›®ã€åŸºäºè¿œç›‘ç£çš„ä¸­æ–‡å…³ç³»æŠ½å–ã€è¯­éŸ³æƒ…æ„Ÿåˆ†æã€ä¸­æ–‡ULMFiT-æƒ…æ„Ÿåˆ†æ-æ–‡æœ¬åˆ†ç±»-è¯­æ–™åŠæ¨¡å‹ã€ä¸€ä¸ªæ‹ç…§åšé¢˜ç¨‹åºã€ä¸–ç•Œå„å›½å¤§è§„æ¨¡äººååº“ã€ä¸€ä¸ªåˆ©ç”¨æœ‰è¶£ä¸­æ–‡è¯­æ–™åº“ qingyun è®­ç»ƒå‡ºæ¥çš„ä¸­æ–‡èŠå¤©æœºå™¨äººã€ä¸­æ–‡èŠå¤©æœºå™¨äººseqGANã€çœå¸‚åŒºé•‡è¡Œæ”¿åŒºåˆ’æ•°æ®å¸¦æ‹¼éŸ³æ ‡æ³¨ã€æ•™è‚²è¡Œä¸šæ–°é—»è¯­æ–™åº“åŒ…å«è‡ªåŠ¨æ–‡æ‘˜åŠŸèƒ½ã€å¼€æ”¾äº†å¯¹è¯æœºå™¨äºº-çŸ¥è¯†å›¾è°±-è¯­ä¹‰ç†è§£-è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŠæ•°æ®ã€ä¸­æ–‡çŸ¥è¯†å›¾è°±ï¼šåŸºäºç™¾åº¦ç™¾ç§‘ä¸­æ–‡é¡µé¢-æŠ½å–ä¸‰å…ƒç»„ä¿¡æ¯-æ„å»ºä¸­æ–‡çŸ¥è¯†å›¾è°±ã€masr: ä¸­æ–‡è¯­éŸ³è¯†åˆ«-æä¾›é¢„è®­ç»ƒæ¨¡å‹-é«˜è¯†åˆ«ç‡ã€PythonéŸ³é¢‘æ•°æ®å¢å¹¿åº“ã€ä¸­æ–‡å…¨è¯è¦†ç›–BERTåŠä¸¤ä»½é˜…è¯»ç†è§£æ•°æ®ã€ConvLabï¼šå¼€æºå¤šåŸŸç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿå¹³å°ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ã€åŸºäºæœ€æ–°ç‰ˆæœ¬rasaæ­å»ºçš„å¯¹è¯ç³»ç»Ÿã€åŸºäºTensorFlowå’ŒBERTçš„ç®¡é“å¼å®ä½“åŠå…³ç³»æŠ½å–ã€ä¸€ä¸ªå°å‹çš„è¯åˆ¸çŸ¥è¯†å›¾è°±/çŸ¥è¯†åº“ã€å¤ç›˜æ‰€æœ‰NLPæ¯”èµ›çš„TOPæ–¹æ¡ˆã€OpenCLaPï¼šå¤šé¢†åŸŸå¼€æºä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»“åº“ã€UERï¼šåŸºäºä¸åŒè¯­æ–™+ç¼–ç å™¨+ç›®æ ‡ä»»åŠ¡çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ä»“åº“ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å‘é‡åˆé›†ã€åŸºäºé‡‘è-å¸æ³•é¢†åŸŸ(å…¼æœ‰é—²èŠæ€§è´¨)çš„èŠå¤©æœºå™¨äººã€g2pCï¼šåŸºäºä¸Šä¸‹æ–‡çš„æ±‰è¯­è¯»éŸ³è‡ªåŠ¨æ ‡è®°æ¨¡å—ã€Zincbase çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·åŒ…ã€è¯—æ­Œè´¨é‡è¯„ä»·/ç»†ç²’åº¦æƒ…æ„Ÿè¯—æ­Œè¯­æ–™åº“ã€å¿«é€Ÿè½¬åŒ–ã€Œä¸­æ–‡æ•°å­—ã€å’Œã€Œé˜¿æ‹‰ä¼¯æ•°å­—ã€ã€ç™¾åº¦çŸ¥é“é—®ç­”è¯­æ–™åº“ã€åŸºäºçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿã€jieba_fast åŠ é€Ÿç‰ˆçš„jiebaã€æ­£åˆ™è¡¨è¾¾å¼æ•™ç¨‹ã€ä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†ã€åŸºäºBERTç­‰æœ€æ–°è¯­è¨€æ¨¡å‹çš„æŠ½å–å¼æ‘˜è¦æå–ã€Pythonåˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæ–‡æœ¬æ‘˜è¦çš„ç»¼åˆæŒ‡å—ã€çŸ¥è¯†å›¾è°±æ·±åº¦å­¦ä¹ ç›¸å…³èµ„æ–™æ•´ç†ã€ç»´åŸºå¤§è§„æ¨¡å¹³è¡Œæ–‡æœ¬è¯­æ–™ã€StanfordNLP 0.2.0ï¼šçº¯Pythonç‰ˆè‡ªç„¶è¯­è¨€å¤„ç†åŒ…ã€NeuralNLP-NeuralClassifierï¼šè…¾è®¯å¼€æºæ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»å·¥å…·ã€ç«¯åˆ°ç«¯çš„å°é—­åŸŸå¯¹è¯ç³»ç»Ÿã€ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ï¼šNeuroNER vs. BertNERã€æ–°é—»äº‹ä»¶çº¿ç´¢æŠ½å–ã€2019å¹´ç™¾åº¦çš„ä¸‰å…ƒç»„æŠ½å–æ¯”èµ›ï¼šâ€œç§‘å­¦ç©ºé—´é˜Ÿâ€æºç ã€åŸºäºä¾å­˜å¥æ³•çš„å¼€æ”¾åŸŸæ–‡æœ¬çŸ¥è¯†ä¸‰å…ƒç»„æŠ½å–å’ŒçŸ¥è¯†åº“æ„å»ºã€ä¸­æ–‡çš„GPT2è®­ç»ƒä»£ç ã€ML-NLP - æœºå™¨å­¦ä¹ (Machine Learning)NLPé¢è¯•ä¸­å¸¸è€ƒåˆ°çš„çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°ã€nlp4han:ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·é›†(æ–­å¥/åˆ†è¯/è¯æ€§æ ‡æ³¨/ç»„å—/å¥æ³•åˆ†æ/è¯­ä¹‰åˆ†æ/NER/Nå…ƒè¯­æ³•/HMM/ä»£è¯æ¶ˆè§£/æƒ…æ„Ÿåˆ†æ/æ‹¼å†™æ£€æŸ¥ã€XLMï¼šFacebookçš„è·¨è¯­è¨€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€ç”¨åŸºäºBERTçš„å¾®è°ƒå’Œç‰¹å¾æå–æ–¹æ³•æ¥è¿›è¡ŒçŸ¥è¯†å›¾è°±ç™¾åº¦ç™¾ç§‘äººç‰©è¯æ¡å±æ€§æŠ½å–ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³çš„å¼€æ”¾ä»»åŠ¡-æ•°æ®é›†-å½“å‰æœ€ä½³ç»“æœã€CoupletAI - åŸºäºCNN+Bi-LSTM+Attention çš„è‡ªåŠ¨å¯¹å¯¹è”ç³»ç»Ÿã€æŠ½è±¡çŸ¥è¯†å›¾è°±ã€MiningZhiDaoQACorpus - 580ä¸‡ç™¾åº¦çŸ¥é“é—®ç­”æ•°æ®æŒ–æ˜é¡¹ç›®ã€brat rapid annotation tool: åºåˆ—æ ‡æ³¨å·¥å…·ã€å¤§è§„æ¨¡ä¸­æ–‡çŸ¥è¯†å›¾è°±æ•°æ®ï¼š1.4äº¿å®ä½“ã€æ•°æ®å¢å¼ºåœ¨æœºå™¨ç¿»è¯‘åŠå…¶ä»–nlpä»»åŠ¡ä¸­çš„åº”ç”¨åŠæ•ˆæœã€allennlpé˜…è¯»ç†è§£:æ”¯æŒå¤šç§æ•°æ®å’Œæ¨¡å‹ã€PDFè¡¨æ ¼æ•°æ®æå–å·¥å…· ã€ Graphbrainï¼šAIå¼€æºè½¯ä»¶åº“å’Œç§‘ç ”å·¥å…·ï¼Œç›®çš„æ˜¯ä¿ƒè¿›è‡ªåŠ¨æ„ä¹‰æå–å’Œæ–‡æœ¬ç†è§£ä»¥åŠçŸ¥è¯†çš„æ¢ç´¢å’Œæ¨æ–­ã€ç®€å†è‡ªåŠ¨ç­›é€‰ç³»ç»Ÿã€åŸºäºå‘½åå®ä½“è¯†åˆ«çš„ç®€å†è‡ªåŠ¨æ‘˜è¦ã€ä¸­æ–‡è¯­è¨€ç†è§£æµ‹è¯„åŸºå‡†ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§çš„æ•°æ®é›†&åŸºå‡†æ¨¡å‹&è¯­æ–™åº“&æ’è¡Œæ¦œã€æ ‘æ´ OCR æ–‡å­—è¯†åˆ« ã€ä»åŒ…å«è¡¨æ ¼çš„æ‰«æå›¾ç‰‡ä¸­è¯†åˆ«è¡¨æ ¼å’Œæ–‡å­—ã€è¯­å£°è¿ç§»ã€Pythonå£è¯­è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·é›†(è‹±æ–‡)ã€ similarityï¼šç›¸ä¼¼åº¦è®¡ç®—å·¥å…·åŒ…ï¼Œjavaç¼–å†™ã€æµ·é‡ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹ ã€Transformers 2.0 ã€åŸºäºå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†Audiosetçš„éŸ³é¢‘å¢å¼º ã€Poplarï¼šç½‘é¡µç‰ˆè‡ªç„¶è¯­è¨€æ ‡æ³¨å·¥å…·ã€å›¾ç‰‡æ–‡å­—å»é™¤ï¼Œå¯ç”¨äºæ¼«ç”»ç¿»è¯‘ ã€186ç§è¯­è¨€çš„æ•°å­—å«æ³•åº“ã€Amazonå‘å¸ƒåŸºäºçŸ¥è¯†çš„äºº-äººå¼€æ”¾é¢†åŸŸå¯¹è¯æ•°æ®é›† ã€ä¸­æ–‡æ–‡æœ¬çº é”™æ¨¡å—ä»£ç ã€ç¹ç®€ä½“è½¬æ¢ ã€ Pythonå®ç°çš„å¤šç§æ–‡æœ¬å¯è¯»æ€§è¯„ä»·æŒ‡æ ‡ã€ç±»ä¼¼äºäººå/åœ°å/ç»„ç»‡æœºæ„åçš„å‘½åä½“è¯†åˆ«æ•°æ®é›† ã€ä¸œå—å¤§å­¦ã€ŠçŸ¥è¯†å›¾è°±ã€‹ç ”ç©¶ç”Ÿè¯¾ç¨‹(èµ„æ–™)ã€. è‹±æ–‡æ‹¼å†™æ£€æŸ¥åº“ ã€ wwsearchæ˜¯ä¼ä¸šå¾®ä¿¡åå°è‡ªç ”çš„å…¨æ–‡æ£€ç´¢å¼•æ“ã€CHAMELEONï¼šæ·±åº¦å­¦ä¹ æ–°é—»æ¨èç³»ç»Ÿå…ƒæ¶æ„ ã€ 8ç¯‡è®ºæ–‡æ¢³ç†BERTç›¸å…³æ¨¡å‹è¿›å±•ä¸åæ€ã€DocSearchï¼šå…è´¹æ–‡æ¡£æœç´¢å¼•æ“ã€ LIDAï¼šè½»é‡äº¤äº’å¼å¯¹è¯æ ‡æ³¨å·¥å…· ã€aili - the fastest in-memory index in the East ä¸œåŠçƒæœ€å¿«å¹¶å‘ç´¢å¼• ã€çŸ¥è¯†å›¾è°±è½¦éŸ³å·¥ä½œé¡¹ç›®ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆèµ„æºå¤§å…¨ ã€ä¸­æ—¥éŸ©åˆ†è¯åº“mecabçš„Pythonæ¥å£åº“ã€ä¸­æ–‡æ–‡æœ¬æ‘˜è¦/å…³é”®è¯æå–ã€æ±‰å­—å­—ç¬¦ç‰¹å¾æå–å™¨ (featurizer)ï¼Œæå–æ±‰å­—çš„ç‰¹å¾ï¼ˆå‘éŸ³ç‰¹å¾ã€å­—å½¢ç‰¹å¾ï¼‰ç”¨åšæ·±åº¦å­¦ä¹ çš„ç‰¹å¾ã€ä¸­æ–‡ç”Ÿæˆä»»åŠ¡åŸºå‡†æµ‹è¯„ ã€ä¸­æ–‡ç¼©å†™æ•°æ®é›†ã€ä¸­æ–‡ä»»åŠ¡åŸºå‡†æµ‹è¯„ - ä»£è¡¨æ€§çš„æ•°æ®é›†-åŸºå‡†(é¢„è®­ç»ƒ)æ¨¡å‹-è¯­æ–™åº“-baseline-å·¥å…·åŒ…-æ’è¡Œæ¦œã€PySS3ï¼šé¢å‘å¯è§£é‡ŠAIçš„SS3æ–‡æœ¬åˆ†ç±»å™¨æœºå™¨å¯è§†åŒ–å·¥å…· ã€ä¸­æ–‡NLPæ•°æ®é›†åˆ—è¡¨ã€COPE - æ ¼å¾‹è¯—ç¼–è¾‘ç¨‹åºã€doccanoï¼šåŸºäºç½‘é¡µçš„å¼€æºååŒå¤šè¯­è¨€æ–‡æœ¬æ ‡æ³¨å·¥å…· ã€PreNLPï¼šè‡ªç„¶è¯­è¨€é¢„å¤„ç†åº“ã€ç®€å•çš„ç®€å†è§£æå™¨ï¼Œç”¨æ¥ä»ç®€å†ä¸­æå–å…³é”®ä¿¡æ¯ã€ç”¨äºä¸­æ–‡é—²èŠçš„GPT2æ¨¡å‹ï¼šGPT2-chitchatã€åŸºäºæ£€ç´¢èŠå¤©æœºå™¨äººå¤šè½®å“åº”é€‰æ‹©ç›¸å…³èµ„æºåˆ—è¡¨(Leaderboardsã€Datasetsã€Papers)ã€(Colab)æŠ½è±¡æ–‡æœ¬æ‘˜è¦å®ç°é›†é”¦(æ•™ç¨‹ ã€è¯è¯­æ‹¼éŸ³æ•°æ®ã€é«˜æ•ˆæ¨¡ç³Šæœç´¢å·¥å…·ã€NLPæ•°æ®å¢å¹¿èµ„æºé›†ã€å¾®è½¯å¯¹è¯æœºå™¨äººæ¡†æ¶ ã€ GitHub Typo Corpusï¼šå¤§è§„æ¨¡GitHubå¤šè¯­è¨€æ‹¼å†™é”™è¯¯/è¯­æ³•é”™è¯¯æ•°æ®é›†ã€TextClusterï¼šçŸ­æ–‡æœ¬èšç±»é¢„å¤„ç†æ¨¡å— Short text clusterã€é¢å‘è¯­éŸ³è¯†åˆ«çš„ä¸­æ–‡æ–‡æœ¬è§„èŒƒåŒ–ã€BLINKï¼šæœ€å…ˆè¿›çš„å®ä½“é“¾æ¥åº“ã€BertPuncï¼šåŸºäºBERTçš„æœ€å…ˆè¿›æ ‡ç‚¹ä¿®å¤æ¨¡å‹ã€Tokenizerï¼šå¿«é€Ÿã€å¯å®šåˆ¶çš„æ–‡æœ¬è¯æ¡åŒ–åº“ã€ä¸­æ–‡è¯­è¨€ç†è§£æµ‹è¯„åŸºå‡†ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§çš„æ•°æ®é›†ã€åŸºå‡†(é¢„è®­ç»ƒ)æ¨¡å‹ã€è¯­æ–™åº“ã€æ’è¡Œæ¦œã€spaCy åŒ»å­¦æ–‡æœ¬æŒ–æ˜ä¸ä¿¡æ¯æå– ã€ NLPä»»åŠ¡ç¤ºä¾‹é¡¹ç›®ä»£ç é›†ã€ pythonæ‹¼å†™æ£€æŸ¥åº“ã€chatbot-list - è¡Œä¸šå†…å…³äºæ™ºèƒ½å®¢æœã€èŠå¤©æœºå™¨äººçš„åº”ç”¨å’Œæ¶æ„ã€ç®—æ³•åˆ†äº«å’Œä»‹ç»ã€è¯­éŸ³è´¨é‡è¯„ä»·æŒ‡æ ‡(MOSNet, BSSEval, STOI, PESQ, SRMR)ã€ ç”¨138GBè¯­æ–™è®­ç»ƒçš„æ³•æ–‡RoBERTaé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ ã€BERT-NER-Pytorchï¼šä¸‰ç§ä¸åŒæ¨¡å¼çš„BERTä¸­æ–‡NERå®éªŒã€æ— é“è¯å…¸ - æœ‰é“è¯å…¸çš„å‘½ä»¤è¡Œç‰ˆæœ¬ï¼Œæ”¯æŒè‹±æ±‰äº’æŸ¥å’Œåœ¨çº¿æŸ¥è¯¢ã€2019å¹´NLPäº®ç‚¹å›é¡¾ã€ Chinese medical dialogue data ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›† ã€æœ€å¥½çš„æ±‰å­—æ•°å­—(ä¸­æ–‡æ•°å­—)-é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢å·¥å…·ã€ åŸºäºç™¾ç§‘çŸ¥è¯†åº“çš„ä¸­æ–‡è¯è¯­å¤šè¯ä¹‰/ä¹‰é¡¹è·å–ä¸ç‰¹å®šå¥å­è¯è¯­è¯­ä¹‰æ¶ˆæ­§ã€awesome-nlp-sentiment-analysis - æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªåŸå› è¯†åˆ«ã€è¯„ä»·å¯¹è±¡å’Œè¯„ä»·è¯æŠ½å–ã€LineFlowï¼šé¢å‘æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶çš„NLPæ•°æ®é«˜æ•ˆåŠ è½½å™¨ã€ä¸­æ–‡åŒ»å­¦NLPå…¬å¼€èµ„æºæ•´ç† ã€MedQuADï¼š(è‹±æ–‡)åŒ»å­¦é—®ç­”æ•°æ®é›†ã€å°†è‡ªç„¶è¯­è¨€æ•°å­—ä¸²è§£æè½¬æ¢ä¸ºæ•´æ•°å’Œæµ®ç‚¹æ•°ã€Transfer Learning in Natural Language Processing (NLP) ã€é¢å‘è¯­éŸ³è¯†åˆ«çš„ä¸­æ–‡/è‹±æ–‡å‘éŸ³è¾å…¸ã€Tokenizersï¼šæ³¨é‡æ€§èƒ½ä¸å¤šåŠŸèƒ½æ€§çš„æœ€å…ˆè¿›åˆ†è¯å™¨ã€CLUENER ç»†ç²’åº¦å‘½åå®ä½“è¯†åˆ« Fine Grained Named Entity Recognitionã€ åŸºäºBERTçš„ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ã€ä¸­æ–‡è°£è¨€æ•°æ®åº“ã€NLPæ•°æ®é›†/åŸºå‡†ä»»åŠ¡å¤§åˆ—è¡¨ã€nlpç›¸å…³çš„ä¸€äº›è®ºæ–‡åŠä»£ç , åŒ…æ‹¬ä¸»é¢˜æ¨¡å‹ã€è¯å‘é‡(Word Embedding)ã€å‘½åå®ä½“è¯†åˆ«(NER)ã€æ–‡æœ¬åˆ†ç±»(Text Classificatin)ã€æ–‡æœ¬ç”Ÿæˆ(Text Generation)ã€æ–‡æœ¬ç›¸ä¼¼æ€§(Text Similarity)è®¡ç®—ç­‰ï¼Œæ¶‰åŠåˆ°å„ç§ä¸nlpç›¸å…³çš„ç®—æ³•ï¼ŒåŸºäºkeraså’Œtensorflow ã€Pythonæ–‡æœ¬æŒ–æ˜/NLPå®æˆ˜ç¤ºä¾‹ã€ Blackstoneï¼šé¢å‘éç»“æ„åŒ–æ³•å¾‹æ–‡æœ¬çš„spaCy pipelineå’ŒNLPæ¨¡å‹é€šè¿‡åŒä¹‰è¯æ›¿æ¢å®ç°æ–‡æœ¬â€œå˜è„¸â€ ã€ä¸­æ–‡ é¢„è®­ç»ƒ ELECTREA æ¨¡å‹: åŸºäºå¯¹æŠ—å­¦ä¹  pretrain Chinese Model ã€albert-chinese-ner - ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ALBERTåšä¸­æ–‡NER ã€åŸºäºGPT2çš„ç‰¹å®šä¸»é¢˜æ–‡æœ¬ç”Ÿæˆ/æ–‡æœ¬å¢å¹¿ã€å¼€æºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åˆé›†ã€å¤šè¯­è¨€å¥å‘é‡åŒ…ã€ç¼–ç ã€æ ‡è®°å’Œå®ç°ï¼šä¸€ç§å¯æ§é«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ã€ è‹±æ–‡è„è¯å¤§åˆ—è¡¨ ã€attnvisï¼šGPT2ã€BERTç­‰transformerè¯­è¨€æ¨¡å‹æ³¨æ„åŠ›äº¤äº’å¯è§†åŒ–ã€CoVoSTï¼šFacebookå‘å¸ƒçš„å¤šè¯­ç§è¯­éŸ³-æ–‡æœ¬ç¿»è¯‘è¯­æ–™åº“ï¼ŒåŒ…æ‹¬11ç§è¯­è¨€(æ³•è¯­ã€å¾·è¯­ã€è·å…°è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ã€åœŸè€³å…¶è¯­ã€æ³¢æ–¯è¯­ã€ç‘å…¸è¯­ã€è’™å¤è¯­å’Œä¸­æ–‡)çš„è¯­éŸ³ã€æ–‡å­—è½¬å½•åŠè‹±æ–‡è¯‘æ–‡ã€Jiaguè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…· - ä»¥BiLSTMç­‰æ¨¡å‹ä¸ºåŸºç¡€ï¼Œæä¾›çŸ¥è¯†å›¾è°±å…³ç³»æŠ½å– ä¸­æ–‡åˆ†è¯ è¯æ€§æ ‡æ³¨ å‘½åå®ä½“è¯†åˆ« æƒ…æ„Ÿåˆ†æ æ–°è¯å‘ç° å…³é”®è¯ æ–‡æœ¬æ‘˜è¦ æ–‡æœ¬èšç±»ç­‰åŠŸèƒ½ã€ç”¨unetå®ç°å¯¹æ–‡æ¡£è¡¨æ ¼çš„è‡ªåŠ¨æ£€æµ‹ï¼Œè¡¨æ ¼é‡å»ºã€NLPäº‹ä»¶æå–æ–‡çŒ®èµ„æºåˆ—è¡¨ ã€ é‡‘èé¢†åŸŸè‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶èµ„æºå¤§åˆ—è¡¨ã€CLUEDatasetSearch - ä¸­è‹±æ–‡NLPæ•°æ®é›†ï¼šæœç´¢æ‰€æœ‰ä¸­æ–‡NLPæ•°æ®é›†ï¼Œé™„å¸¸ç”¨è‹±æ–‡NLPæ•°æ®é›† ã€medical_NER - ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±å‘½åå®ä½“è¯†åˆ« ã€(å“ˆä½›)è®²å› æœæ¨ç†çš„å…è´¹ä¹¦ã€çŸ¥è¯†å›¾è°±ç›¸å…³å­¦ä¹ èµ„æ–™/æ•°æ®é›†/å·¥å…·èµ„æºå¤§åˆ—è¡¨ã€Forteï¼šçµæ´»å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†pipelineå·¥å…·é›† ã€Pythonå­—ç¬¦ä¸²ç›¸ä¼¼æ€§ç®—æ³•åº“ã€PyLaiaï¼šé¢å‘æ‰‹å†™æ–‡æ¡£åˆ†æçš„æ·±åº¦å­¦ä¹ å·¥å…·åŒ…ã€TextFoolerï¼šé’ˆå¯¹æ–‡æœ¬åˆ†ç±»/æ¨ç†çš„å¯¹æŠ—æ–‡æœ¬ç”Ÿæˆæ¨¡å—ã€Haystackï¼šçµæ´»ã€å¼ºå¤§çš„å¯æ‰©å±•é—®ç­”(QA)æ¡†æ¶ã€ä¸­æ–‡å…³é”®çŸ­è¯­æŠ½å–å·¥å…·",
      "url": "https://github.com/fighting41love/funNLP",
      "published_date": "2018-08-21T11:20:39Z",
      "stars": 74748,
      "forks": 14912,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "medical",
        "Machine Learning",
        "nlp",
        "NLP",
        "ML"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "fighting41love"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "py-gpt",
      "content": "Desktop AI Assistant powered by o1, o3, GPT-4, Gemini, Claude, Ollama, DeepSeek, Grok, Bielik, chat, vision, voice control, image generation and analysis, agents, command execution, file upload/download, speech synthesis and recognition, access to Web, memory, presets, assistants, plugins, and more. Linux, Windows, Mac",
      "url": "https://github.com/szczyglis-dev/py-gpt",
      "published_date": "2023-04-09T23:48:06Z",
      "stars": 1107,
      "forks": 215,
      "language": "Python",
      "entities": [
        "Desktop AI",
        "AI"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "szczyglis-dev"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "my-voice-analysis",
      "content": "My-Voice Analysis is a Python library for the analysis of voice (simultaneous speech, high entropy) without the need of a transcription. It breaks utterances and detects syllable boundaries, fundamental frequency contours, and formants.",
      "url": "https://github.com/Shahabks/my-voice-analysis",
      "published_date": "2018-11-29T17:05:36Z",
      "stars": 321,
      "forks": 92,
      "language": "Python",
      "entities": [
        "Voice Analysis"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "Shahabks"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Parkinson-Disease-Prediction",
      "content": "Introduction  Parkinsonâ€™s Disease is the second most prevalent neurodegenerative disorder after Alzheimerâ€™s, affecting more than 10 million people worldwide. Parkinsonâ€™s is characterized primarily by the deterioration of motor and cognitive ability. There is no single test which can be administered for diagnosis. Instead, doctors must perform a careful clinical analysis of the patientâ€™s medical history. Unfortunately, this method of diagnosis is highly inaccurate. A study from the National Institute of Neurological Disorders finds that early diagnosis (having symptoms for 5 years or less) is only 53% accurate. This is not much better than random guessing, but an early diagnosis is critical to effective treatment. Because of these difficulties, I investigate a machine learning approach to accurately diagnose Parkinsonâ€™s, using a dataset of various speech features (a non-invasive yet characteristic tool) from the University of Oxford. Why speech features? Speech is very predictive and characteristic of Parkinsonâ€™s disease; almost every Parkinsonâ€™s patient experiences severe vocal degradation (inability to produce sustained phonations, tremor, hoarseness), so it makes sense to use voice to diagnose the disease. Voice analysis gives the added benefit of being non-invasive, inexpensive, and very easy to extract clinically. Background  Parkinson's Disease  Parkinsonâ€™s is a progressive neurodegenerative condition resulting from the death of the dopamine containing cells of the substantia nigra (which plays an important role in movement). Symptoms include: â€œfrozenâ€ facial features, bradykinesia (slowness of movement), akinesia (impairment of voluntary movement), tremor, and voice impairment. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. Performance Metrics  TP = true positive, FP = false positive, TN = true negative, FN = false negative Accuracy: (TP+TN)/(P+N) Matthews Correlation Coefficient: 1=perfect, 0=random, -1=completely inaccurate Algorithms Employed  Logistic Regression (LR): Uses the sigmoid logistic equation with weights (coefficient values) and biases (constants) to model the probability of a certain class for binary classification. An output of 1 represents one class, and an output of 0 represents the other. Training the model will learn the optimal weights and biases. Linear Discriminant Analysis (LDA): Assumes that the data is Gaussian and each feature has the same variance. LDA estimates the mean and variance for each class from the training data, and then uses properties of statistics (Bayes theorem , Gaussian distribution, etc) to compute the probability of a particular instance belonging to a given class. The class with the largest probability is the prediction. k Nearest Neighbors (KNN): Makes predictions about the validation set using the entire training set. KNN makes a prediction about a new instance by searching through the entire set to find the k â€œclosestâ€ instances. â€œClosenessâ€ is determined using a proximity measurement (Euclidean) across all features. The class that the majority of the k closest instances belong to is the class that the model predicts the new instance to be. Decision Tree (DT): Represented by a binary tree, where each root node represents an input variable and a split point, and each leaf node contains an output used to make a prediction. Neural Network (NN): Models the way the human brain makes decisions. Each neuron takes in 1+ inputs, and then uses an activation function to process the input with weights and biases to produce an output. Neurons can be arranged into layers, and multiple layers can form a network to model complex decisions. Training the network involves using the training instances to optimize the weights and biases. Naive Bayes (NB): Simplifies the calculation of probabilities by assuming that all features are independent of one another (a strong but effective assumption). Employs Bayes Theorem to calculate the probabilities that the instance to be predicted is in each class, then finds the class with the highest probability. Gradient Boost (GB): Generally used when seeking a model with very high predictive performance. Used to reduce bias and variance (â€œerrorâ€) by combining multiple â€œweak learnersâ€ (not very good models) to create a â€œstrong learnerâ€ (high performance model). Involves 3 elements: a loss function (error function) to be optimized, a weak learner (decision tree) to make predictions, and an additive model to add trees to minimize the loss function. Gradient descent is used to minimize error after adding each tree (one by one). Engineering Goal  Produce a machine learning model to diagnose Parkinsonâ€™s disease given various features of a patientâ€™s speech with at least 90% accuracy and/or a Matthews Correlation Coefficient of at least 0.9. Compare various algorithms and parameters to determine the best model for predicting Parkinsonâ€™s.  Dataset Description  Source: the University of Oxford 195 instances (147 subjects with Parkinsonâ€™s, 48 without Parkinsonâ€™s) 22 features (elements that are possibly characteristic of Parkinsonâ€™s, such as frequency, pitch, amplitude / period of the sound wave) 1 label (1 for Parkinsonâ€™s, 0 for no Parkinsonâ€™s) Project Pipeline  pipeline  Summary of Procedure  Split the Oxford Parkinsonâ€™s Dataset into two parts: one for training, one for validation (evaluate how well the model performs) Train each of the following algorithms with the training set: Logistic Regression, Linear Discriminant Analysis, k Nearest Neighbors, Decision Tree, Neural Network, Naive Bayes, Gradient Boost Evaluate results using the validation set Repeat for the following training set to validation set splits: 80% training / 20% validation, 75% / 25%, and 70% / 30% Repeat for a rescaled version of the dataset (scale all the numbers in the dataset to a range from 0 to 1: this helps to reduce the effect of outliers) Conduct 5 trials and average the results Data  a_o  a_r  m_o  m_r  Data Analysis  In general, the models tended to perform the best (both in terms of accuracy and Matthews Correlation Coefficient) on the rescaled dataset with a 75-25 train-test split. The two highest performing algorithms, k Nearest Neighbors and the Neural Network, both achieved an accuracy of 98%. The NN achieved a MCC of 0.96, while KNN achieved a MCC of 0.94. These figures outperform most existing literature and significantly outperform current methods of diagnosis. Conclusion and Significance  These robust results suggest that a machine learning approach can indeed be implemented to significantly improve diagnosis methods of Parkinsonâ€™s disease. Given the necessity of early diagnosis for effective treatment, my machine learning models provide a very promising alternative to the current, rather ineffective method of diagnosis. Current methods of early diagnosis are only 53% accurate, while my machine learning model produces 98% accuracy. This 45% increase is critical because an accurate, early diagnosis is needed to effectively treat the disease. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. With an earlier diagnosis, much of this degradation could have been slowed or treated. My results are very significant because Parkinsonâ€™s affects over 10 million people worldwide who could benefit greatly from an early, accurate diagnosis. Not only is my machine learning approach more accurate in terms of diagnostic accuracy, it is also more scalable, less expensive, and therefore more accessible to people who might not have access to established medical facilities and professionals. The diagnosis is also much simpler, requiring only a 10-15 second voice recording and producing an immediate diagnosis. Future Research  Given more time and resources, I would investigate the following: Create a mobile application which would allow the user to record his/her voice, extract the necessary vocal features, and feed it into my machine learning model to diagnose Parkinsonâ€™s. Use larger datasets in conjunction with the University of Oxford dataset. Tune and improve my models even further to achieve even better results. Investigate different structures and types of neural networks. Construct a novel algorithm specifically suited for the prediction of Parkinsonâ€™s. Generalize my findings and algorithms for all types of dementia disorders, such as Alzheimerâ€™s. References  Bind, Shubham. \"A Survey of Machine Learning Based Approaches for Parkinson Disease Prediction.\" International Journal of Computer Science and Information Technologies 6 (2015): n. pag. International Journal of Computer Science and Information Technologies. 2015. Web. 8 Mar. 2017. Brooks, Megan. \"Diagnosing Parkinson's Disease Still Challenging.\" Medscape Medical News. National Institute of Neurological Disorders, 31 July 2014. Web. 20 Mar. 2017. Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) Hashmi, Sumaiya F. \"A Machine Learning Approach to Diagnosis of Parkinsonâ€™s Disease.\"Claremont Colleges Scholarship. Claremont College, 2013. Web. 10 Mar. 2017. Karplus, Abraham. \"Machine Learning Algorithms for Cancer Diagnosis.\" Machine Learning Algorithms for Cancer Diagnosis (n.d.): n. pag. Mar. 2012. Web. 20 Mar. 2017. Little, Max. \"Parkinsons Data Set.\" UCI Machine Learning Repository. University of Oxford, 26 June 2008. Web. 20 Feb. 2017. Ozcift, Akin, and Arif Gulten. \"Classifier Ensemble Construction with Rotation Forest to Improve Medical Diagnosis Performance of Machine Learning Algorithms.\" Computer Methods and Programs in Biomedicine 104.3 (2011): 443-51. Semantic Scholar. 2011. Web. 15 Mar. 2017. \"Parkinsonâ€™s Disease Dementia.\" UCI MIND. N.p., 19 Oct. 2015. Web. 17 Feb. 2017. Salvatore, C., A. Cerasa, I. Castiglioni, F. Gallivanone, A. Augimeri, M. Lopez, G. Arabia, M. Morelli, M.c. Gilardi, and A. Quattrone. \"Machine Learning on Brain MRI Data for Differential Diagnosis of Parkinson's Disease and Progressive Supranuclear Palsy.\"Journal of Neuroscience Methods 222 (2014): 230-37. 2014. Web. 18 Mar. 2017. Shahbakhi, Mohammad, Danial Taheri Far, and Ehsan Tahami. \"Speech Analysis for Diagnosis of Parkinsonâ€™s Disease Using Genetic Algorithm and Support Vector Machine.\"Journal of Biomedical Science and Engineering 07.04 (2014): 147-56. Scientific Research. July 2014. Web. 2 Mar. 2017. \"Speech and Communication.\" Speech and Communication. Parkinson's Disease Foundation, n.d. Web. 22 Mar. 2017. Sriram, Tarigoppula V. S., M. Venkateswara Rao, G. V. Satya Narayana, and D. S. V. G. K. Kaladhar. \"Diagnosis of Parkinson Disease Using Machine Learning and Data Mining Systems from Voice Dataset.\" SpringerLink. Springer, Cham, 01 Jan. 1970. Web. 17 Mar. 2017.",
      "url": "https://github.com/Aastha2104/Parkinson-Disease-Prediction",
      "published_date": "2021-01-16T07:02:34Z",
      "stars": 162,
      "forks": 31,
      "language": "Python",
      "entities": [
        "machine learning",
        "medical",
        "Data Mining Systems",
        "Machine Learning",
        "Medscape Medical",
        "Voice analysis",
        "diagnosis",
        "Diagnosis",
        "Improve Medical",
        "Neural Network",
        "clinical",
        "patient",
        "Medical",
        "who",
        "Information Technologies",
        "treatment",
        "disease",
        "Disease"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "Aastha2104"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "VoiceLab",
      "content": "Automated Reproducible Acoustical Analysis",
      "url": "https://github.com/Voice-Lab/VoiceLab",
      "published_date": "2020-02-12T20:25:21Z",
      "stars": 157,
      "forks": 19,
      "language": "Python",
      "entities": [],
      "keywords": [
        "voice analysis"
      ],
      "owner": "Voice-Lab"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "torch-nansypp",
      "content": "NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis",
      "url": "https://github.com/revsic/torch-nansypp",
      "published_date": "2022-12-08T00:52:51Z",
      "stars": 147,
      "forks": 11,
      "language": "Python",
      "entities": [],
      "keywords": [
        "voice analysis"
      ],
      "owner": "revsic"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "hume-python-sdk",
      "content": "Python client for Hume AI",
      "url": "https://github.com/HumeAI/hume-python-sdk",
      "published_date": "2022-08-03T16:18:48Z",
      "stars": 126,
      "forks": 34,
      "language": "Python",
      "entities": [
        "AI",
        "Hume AI"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "HumeAI"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "PyBot-A-ChatBot-For-Answering-Python-Queries-Using-NLP",
      "content": "Pybot can change the way learners try to learn python programming language in a more interactive way. This chatbot will try to solve or provide answer to almost every python related issues or queries that the user is asking for. We are implementing NLP for improving the efficiency of the chatbot. We will include voice feature for more interactivity to the user. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation. NLTK has been called â€œa wonderful tool for teaching and working in, computational linguistics using Python,â€ and â€œan amazing library to play with natural language.The main issue with text data is that it is all in text format (strings). However, the Machine learning algorithms need some sort of numerical feature vector in order to perform the task. So before we start with any NLP project we need to pre-process it to make it ideal for working. Converting the entire text into uppercase or lowercase, so that the algorithm does not treat the same words in different cases as different Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.Removing Noise i.e everything that isnâ€™t in a standard number or letter.Removing Stop words. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form â€” generally a written word form. Example if we were to stem the following words: â€œStemsâ€, â€œStemmingâ€, â€œStemmedâ€, â€œand Stemtizationâ€, the result would be a single word â€œstemâ€. A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that â€œrunâ€ is a base form for words like â€œrunningâ€ or â€œranâ€ or that the word â€œbetterâ€ and â€œgoodâ€ are in the same lemma so they are considered the same.",
      "url": "https://github.com/abhishek305/PyBot-A-ChatBot-For-Answering-Python-Queries-Using-NLP",
      "published_date": "2019-04-27T17:42:21Z",
      "stars": 92,
      "forks": 37,
      "language": "Python",
      "entities": [
        "Machine learning",
        "NLP"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "abhishek305"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "opensauce-python",
      "content": "Voice analysis software (Python port of VoiceSauce)",
      "url": "https://github.com/voicesauce/opensauce-python",
      "published_date": "2014-03-21T18:02:24Z",
      "stars": 59,
      "forks": 16,
      "language": "Python",
      "entities": [
        "Voice analysis"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "voicesauce"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "python_wizard",
      "content": "Command line LPC analysis tool to generate bitstreams for the Texas Instruments TMS5220 chip",
      "url": "https://github.com/ptwz/python_wizard",
      "published_date": "2017-10-08T09:05:46Z",
      "stars": 45,
      "forks": 14,
      "language": "Python",
      "entities": [],
      "keywords": [
        "voice analysis"
      ],
      "owner": "ptwz"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "face_recognition",
      "content": "The world's simplest facial recognition api for Python and the command line",
      "url": "https://github.com/ageitgey/face_recognition",
      "published_date": "2017-03-03T21:52:39Z",
      "stars": 55050,
      "forks": 13624,
      "language": "Python",
      "entities": [
        "facial recognition"
      ],
      "keywords": [
        "facial recognition"
      ],
      "owner": "ageitgey"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "deepface",
      "content": "A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python",
      "url": "https://github.com/serengil/deepface",
      "published_date": "2020-02-08T20:42:28Z",
      "stars": 19670,
      "forks": 2664,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "serengil"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "fawkes",
      "content": "Fawkes, privacy preserving tool against facial recognition systems. More info at https://sandlab.cs.uchicago.edu/fawkes",
      "url": "https://github.com/Shawn-Shan/fawkes",
      "published_date": "2020-05-18T00:16:49Z",
      "stars": 5382,
      "forks": 495,
      "language": "Python",
      "entities": [
        "facial recognition"
      ],
      "keywords": [
        "facial recognition"
      ],
      "owner": "Shawn-Shan"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "facenet-pytorch",
      "content": "Pretrained Pytorch face detection (MTCNN) and facial recognition (InceptionResnet) models",
      "url": "https://github.com/timesler/facenet-pytorch",
      "published_date": "2019-05-25T01:29:24Z",
      "stars": 4929,
      "forks": 993,
      "language": "Python",
      "entities": [
        "facial recognition"
      ],
      "keywords": [
        "facial recognition"
      ],
      "owner": "timesler"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Facial-Expression-Recognition.Pytorch",
      "content": "A CNN based pytorch implementation on facial expression recognition (FER2013 and CK+), achieving 73.112% (state-of-the-art) in FER2013 and 94.64% in CK+ dataset",
      "url": "https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch",
      "published_date": "2018-07-15T06:29:38Z",
      "stars": 1889,
      "forks": 563,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "WuJie1010"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "HRNet-Facial-Landmark-Detection",
      "content": "This is an official implementation of facial landmark detection for our TPAMI paper \"Deep High-Resolution Representation Learning for Visual Recognition\". https://arxiv.org/abs/1908.07919",
      "url": "https://github.com/HRNet/HRNet-Facial-Landmark-Detection",
      "published_date": "2019-04-09T13:25:44Z",
      "stars": 1101,
      "forks": 268,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "HRNet"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "EmoPy",
      "content": "A deep neural net toolkit for emotion analysis via Facial Expression Recognition (FER)",
      "url": "https://github.com/thoughtworksarts/EmoPy",
      "published_date": "2017-12-20T02:19:22Z",
      "stars": 959,
      "forks": 264,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "thoughtworksarts"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "FacialExpressionRecognition",
      "content": "äººè„¸è¯†åˆ«ä¹‹è¡¨æƒ…è¯†åˆ«é¡¹ç›®ç›¸å…³æºç ",
      "url": "https://github.com/luanshiyinyang/FacialExpressionRecognition",
      "published_date": "2019-06-18T08:16:47Z",
      "stars": 888,
      "forks": 176,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "luanshiyinyang"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "facial-expression-recognition",
      "content": null,
      "url": "https://github.com/rondinellimorais/facial-expression-recognition",
      "published_date": "2022-04-04T23:56:37Z",
      "stars": 853,
      "forks": 82,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "rondinellimorais"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Facial-Expression-Recognition",
      "content": "Facial-Expression-Recognition in TensorFlow. Detecting faces in video and recognize the expression(emotion).",
      "url": "https://github.com/xionghc/Facial-Expression-Recognition",
      "published_date": "2017-07-17T10:48:03Z",
      "stars": 665,
      "forks": 193,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "xionghc"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Tongue_diagnosis_Sysytem",
      "content": "åŸºäºæ·±åº¦å­¦ä¹ çš„èˆŒè±¡è¯Šæ–­ç³»ç»Ÿ ",
      "url": "https://github.com/NinjaRabbitOvO/Tongue_diagnosis_Sysytem",
      "published_date": "2022-03-25T12:04:13Z",
      "stars": 53,
      "forks": 17,
      "language": null,
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "NinjaRabbitOvO"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Tongue_diagnosis",
      "content": "åŸºäºæ·±åº¦å­¦ä¹ çš„èˆŒè‹”æ£€æµ‹æ¯•è®¾ ç•™æ¡£",
      "url": "https://github.com/812411838/Tongue_diagnosis",
      "published_date": "2023-02-17T05:43:51Z",
      "stars": 40,
      "forks": 4,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "812411838"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "TongueDiagnosis",
      "content": "èˆŒè¯Šç³»ç»Ÿï¼Œè¯Šæ–­èˆŒå¤´ç—‡çŠ¶ï¼Œç»™å‡ºé£Ÿç–—æ–¹æ¡ˆã€‚",
      "url": "https://github.com/IronSpiderMan/TongueDiagnosis",
      "published_date": "2023-02-11T08:59:38Z",
      "stars": 31,
      "forks": 10,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "IronSpiderMan"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "tongue-diagnosis",
      "content": "Tongue :stuck_out_tongue: diagnose by CNN, VGG-16, Res-50 models and a web application demo.",
      "url": "https://github.com/YaoxiangLi/tongue-diagnosis",
      "published_date": "2018-03-19T06:57:39Z",
      "stars": 12,
      "forks": 1,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "YaoxiangLi"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "SelectorNet",
      "content": "The project implementation of the paper \"A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Diagnosis\".",
      "url": "https://github.com/cshan-github/SelectorNet",
      "published_date": "2023-08-15T15:21:10Z",
      "stars": 11,
      "forks": 0,
      "language": "Python",
      "entities": [
        "TCM",
        "Diagnosis"
      ],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "cshan-github"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "chinese-medicine-tongue-diagnosis",
      "content": "ä¸­åŒ»èˆŒè¯Šç³»ç»Ÿ - ä¸€ä¸ªåŸºäºAIçš„ä¸­åŒ»èˆŒè¯Šåˆ†æå’ŒçŸ¥è¯†åº“ç³»ç»Ÿ",
      "url": "https://github.com/invisible30/chinese-medicine-tongue-diagnosis",
      "published_date": "2025-05-02T03:19:06Z",
      "stars": 10,
      "forks": 4,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "invisible30"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "tongueDiagnosis",
      "content": null,
      "url": "https://github.com/zhiyingproject/tongueDiagnosis",
      "published_date": "2021-06-24T19:42:32Z",
      "stars": 5,
      "forks": 1,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "zhiyingproject"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "TongueDiagnosis",
      "content": null,
      "url": "https://github.com/Sondermmu/TongueDiagnosis",
      "published_date": "2025-05-26T07:00:14Z",
      "stars": 3,
      "forks": 0,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "Sondermmu"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Tongue-Image-Analysis-for-Covid-19-Diagnosis-and-Disease-Detection",
      "content": null,
      "url": "https://github.com/varsharamesh82/Tongue-Image-Analysis-for-Covid-19-Diagnosis-and-Disease-Detection",
      "published_date": "2022-04-04T07:50:21Z",
      "stars": 2,
      "forks": 0,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "varsharamesh82"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "tongue-diagnosis",
      "content": null,
      "url": "https://github.com/ray-SDJ/tongue-diagnosis",
      "published_date": "2024-07-29T23:11:11Z",
      "stars": 1,
      "forks": 0,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "ray-SDJ"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI_Hospital",
      "content": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis",
      "url": "https://github.com/LibertFan/AI_Hospital",
      "published_date": "2024-02-21T02:22:39Z",
      "stars": 168,
      "forks": 23,
      "language": "Python",
      "entities": [
        "Clinical",
        "Diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "LibertFan"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Image_Recognition_WebGUI",
      "content": "âœ¨åŸºäº 3D å·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„é˜¿å°”å…¹æµ·é»˜æ™ºèƒ½è¯Šæ–­ Web åº”ç”¨ Alzheimer's Intelligent Diagnosis Web Application based on 3D Convolutional Neural Network and the ADNI Dataset âœ¨ (with README in English) ğŸš©ï¼šå›¾åƒè¯†åˆ«å¯è§†åŒ–ç•Œé¢ï¼Œå¿«é€Ÿéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ºç½‘é¡µåº”ç”¨ï¼ŒWebé¢„æµ‹ç³»ç»Ÿï¼Œå†³ç­–æ”¯æŒç³»ç»Ÿ(DSS)ï¼Œå›¾åƒè¯†åˆ«å‰ç«¯ç½‘é¡µï¼Œå›¾åƒè¯†åˆ«Demoå±•ç¤º-Pywebioã€‚AIäººå·¥æ™ºèƒ½å›¾åƒè¯†åˆ«-Pytorchï¼›niiåŒ»å­¦å½±åƒå¤„ç†ï¼›ADNIæ•°æ®é›†ã€‚100%çº¯Pythonä»£ç ï¼Œè½»é‡åŒ–ï¼Œæ˜“å¤ç° ",
      "url": "https://github.com/bytesc/Image_Recognition_WebGUI",
      "published_date": "2023-04-26T03:16:44Z",
      "stars": 139,
      "forks": 9,
      "language": "Python",
      "entities": [
        "Neural Network",
        "Diagnosis"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "bytesc"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "HealthChatbot",
      "content": "ğŸ¤– HealthCare ChatBot Major -1 (4th year - 7th semester)  Health Care Chat-Bot is a Healthcare Domain Chatbot to simulate the predictions of a General Physician.  ChatBot can be described as software that can chat with people using artificial intelligence. These software are used to perform tasks such as quickly responding to users, informing them, helping to purchase products and providing better service to customers. We have made a healthcare based chatbot.  The three main areas where chatbots can be used are diagnostics, patient engagement outside medical facilities, and mental health. In our major we are working on diagnostic.  ğŸ“ƒ Brief A chatbot is an artificially intelligent creature which can converse with humans. This could be text-based, or a spoken conversation. In our project we will be using Python as it is currently the most popular language for creating an AI chatbot. In the middle of AI chatbot, architecture is the Natural Language Processing (NLP) layer.  This project aims to build an user-friendly healthcare chatbot which facilitates the job of a healthcare provider and helps improve their performance by interacting with users in a human-like way.  Through chatbots one can communicate with text or voice interface and get reply through artificial intelligence  Typically, a chat bot will communicate with a real person. Chat bots are used in applications such as E-commerce customer service, Call centres, Internet gaming,etc.  Chatbots are programs built to automatically engage with received messages. Chatbots can be programmed to respond the same way each time, to respond differently to messages containing certain keywords and even to use machine learning to adapt their responses to fit the situation.  A developing number of hospitals, nursing homes, and even private centres, presently utilize online Chatbots for human services on their sites. These bots connect with potential patients visiting the site, helping them discover specialists, booking their appointments, and getting them access to the correct treatment.  In any case, the utilization of artificial intelligence in an industry where individualsâ€™ lives could be in question, still starts misgivings in individuals. It brings up issues about whether the task mentioned above ought to be assigned to human staff. This healthcare chatbot system will help hospitals to provide healthcare support online 24 x 7, it answers deep as well as general questions. It also helps to generate leads and automatically delivers the information of leads to sales. By asking the questions in series it helps patients by guiding what exactly he/she is looking for.  ğŸ“œ Problem Statement During the pandemic, it is more important than ever to get your regular check-ups and to continue to take prescription medications. The healthier you are, the more likely you are to recover quickly from an illness.  In this time patients or health care workers within their practice, providers are deferring elective and preventive visits, such as annual physicals. For some, it is not possible to consult online. In this case, to avoid false information, our project can be of help.  ğŸ“‡ Features Register Screen. Sign-in Screen. Generates database for user login system. Offers you a GUI Based Chatbot for patients for diagnosing. [A pragmatic Approach for Diagnosis] Reccomends an appropriate doctor to you for the following symptom. ğŸ“œ Modules Used Our program uses a number of python modules to work properly:  tkinter os webbrowser numpy pandas matplotlib ğŸ“ƒ Algorithm We have used Decision tree for our health care based chat bot.  Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.It usually mimic human thinking ability while making a decision, so it is easy to understand.  :suspect: Project Members Anushka Bansal - 500067844 - R164218014 Shreya Sharma - 500068573 - R164218070 Silvi - 500069092 - R164218072 Ishika Agrawal - 500071154 - R164218097",
      "url": "https://github.com/shreyasharma04/HealthChatbot",
      "published_date": "2022-02-21T08:27:25Z",
      "stars": 113,
      "forks": 20,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "machine learning",
        "medical",
        "healthcare",
        "Healthcare",
        "Diagnosis",
        "symptom",
        "NLP",
        "patient",
        "treatment",
        "HealthCare",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "shreyasharma04"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Image-Recognition-system",
      "content": "âœ¨åŸºäº 3D å·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„é˜¿å°”å…¹æµ·é»˜æ™ºèƒ½è¯Šæ–­ Web åº”ç”¨  Alzheimer's Intelligent Diagnosis Web Application based on 3D Convolutional Neural Network and the ADNI Dataset âœ¨ ğŸš©(with README in English) ğŸ“Œå«åœ¨çº¿demoï¼šåŒ»å­¦å½±åƒè¯†åˆ«ç³»ç»Ÿï¼Œå›¾åƒè¯†åˆ«å¯è§†åŒ–ç•Œé¢ï¼ŒOCRï¼Œå¿«é€Ÿéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ºç½‘é¡µåº”ç”¨ï¼ŒWebé¢„æµ‹ç³»ç»Ÿï¼Œå†³ç­–æ”¯æŒç³»ç»Ÿ(DSS)ï¼Œå›¾åƒè¯†åˆ«å‰ç«¯ç½‘é¡µï¼Œå›¾åƒè¯†åˆ«Demoå±•ç¤º-Pywebioã€‚AIäººå·¥æ™ºèƒ½å›¾åƒè¯†åˆ«-Pytorchï¼›niiåŒ»å­¦å½±åƒå¤„ç†ï¼›ADNIæ•°æ®é›†ã€‚100%çº¯Pythonä»£ç ï¼Œè½»é‡åŒ–ï¼Œæ˜“å¤ç° ",
      "url": "https://github.com/bytesc/Image-Recognition-system",
      "published_date": "2023-06-13T09:22:54Z",
      "stars": 105,
      "forks": 13,
      "language": "Python",
      "entities": [
        "Neural Network",
        "Diagnosis"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "bytesc"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "SLIViT",
      "content": "An AI framework for clinical diagnosis of 3D biomedical scans",
      "url": "https://github.com/cozygene/SLIViT",
      "published_date": "2024-10-05T05:24:56Z",
      "stars": 101,
      "forks": 27,
      "language": "Python",
      "entities": [
        "clinical",
        "An AI",
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "cozygene"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "nmed2024",
      "content": "AI-based differential diagnosis of dementia etiologies on multimodal data",
      "url": "https://github.com/vkola-lab/nmed2024",
      "published_date": "2024-05-21T12:24:52Z",
      "stars": 89,
      "forks": 16,
      "language": "Python",
      "entities": [
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "vkola-lab"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI-Healthcare-chatbot",
      "content": "Through a series of questions about symptoms it diagnosis the health condition of patient. ",
      "url": "https://github.com/vsharathchandra/AI-Healthcare-chatbot",
      "published_date": "2018-03-20T16:52:42Z",
      "stars": 86,
      "forks": 64,
      "language": "Python",
      "entities": [
        "diagnosis",
        "patient"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "vsharathchandra"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AIOsp-Fault-Diagnosis",
      "content": null,
      "url": "https://github.com/OS-ABC/AIOsp-Fault-Diagnosis",
      "published_date": "2021-03-07T08:55:33Z",
      "stars": 49,
      "forks": 19,
      "language": "Python",
      "entities": [],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "OS-ABC"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "picai_baseline",
      "content": "Baseline AI models for 3D csPCa detection/diagnosis in bpMRI",
      "url": "https://github.com/DIAGNijmegen/picai_baseline",
      "published_date": "2022-06-14T16:26:24Z",
      "stars": 49,
      "forks": 28,
      "language": "Python",
      "entities": [
        "Baseline AI",
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "DIAGNijmegen"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI4Eyes",
      "content": "Multi-level diagnosis of cataract from anterior images via deep learning",
      "url": "https://github.com/wuxianjia1996/AI4Eyes",
      "published_date": "2022-05-16T08:41:59Z",
      "stars": 46,
      "forks": 0,
      "language": "Python",
      "entities": [
        "diagnosis",
        "deep learning"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "wuxianjia1996"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "funNLP",
      "content": "ä¸­è‹±æ–‡æ•æ„Ÿè¯ã€è¯­è¨€æ£€æµ‹ã€ä¸­å¤–æ‰‹æœº/ç”µè¯å½’å±åœ°/è¿è¥å•†æŸ¥è¯¢ã€åå­—æ¨æ–­æ€§åˆ«ã€æ‰‹æœºå·æŠ½å–ã€èº«ä»½è¯æŠ½å–ã€é‚®ç®±æŠ½å–ã€ä¸­æ—¥æ–‡äººååº“ã€ä¸­æ–‡ç¼©å†™åº“ã€æ‹†å­—è¯å…¸ã€è¯æ±‡æƒ…æ„Ÿå€¼ã€åœç”¨è¯ã€ååŠ¨è¯è¡¨ã€æš´æè¯è¡¨ã€ç¹ç®€ä½“è½¬æ¢ã€è‹±æ–‡æ¨¡æ‹Ÿä¸­æ–‡å‘éŸ³ã€æ±ªå³°æ­Œè¯ç”Ÿæˆå™¨ã€èŒä¸šåç§°è¯åº“ã€åŒä¹‰è¯åº“ã€åä¹‰è¯åº“ã€å¦å®šè¯åº“ã€æ±½è½¦å“ç‰Œè¯åº“ã€æ±½è½¦é›¶ä»¶è¯åº“ã€è¿ç»­è‹±æ–‡åˆ‡å‰²ã€å„ç§ä¸­æ–‡è¯å‘é‡ã€å…¬å¸åå­—å¤§å…¨ã€å¤è¯—è¯åº“ã€ITè¯åº“ã€è´¢ç»è¯åº“ã€æˆè¯­è¯åº“ã€åœ°åè¯åº“ã€å†å²åäººè¯åº“ã€è¯—è¯è¯åº“ã€åŒ»å­¦è¯åº“ã€é¥®é£Ÿè¯åº“ã€æ³•å¾‹è¯åº“ã€æ±½è½¦è¯åº“ã€åŠ¨ç‰©è¯åº“ã€ä¸­æ–‡èŠå¤©è¯­æ–™ã€ä¸­æ–‡è°£è¨€æ•°æ®ã€ç™¾åº¦ä¸­æ–‡é—®ç­”æ•°æ®é›†ã€å¥å­ç›¸ä¼¼åº¦åŒ¹é…ç®—æ³•é›†åˆã€bertèµ„æºã€æ–‡æœ¬ç”Ÿæˆ&æ‘˜è¦ç›¸å…³å·¥å…·ã€cocoNLPä¿¡æ¯æŠ½å–å·¥å…·ã€å›½å†…ç”µè¯å·ç æ­£åˆ™åŒ¹é…ã€æ¸…åå¤§å­¦XLORE:ä¸­è‹±æ–‡è·¨è¯­è¨€ç™¾ç§‘çŸ¥è¯†å›¾è°±ã€æ¸…åå¤§å­¦äººå·¥æ™ºèƒ½æŠ€æœ¯ç³»åˆ—æŠ¥å‘Šã€è‡ªç„¶è¯­è¨€ç”Ÿæˆã€NLUå¤ªéš¾äº†ç³»åˆ—ã€è‡ªåŠ¨å¯¹è”æ•°æ®åŠæœºå™¨äººã€ç”¨æˆ·åé»‘åå•åˆ—è¡¨ã€ç½ªåæ³•åŠ¡åè¯åŠåˆ†ç±»æ¨¡å‹ã€å¾®ä¿¡å…¬ä¼—å·è¯­æ–™ã€cs224næ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†è¯¾ç¨‹ã€ä¸­æ–‡æ‰‹å†™æ±‰å­—è¯†åˆ«ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç† è¯­æ–™/æ•°æ®é›†ã€å˜é‡å‘½åç¥å™¨ã€åˆ†è¯è¯­æ–™åº“+ä»£ç ã€ä»»åŠ¡å‹å¯¹è¯è‹±æ–‡æ•°æ®é›†ã€ASR è¯­éŸ³æ•°æ®é›† + åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€ç¬‘å£°æ£€æµ‹å™¨ã€Microsoftå¤šè¯­è¨€æ•°å­—/å•ä½/å¦‚æ—¥æœŸæ—¶é—´è¯†åˆ«åŒ…ã€ä¸­åæ–°åå­—å…¸æ•°æ®åº“åŠapi(åŒ…æ‹¬å¸¸ç”¨æ­‡åè¯­ã€æˆè¯­ã€è¯è¯­å’Œæ±‰å­—)ã€æ–‡æ¡£å›¾è°±è‡ªåŠ¨ç”Ÿæˆã€SpaCy ä¸­æ–‡æ¨¡å‹ã€Common Voiceè¯­éŸ³è¯†åˆ«æ•°æ®é›†æ–°ç‰ˆã€ç¥ç»ç½‘ç»œå…³ç³»æŠ½å–ã€åŸºäºbertçš„å‘½åå®ä½“è¯†åˆ«ã€å…³é”®è¯(Keyphrase)æŠ½å–åŒ…pkeã€åŸºäºåŒ»ç–—é¢†åŸŸçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿã€åŸºäºä¾å­˜å¥æ³•ä¸è¯­ä¹‰è§’è‰²æ ‡æ³¨çš„äº‹ä»¶ä¸‰å…ƒç»„æŠ½å–ã€ä¾å­˜å¥æ³•åˆ†æ4ä¸‡å¥é«˜è´¨é‡æ ‡æ³¨æ•°æ®ã€cnocrï¼šç”¨æ¥åšä¸­æ–‡OCRçš„Python3åŒ…ã€ä¸­æ–‡äººç‰©å…³ç³»çŸ¥è¯†å›¾è°±é¡¹ç›®ã€ä¸­æ–‡nlpç«èµ›é¡¹ç›®åŠä»£ç æ±‡æ€»ã€ä¸­æ–‡å­—ç¬¦æ•°æ®ã€speech-aligner: ä»â€œäººå£°è¯­éŸ³â€åŠå…¶â€œè¯­è¨€æ–‡æœ¬â€äº§ç”ŸéŸ³ç´ çº§åˆ«æ—¶é—´å¯¹é½æ ‡æ³¨çš„å·¥å…·ã€AmpliGraph: çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ (Python)åº“ï¼šçŸ¥è¯†å›¾è°±æ¦‚å¿µé“¾æ¥é¢„æµ‹ã€Scattertext æ–‡æœ¬å¯è§†åŒ–(python)ã€è¯­è¨€/çŸ¥è¯†è¡¨ç¤ºå·¥å…·ï¼šBERT & ERNIEã€ä¸­æ–‡å¯¹æ¯”è‹±æ–‡è‡ªç„¶è¯­è¨€å¤„ç†NLPçš„åŒºåˆ«ç»¼è¿°ã€Synonymsä¸­æ–‡è¿‘ä¹‰è¯å·¥å…·åŒ…ã€HarvestTexté¢†åŸŸè‡ªé€‚åº”æ–‡æœ¬æŒ–æ˜å·¥å…·ï¼ˆæ–°è¯å‘ç°-æƒ…æ„Ÿåˆ†æ-å®ä½“é“¾æ¥ç­‰ï¼‰ã€word2wordï¼š(Python)æ–¹ä¾¿æ˜“ç”¨çš„å¤šè¯­è¨€è¯-è¯å¯¹é›†ï¼š62ç§è¯­è¨€/3,564ä¸ªå¤šè¯­è¨€å¯¹ã€è¯­éŸ³è¯†åˆ«è¯­æ–™ç”Ÿæˆå·¥å…·ï¼šä»å…·æœ‰éŸ³é¢‘/å­—å¹•çš„åœ¨çº¿è§†é¢‘åˆ›å»ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)è¯­æ–™åº“ã€æ„å»ºåŒ»ç–—å®ä½“è¯†åˆ«çš„æ¨¡å‹ï¼ˆåŒ…å«è¯å…¸å’Œè¯­æ–™æ ‡æ³¨ï¼‰ã€å•æ–‡æ¡£éç›‘ç£çš„å…³é”®è¯æŠ½å–ã€Kashgariä¸­ä½¿ç”¨gpt-2è¯­è¨€æ¨¡å‹ã€å¼€æºçš„é‡‘èæŠ•èµ„æ•°æ®æå–å·¥å…·ã€æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦åº“TextTeaser: ä»…æ”¯æŒè‹±æ–‡ã€äººæ°‘æ—¥æŠ¥è¯­æ–™å¤„ç†å·¥å…·é›†ã€ä¸€äº›å…³äºè‡ªç„¶è¯­è¨€çš„åŸºæœ¬æ¨¡å‹ã€åŸºäº14Wæ­Œæ›²çŸ¥è¯†åº“çš„é—®ç­”å°è¯•--åŠŸèƒ½åŒ…æ‹¬æ­Œè¯æ¥é¾™andå·²çŸ¥æ­Œè¯æ‰¾æ­Œæ›²ä»¥åŠæ­Œæ›²æ­Œæ‰‹æ­Œè¯ä¸‰è§’å…³ç³»çš„é—®ç­”ã€åŸºäºSiamese bilstmæ¨¡å‹çš„ç›¸ä¼¼å¥å­åˆ¤å®šæ¨¡å‹å¹¶æä¾›è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ã€ç”¨Transformerç¼–è§£ç æ¨¡å‹å®ç°çš„æ ¹æ®Hacker Newsæ–‡ç« æ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆè¯„è®ºã€ç”¨BERTè¿›è¡Œåºåˆ—æ ‡è®°å’Œæ–‡æœ¬åˆ†ç±»çš„æ¨¡æ¿ä»£ç ã€LitBankï¼šNLPæ•°æ®é›†â€”â€”æ”¯æŒè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—äººæ–‡å­¦ç§‘ä»»åŠ¡çš„100éƒ¨å¸¦æ ‡è®°è‹±æ–‡å°è¯´è¯­æ–™ã€ç™¾åº¦å¼€æºçš„åŸºå‡†ä¿¡æ¯æŠ½å–ç³»ç»Ÿã€è™šå‡æ–°é—»æ•°æ®é›†ã€Facebook: LAMAè¯­è¨€æ¨¡å‹åˆ†æï¼Œæä¾›Transformer-XL/BERT/ELMo/GPTé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€è®¿é—®æ¥å£ã€CommonsenseQAï¼šé¢å‘å¸¸è¯†çš„è‹±æ–‡QAæŒ‘æˆ˜ã€ä¸­æ–‡çŸ¥è¯†å›¾è°±èµ„æ–™ã€æ•°æ®åŠå·¥å…·ã€å„å¤§å…¬å¸å†…éƒ¨é‡Œå¤§ç‰›åˆ†äº«çš„æŠ€æœ¯æ–‡æ¡£ PDF æˆ–è€… PPTã€è‡ªç„¶è¯­è¨€ç”ŸæˆSQLè¯­å¥ï¼ˆè‹±æ–‡ï¼‰ã€ä¸­æ–‡NLPæ•°æ®å¢å¼ºï¼ˆEDAï¼‰å·¥å…·ã€è‹±æ–‡NLPæ•°æ®å¢å¼ºå·¥å…· ã€åŸºäºåŒ»è¯çŸ¥è¯†å›¾è°±çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€äº¬ä¸œå•†å“çŸ¥è¯†å›¾è°±ã€åŸºäºmongodbå­˜å‚¨çš„å†›äº‹é¢†åŸŸçŸ¥è¯†å›¾è°±é—®ç­”é¡¹ç›®ã€åŸºäºè¿œç›‘ç£çš„ä¸­æ–‡å…³ç³»æŠ½å–ã€è¯­éŸ³æƒ…æ„Ÿåˆ†æã€ä¸­æ–‡ULMFiT-æƒ…æ„Ÿåˆ†æ-æ–‡æœ¬åˆ†ç±»-è¯­æ–™åŠæ¨¡å‹ã€ä¸€ä¸ªæ‹ç…§åšé¢˜ç¨‹åºã€ä¸–ç•Œå„å›½å¤§è§„æ¨¡äººååº“ã€ä¸€ä¸ªåˆ©ç”¨æœ‰è¶£ä¸­æ–‡è¯­æ–™åº“ qingyun è®­ç»ƒå‡ºæ¥çš„ä¸­æ–‡èŠå¤©æœºå™¨äººã€ä¸­æ–‡èŠå¤©æœºå™¨äººseqGANã€çœå¸‚åŒºé•‡è¡Œæ”¿åŒºåˆ’æ•°æ®å¸¦æ‹¼éŸ³æ ‡æ³¨ã€æ•™è‚²è¡Œä¸šæ–°é—»è¯­æ–™åº“åŒ…å«è‡ªåŠ¨æ–‡æ‘˜åŠŸèƒ½ã€å¼€æ”¾äº†å¯¹è¯æœºå™¨äºº-çŸ¥è¯†å›¾è°±-è¯­ä¹‰ç†è§£-è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŠæ•°æ®ã€ä¸­æ–‡çŸ¥è¯†å›¾è°±ï¼šåŸºäºç™¾åº¦ç™¾ç§‘ä¸­æ–‡é¡µé¢-æŠ½å–ä¸‰å…ƒç»„ä¿¡æ¯-æ„å»ºä¸­æ–‡çŸ¥è¯†å›¾è°±ã€masr: ä¸­æ–‡è¯­éŸ³è¯†åˆ«-æä¾›é¢„è®­ç»ƒæ¨¡å‹-é«˜è¯†åˆ«ç‡ã€PythonéŸ³é¢‘æ•°æ®å¢å¹¿åº“ã€ä¸­æ–‡å…¨è¯è¦†ç›–BERTåŠä¸¤ä»½é˜…è¯»ç†è§£æ•°æ®ã€ConvLabï¼šå¼€æºå¤šåŸŸç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿå¹³å°ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ã€åŸºäºæœ€æ–°ç‰ˆæœ¬rasaæ­å»ºçš„å¯¹è¯ç³»ç»Ÿã€åŸºäºTensorFlowå’ŒBERTçš„ç®¡é“å¼å®ä½“åŠå…³ç³»æŠ½å–ã€ä¸€ä¸ªå°å‹çš„è¯åˆ¸çŸ¥è¯†å›¾è°±/çŸ¥è¯†åº“ã€å¤ç›˜æ‰€æœ‰NLPæ¯”èµ›çš„TOPæ–¹æ¡ˆã€OpenCLaPï¼šå¤šé¢†åŸŸå¼€æºä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»“åº“ã€UERï¼šåŸºäºä¸åŒè¯­æ–™+ç¼–ç å™¨+ç›®æ ‡ä»»åŠ¡çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ä»“åº“ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å‘é‡åˆé›†ã€åŸºäºé‡‘è-å¸æ³•é¢†åŸŸ(å…¼æœ‰é—²èŠæ€§è´¨)çš„èŠå¤©æœºå™¨äººã€g2pCï¼šåŸºäºä¸Šä¸‹æ–‡çš„æ±‰è¯­è¯»éŸ³è‡ªåŠ¨æ ‡è®°æ¨¡å—ã€Zincbase çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·åŒ…ã€è¯—æ­Œè´¨é‡è¯„ä»·/ç»†ç²’åº¦æƒ…æ„Ÿè¯—æ­Œè¯­æ–™åº“ã€å¿«é€Ÿè½¬åŒ–ã€Œä¸­æ–‡æ•°å­—ã€å’Œã€Œé˜¿æ‹‰ä¼¯æ•°å­—ã€ã€ç™¾åº¦çŸ¥é“é—®ç­”è¯­æ–™åº“ã€åŸºäºçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿã€jieba_fast åŠ é€Ÿç‰ˆçš„jiebaã€æ­£åˆ™è¡¨è¾¾å¼æ•™ç¨‹ã€ä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†ã€åŸºäºBERTç­‰æœ€æ–°è¯­è¨€æ¨¡å‹çš„æŠ½å–å¼æ‘˜è¦æå–ã€Pythonåˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæ–‡æœ¬æ‘˜è¦çš„ç»¼åˆæŒ‡å—ã€çŸ¥è¯†å›¾è°±æ·±åº¦å­¦ä¹ ç›¸å…³èµ„æ–™æ•´ç†ã€ç»´åŸºå¤§è§„æ¨¡å¹³è¡Œæ–‡æœ¬è¯­æ–™ã€StanfordNLP 0.2.0ï¼šçº¯Pythonç‰ˆè‡ªç„¶è¯­è¨€å¤„ç†åŒ…ã€NeuralNLP-NeuralClassifierï¼šè…¾è®¯å¼€æºæ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»å·¥å…·ã€ç«¯åˆ°ç«¯çš„å°é—­åŸŸå¯¹è¯ç³»ç»Ÿã€ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ï¼šNeuroNER vs. BertNERã€æ–°é—»äº‹ä»¶çº¿ç´¢æŠ½å–ã€2019å¹´ç™¾åº¦çš„ä¸‰å…ƒç»„æŠ½å–æ¯”èµ›ï¼šâ€œç§‘å­¦ç©ºé—´é˜Ÿâ€æºç ã€åŸºäºä¾å­˜å¥æ³•çš„å¼€æ”¾åŸŸæ–‡æœ¬çŸ¥è¯†ä¸‰å…ƒç»„æŠ½å–å’ŒçŸ¥è¯†åº“æ„å»ºã€ä¸­æ–‡çš„GPT2è®­ç»ƒä»£ç ã€ML-NLP - æœºå™¨å­¦ä¹ (Machine Learning)NLPé¢è¯•ä¸­å¸¸è€ƒåˆ°çš„çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°ã€nlp4han:ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·é›†(æ–­å¥/åˆ†è¯/è¯æ€§æ ‡æ³¨/ç»„å—/å¥æ³•åˆ†æ/è¯­ä¹‰åˆ†æ/NER/Nå…ƒè¯­æ³•/HMM/ä»£è¯æ¶ˆè§£/æƒ…æ„Ÿåˆ†æ/æ‹¼å†™æ£€æŸ¥ã€XLMï¼šFacebookçš„è·¨è¯­è¨€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€ç”¨åŸºäºBERTçš„å¾®è°ƒå’Œç‰¹å¾æå–æ–¹æ³•æ¥è¿›è¡ŒçŸ¥è¯†å›¾è°±ç™¾åº¦ç™¾ç§‘äººç‰©è¯æ¡å±æ€§æŠ½å–ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³çš„å¼€æ”¾ä»»åŠ¡-æ•°æ®é›†-å½“å‰æœ€ä½³ç»“æœã€CoupletAI - åŸºäºCNN+Bi-LSTM+Attention çš„è‡ªåŠ¨å¯¹å¯¹è”ç³»ç»Ÿã€æŠ½è±¡çŸ¥è¯†å›¾è°±ã€MiningZhiDaoQACorpus - 580ä¸‡ç™¾åº¦çŸ¥é“é—®ç­”æ•°æ®æŒ–æ˜é¡¹ç›®ã€brat rapid annotation tool: åºåˆ—æ ‡æ³¨å·¥å…·ã€å¤§è§„æ¨¡ä¸­æ–‡çŸ¥è¯†å›¾è°±æ•°æ®ï¼š1.4äº¿å®ä½“ã€æ•°æ®å¢å¼ºåœ¨æœºå™¨ç¿»è¯‘åŠå…¶ä»–nlpä»»åŠ¡ä¸­çš„åº”ç”¨åŠæ•ˆæœã€allennlpé˜…è¯»ç†è§£:æ”¯æŒå¤šç§æ•°æ®å’Œæ¨¡å‹ã€PDFè¡¨æ ¼æ•°æ®æå–å·¥å…· ã€ Graphbrainï¼šAIå¼€æºè½¯ä»¶åº“å’Œç§‘ç ”å·¥å…·ï¼Œç›®çš„æ˜¯ä¿ƒè¿›è‡ªåŠ¨æ„ä¹‰æå–å’Œæ–‡æœ¬ç†è§£ä»¥åŠçŸ¥è¯†çš„æ¢ç´¢å’Œæ¨æ–­ã€ç®€å†è‡ªåŠ¨ç­›é€‰ç³»ç»Ÿã€åŸºäºå‘½åå®ä½“è¯†åˆ«çš„ç®€å†è‡ªåŠ¨æ‘˜è¦ã€ä¸­æ–‡è¯­è¨€ç†è§£æµ‹è¯„åŸºå‡†ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§çš„æ•°æ®é›†&åŸºå‡†æ¨¡å‹&è¯­æ–™åº“&æ’è¡Œæ¦œã€æ ‘æ´ OCR æ–‡å­—è¯†åˆ« ã€ä»åŒ…å«è¡¨æ ¼çš„æ‰«æå›¾ç‰‡ä¸­è¯†åˆ«è¡¨æ ¼å’Œæ–‡å­—ã€è¯­å£°è¿ç§»ã€Pythonå£è¯­è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·é›†(è‹±æ–‡)ã€ similarityï¼šç›¸ä¼¼åº¦è®¡ç®—å·¥å…·åŒ…ï¼Œjavaç¼–å†™ã€æµ·é‡ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹ ã€Transformers 2.0 ã€åŸºäºå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†Audiosetçš„éŸ³é¢‘å¢å¼º ã€Poplarï¼šç½‘é¡µç‰ˆè‡ªç„¶è¯­è¨€æ ‡æ³¨å·¥å…·ã€å›¾ç‰‡æ–‡å­—å»é™¤ï¼Œå¯ç”¨äºæ¼«ç”»ç¿»è¯‘ ã€186ç§è¯­è¨€çš„æ•°å­—å«æ³•åº“ã€Amazonå‘å¸ƒåŸºäºçŸ¥è¯†çš„äºº-äººå¼€æ”¾é¢†åŸŸå¯¹è¯æ•°æ®é›† ã€ä¸­æ–‡æ–‡æœ¬çº é”™æ¨¡å—ä»£ç ã€ç¹ç®€ä½“è½¬æ¢ ã€ Pythonå®ç°çš„å¤šç§æ–‡æœ¬å¯è¯»æ€§è¯„ä»·æŒ‡æ ‡ã€ç±»ä¼¼äºäººå/åœ°å/ç»„ç»‡æœºæ„åçš„å‘½åä½“è¯†åˆ«æ•°æ®é›† ã€ä¸œå—å¤§å­¦ã€ŠçŸ¥è¯†å›¾è°±ã€‹ç ”ç©¶ç”Ÿè¯¾ç¨‹(èµ„æ–™)ã€. è‹±æ–‡æ‹¼å†™æ£€æŸ¥åº“ ã€ wwsearchæ˜¯ä¼ä¸šå¾®ä¿¡åå°è‡ªç ”çš„å…¨æ–‡æ£€ç´¢å¼•æ“ã€CHAMELEONï¼šæ·±åº¦å­¦ä¹ æ–°é—»æ¨èç³»ç»Ÿå…ƒæ¶æ„ ã€ 8ç¯‡è®ºæ–‡æ¢³ç†BERTç›¸å…³æ¨¡å‹è¿›å±•ä¸åæ€ã€DocSearchï¼šå…è´¹æ–‡æ¡£æœç´¢å¼•æ“ã€ LIDAï¼šè½»é‡äº¤äº’å¼å¯¹è¯æ ‡æ³¨å·¥å…· ã€aili - the fastest in-memory index in the East ä¸œåŠçƒæœ€å¿«å¹¶å‘ç´¢å¼• ã€çŸ¥è¯†å›¾è°±è½¦éŸ³å·¥ä½œé¡¹ç›®ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆèµ„æºå¤§å…¨ ã€ä¸­æ—¥éŸ©åˆ†è¯åº“mecabçš„Pythonæ¥å£åº“ã€ä¸­æ–‡æ–‡æœ¬æ‘˜è¦/å…³é”®è¯æå–ã€æ±‰å­—å­—ç¬¦ç‰¹å¾æå–å™¨ (featurizer)ï¼Œæå–æ±‰å­—çš„ç‰¹å¾ï¼ˆå‘éŸ³ç‰¹å¾ã€å­—å½¢ç‰¹å¾ï¼‰ç”¨åšæ·±åº¦å­¦ä¹ çš„ç‰¹å¾ã€ä¸­æ–‡ç”Ÿæˆä»»åŠ¡åŸºå‡†æµ‹è¯„ ã€ä¸­æ–‡ç¼©å†™æ•°æ®é›†ã€ä¸­æ–‡ä»»åŠ¡åŸºå‡†æµ‹è¯„ - ä»£è¡¨æ€§çš„æ•°æ®é›†-åŸºå‡†(é¢„è®­ç»ƒ)æ¨¡å‹-è¯­æ–™åº“-baseline-å·¥å…·åŒ…-æ’è¡Œæ¦œã€PySS3ï¼šé¢å‘å¯è§£é‡ŠAIçš„SS3æ–‡æœ¬åˆ†ç±»å™¨æœºå™¨å¯è§†åŒ–å·¥å…· ã€ä¸­æ–‡NLPæ•°æ®é›†åˆ—è¡¨ã€COPE - æ ¼å¾‹è¯—ç¼–è¾‘ç¨‹åºã€doccanoï¼šåŸºäºç½‘é¡µçš„å¼€æºååŒå¤šè¯­è¨€æ–‡æœ¬æ ‡æ³¨å·¥å…· ã€PreNLPï¼šè‡ªç„¶è¯­è¨€é¢„å¤„ç†åº“ã€ç®€å•çš„ç®€å†è§£æå™¨ï¼Œç”¨æ¥ä»ç®€å†ä¸­æå–å…³é”®ä¿¡æ¯ã€ç”¨äºä¸­æ–‡é—²èŠçš„GPT2æ¨¡å‹ï¼šGPT2-chitchatã€åŸºäºæ£€ç´¢èŠå¤©æœºå™¨äººå¤šè½®å“åº”é€‰æ‹©ç›¸å…³èµ„æºåˆ—è¡¨(Leaderboardsã€Datasetsã€Papers)ã€(Colab)æŠ½è±¡æ–‡æœ¬æ‘˜è¦å®ç°é›†é”¦(æ•™ç¨‹ ã€è¯è¯­æ‹¼éŸ³æ•°æ®ã€é«˜æ•ˆæ¨¡ç³Šæœç´¢å·¥å…·ã€NLPæ•°æ®å¢å¹¿èµ„æºé›†ã€å¾®è½¯å¯¹è¯æœºå™¨äººæ¡†æ¶ ã€ GitHub Typo Corpusï¼šå¤§è§„æ¨¡GitHubå¤šè¯­è¨€æ‹¼å†™é”™è¯¯/è¯­æ³•é”™è¯¯æ•°æ®é›†ã€TextClusterï¼šçŸ­æ–‡æœ¬èšç±»é¢„å¤„ç†æ¨¡å— Short text clusterã€é¢å‘è¯­éŸ³è¯†åˆ«çš„ä¸­æ–‡æ–‡æœ¬è§„èŒƒåŒ–ã€BLINKï¼šæœ€å…ˆè¿›çš„å®ä½“é“¾æ¥åº“ã€BertPuncï¼šåŸºäºBERTçš„æœ€å…ˆè¿›æ ‡ç‚¹ä¿®å¤æ¨¡å‹ã€Tokenizerï¼šå¿«é€Ÿã€å¯å®šåˆ¶çš„æ–‡æœ¬è¯æ¡åŒ–åº“ã€ä¸­æ–‡è¯­è¨€ç†è§£æµ‹è¯„åŸºå‡†ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§çš„æ•°æ®é›†ã€åŸºå‡†(é¢„è®­ç»ƒ)æ¨¡å‹ã€è¯­æ–™åº“ã€æ’è¡Œæ¦œã€spaCy åŒ»å­¦æ–‡æœ¬æŒ–æ˜ä¸ä¿¡æ¯æå– ã€ NLPä»»åŠ¡ç¤ºä¾‹é¡¹ç›®ä»£ç é›†ã€ pythonæ‹¼å†™æ£€æŸ¥åº“ã€chatbot-list - è¡Œä¸šå†…å…³äºæ™ºèƒ½å®¢æœã€èŠå¤©æœºå™¨äººçš„åº”ç”¨å’Œæ¶æ„ã€ç®—æ³•åˆ†äº«å’Œä»‹ç»ã€è¯­éŸ³è´¨é‡è¯„ä»·æŒ‡æ ‡(MOSNet, BSSEval, STOI, PESQ, SRMR)ã€ ç”¨138GBè¯­æ–™è®­ç»ƒçš„æ³•æ–‡RoBERTaé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ ã€BERT-NER-Pytorchï¼šä¸‰ç§ä¸åŒæ¨¡å¼çš„BERTä¸­æ–‡NERå®éªŒã€æ— é“è¯å…¸ - æœ‰é“è¯å…¸çš„å‘½ä»¤è¡Œç‰ˆæœ¬ï¼Œæ”¯æŒè‹±æ±‰äº’æŸ¥å’Œåœ¨çº¿æŸ¥è¯¢ã€2019å¹´NLPäº®ç‚¹å›é¡¾ã€ Chinese medical dialogue data ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›† ã€æœ€å¥½çš„æ±‰å­—æ•°å­—(ä¸­æ–‡æ•°å­—)-é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢å·¥å…·ã€ åŸºäºç™¾ç§‘çŸ¥è¯†åº“çš„ä¸­æ–‡è¯è¯­å¤šè¯ä¹‰/ä¹‰é¡¹è·å–ä¸ç‰¹å®šå¥å­è¯è¯­è¯­ä¹‰æ¶ˆæ­§ã€awesome-nlp-sentiment-analysis - æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªåŸå› è¯†åˆ«ã€è¯„ä»·å¯¹è±¡å’Œè¯„ä»·è¯æŠ½å–ã€LineFlowï¼šé¢å‘æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶çš„NLPæ•°æ®é«˜æ•ˆåŠ è½½å™¨ã€ä¸­æ–‡åŒ»å­¦NLPå…¬å¼€èµ„æºæ•´ç† ã€MedQuADï¼š(è‹±æ–‡)åŒ»å­¦é—®ç­”æ•°æ®é›†ã€å°†è‡ªç„¶è¯­è¨€æ•°å­—ä¸²è§£æè½¬æ¢ä¸ºæ•´æ•°å’Œæµ®ç‚¹æ•°ã€Transfer Learning in Natural Language Processing (NLP) ã€é¢å‘è¯­éŸ³è¯†åˆ«çš„ä¸­æ–‡/è‹±æ–‡å‘éŸ³è¾å…¸ã€Tokenizersï¼šæ³¨é‡æ€§èƒ½ä¸å¤šåŠŸèƒ½æ€§çš„æœ€å…ˆè¿›åˆ†è¯å™¨ã€CLUENER ç»†ç²’åº¦å‘½åå®ä½“è¯†åˆ« Fine Grained Named Entity Recognitionã€ åŸºäºBERTçš„ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ã€ä¸­æ–‡è°£è¨€æ•°æ®åº“ã€NLPæ•°æ®é›†/åŸºå‡†ä»»åŠ¡å¤§åˆ—è¡¨ã€nlpç›¸å…³çš„ä¸€äº›è®ºæ–‡åŠä»£ç , åŒ…æ‹¬ä¸»é¢˜æ¨¡å‹ã€è¯å‘é‡(Word Embedding)ã€å‘½åå®ä½“è¯†åˆ«(NER)ã€æ–‡æœ¬åˆ†ç±»(Text Classificatin)ã€æ–‡æœ¬ç”Ÿæˆ(Text Generation)ã€æ–‡æœ¬ç›¸ä¼¼æ€§(Text Similarity)è®¡ç®—ç­‰ï¼Œæ¶‰åŠåˆ°å„ç§ä¸nlpç›¸å…³çš„ç®—æ³•ï¼ŒåŸºäºkeraså’Œtensorflow ã€Pythonæ–‡æœ¬æŒ–æ˜/NLPå®æˆ˜ç¤ºä¾‹ã€ Blackstoneï¼šé¢å‘éç»“æ„åŒ–æ³•å¾‹æ–‡æœ¬çš„spaCy pipelineå’ŒNLPæ¨¡å‹é€šè¿‡åŒä¹‰è¯æ›¿æ¢å®ç°æ–‡æœ¬â€œå˜è„¸â€ ã€ä¸­æ–‡ é¢„è®­ç»ƒ ELECTREA æ¨¡å‹: åŸºäºå¯¹æŠ—å­¦ä¹  pretrain Chinese Model ã€albert-chinese-ner - ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ALBERTåšä¸­æ–‡NER ã€åŸºäºGPT2çš„ç‰¹å®šä¸»é¢˜æ–‡æœ¬ç”Ÿæˆ/æ–‡æœ¬å¢å¹¿ã€å¼€æºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åˆé›†ã€å¤šè¯­è¨€å¥å‘é‡åŒ…ã€ç¼–ç ã€æ ‡è®°å’Œå®ç°ï¼šä¸€ç§å¯æ§é«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ã€ è‹±æ–‡è„è¯å¤§åˆ—è¡¨ ã€attnvisï¼šGPT2ã€BERTç­‰transformerè¯­è¨€æ¨¡å‹æ³¨æ„åŠ›äº¤äº’å¯è§†åŒ–ã€CoVoSTï¼šFacebookå‘å¸ƒçš„å¤šè¯­ç§è¯­éŸ³-æ–‡æœ¬ç¿»è¯‘è¯­æ–™åº“ï¼ŒåŒ…æ‹¬11ç§è¯­è¨€(æ³•è¯­ã€å¾·è¯­ã€è·å…°è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ã€åœŸè€³å…¶è¯­ã€æ³¢æ–¯è¯­ã€ç‘å…¸è¯­ã€è’™å¤è¯­å’Œä¸­æ–‡)çš„è¯­éŸ³ã€æ–‡å­—è½¬å½•åŠè‹±æ–‡è¯‘æ–‡ã€Jiaguè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…· - ä»¥BiLSTMç­‰æ¨¡å‹ä¸ºåŸºç¡€ï¼Œæä¾›çŸ¥è¯†å›¾è°±å…³ç³»æŠ½å– ä¸­æ–‡åˆ†è¯ è¯æ€§æ ‡æ³¨ å‘½åå®ä½“è¯†åˆ« æƒ…æ„Ÿåˆ†æ æ–°è¯å‘ç° å…³é”®è¯ æ–‡æœ¬æ‘˜è¦ æ–‡æœ¬èšç±»ç­‰åŠŸèƒ½ã€ç”¨unetå®ç°å¯¹æ–‡æ¡£è¡¨æ ¼çš„è‡ªåŠ¨æ£€æµ‹ï¼Œè¡¨æ ¼é‡å»ºã€NLPäº‹ä»¶æå–æ–‡çŒ®èµ„æºåˆ—è¡¨ ã€ é‡‘èé¢†åŸŸè‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶èµ„æºå¤§åˆ—è¡¨ã€CLUEDatasetSearch - ä¸­è‹±æ–‡NLPæ•°æ®é›†ï¼šæœç´¢æ‰€æœ‰ä¸­æ–‡NLPæ•°æ®é›†ï¼Œé™„å¸¸ç”¨è‹±æ–‡NLPæ•°æ®é›† ã€medical_NER - ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±å‘½åå®ä½“è¯†åˆ« ã€(å“ˆä½›)è®²å› æœæ¨ç†çš„å…è´¹ä¹¦ã€çŸ¥è¯†å›¾è°±ç›¸å…³å­¦ä¹ èµ„æ–™/æ•°æ®é›†/å·¥å…·èµ„æºå¤§åˆ—è¡¨ã€Forteï¼šçµæ´»å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†pipelineå·¥å…·é›† ã€Pythonå­—ç¬¦ä¸²ç›¸ä¼¼æ€§ç®—æ³•åº“ã€PyLaiaï¼šé¢å‘æ‰‹å†™æ–‡æ¡£åˆ†æçš„æ·±åº¦å­¦ä¹ å·¥å…·åŒ…ã€TextFoolerï¼šé’ˆå¯¹æ–‡æœ¬åˆ†ç±»/æ¨ç†çš„å¯¹æŠ—æ–‡æœ¬ç”Ÿæˆæ¨¡å—ã€Haystackï¼šçµæ´»ã€å¼ºå¤§çš„å¯æ‰©å±•é—®ç­”(QA)æ¡†æ¶ã€ä¸­æ–‡å…³é”®çŸ­è¯­æŠ½å–å·¥å…·",
      "url": "https://github.com/fighting41love/funNLP",
      "published_date": "2018-08-21T11:20:39Z",
      "stars": 74748,
      "forks": 14912,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "medical",
        "Machine Learning",
        "nlp",
        "NLP",
        "ML"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "fighting41love"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "torchio",
      "content": "Medical imaging processing for AI applications.",
      "url": "https://github.com/TorchIO-project/torchio",
      "published_date": "2019-11-26T09:10:09Z",
      "stars": 2235,
      "forks": 248,
      "language": "Python",
      "entities": [
        "Medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "TorchIO-project"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "nitrain",
      "content": "Train AI models efficiently on medical images using any framework",
      "url": "https://github.com/nitrain/nitrain",
      "published_date": "2017-03-01T02:42:12Z",
      "stars": 1871,
      "forks": 303,
      "language": "Python",
      "entities": [
        "medical",
        "Train AI",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "nitrain"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "paperai",
      "content": "ğŸ“„ ğŸ¤– AI for medical and scientific papers",
      "url": "https://github.com/neuml/paperai",
      "published_date": "2020-07-21T18:33:30Z",
      "stars": 1442,
      "forks": 112,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "neuml"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AnkiAIUtils",
      "content": "AI-powered tools to enhance Anki flashcards with explanations, mnemonics, illustrations, and adaptive learning for medical school and beyond",
      "url": "https://github.com/thiswillbeyourgithub/AnkiAIUtils",
      "published_date": "2024-03-29T14:40:31Z",
      "stars": 763,
      "forks": 26,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "thiswillbeyourgithub"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "INSIGHT",
      "content": "INSIGHT is an autonomous AI that can do medical research!",
      "url": "https://github.com/oneil512/INSIGHT",
      "published_date": "2023-04-08T15:20:14Z",
      "stars": 409,
      "forks": 58,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "oneil512"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "annotateai",
      "content": "ğŸ“ Automatically annotate papers using LLMs ",
      "url": "https://github.com/neuml/annotateai",
      "published_date": "2024-12-12T15:51:02Z",
      "stars": 332,
      "forks": 33,
      "language": "Python",
      "entities": [],
      "keywords": [
        "medical AI"
      ],
      "owner": "neuml"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "hi-ml",
      "content": "HI-ML toolbox for deep learning for medical imaging and Azure integration",
      "url": "https://github.com/microsoft/hi-ml",
      "published_date": "2021-07-01T11:03:52Z",
      "stars": 291,
      "forks": 59,
      "language": "Python",
      "entities": [
        "medical",
        "deep learning",
        "ML"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "microsoft"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AIDoctor",
      "content": "AIDoctor training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining, Supervised Finetuning, RLHF(Reward Modeling and Reinforcement Learning) and DPO(Direct Preferencâ€¦",
      "url": "https://github.com/Jerry-XDL/AIDoctor",
      "published_date": "2025-02-23T08:17:56Z",
      "stars": 274,
      "forks": 22,
      "language": "Python",
      "entities": [
        "medical"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "Jerry-XDL"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "kaapana",
      "content": "Kaapana is an open source toolkit for state of the art platform provisioning in the field of medical data analysis. The applications comprise AI-based workflows and federated learning scenarios with a focus on radiological and radiotherapeutic imaging. The name Kaapana comes from the Hawaiian word kaÊ»Äpana, meaning \"distributor\" or \"part\".",
      "url": "https://github.com/kaapana/kaapana",
      "published_date": "2020-08-06T09:28:22Z",
      "stars": 213,
      "forks": 51,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "kaapana"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "MONAI",
      "content": "AI Toolkit for Healthcare Imaging",
      "url": "https://github.com/Project-MONAI/MONAI",
      "published_date": "2019-10-11T16:41:38Z",
      "stars": 6596,
      "forks": 1217,
      "language": "Python",
      "entities": [
        "Healthcare",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "Project-MONAI"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
      "content": "A comprehensive list of Deep Learning / Artificial Intelligence and Machine Learning tutorials - rapidly expanding into areas of AI/Deep Learning / Machine Vision / NLP and industry specific areas such as Climate / Energy, Automotives, Retail, Pharma, Medicine, Healthcare, Policy, Ethics and more.",
      "url": "https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
      "published_date": "2017-07-13T19:46:01Z",
      "stars": 3883,
      "forks": 1633,
      "language": "Python",
      "entities": [
        "Machine Learning",
        "Deep Learning",
        "Healthcare",
        "Medicine",
        "NLP",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "TarrySingh"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "hi-ml",
      "content": "HI-ML toolbox for deep learning for medical imaging and Azure integration",
      "url": "https://github.com/microsoft/hi-ml",
      "published_date": "2021-07-01T11:03:52Z",
      "stars": 291,
      "forks": 59,
      "language": "Python",
      "entities": [
        "medical",
        "deep learning",
        "ML"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "microsoft"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "fuse-med-ml",
      "content": "A python framework accelerating ML based discovery in the medical field by encouraging code reuse. Batteries included :)",
      "url": "https://github.com/BiomedSciAI/fuse-med-ml",
      "published_date": "2021-06-23T12:10:42Z",
      "stars": 148,
      "forks": 36,
      "language": "Python",
      "entities": [
        "medical",
        "ML"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "BiomedSciAI"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "medicare_locator",
      "content": "ğŸ¥Medicare Locator - Open source starter pack for developers to build contextual chatbots and AI assistants in healthcare",
      "url": "https://github.com/RasaHQ/medicare_locator",
      "published_date": "2018-11-21T14:47:45Z",
      "stars": 147,
      "forks": 136,
      "language": "Python",
      "entities": [
        "healthcare",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "RasaHQ"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "HealthChatbot",
      "content": "ğŸ¤– HealthCare ChatBot Major -1 (4th year - 7th semester)  Health Care Chat-Bot is a Healthcare Domain Chatbot to simulate the predictions of a General Physician.  ChatBot can be described as software that can chat with people using artificial intelligence. These software are used to perform tasks such as quickly responding to users, informing them, helping to purchase products and providing better service to customers. We have made a healthcare based chatbot.  The three main areas where chatbots can be used are diagnostics, patient engagement outside medical facilities, and mental health. In our major we are working on diagnostic.  ğŸ“ƒ Brief A chatbot is an artificially intelligent creature which can converse with humans. This could be text-based, or a spoken conversation. In our project we will be using Python as it is currently the most popular language for creating an AI chatbot. In the middle of AI chatbot, architecture is the Natural Language Processing (NLP) layer.  This project aims to build an user-friendly healthcare chatbot which facilitates the job of a healthcare provider and helps improve their performance by interacting with users in a human-like way.  Through chatbots one can communicate with text or voice interface and get reply through artificial intelligence  Typically, a chat bot will communicate with a real person. Chat bots are used in applications such as E-commerce customer service, Call centres, Internet gaming,etc.  Chatbots are programs built to automatically engage with received messages. Chatbots can be programmed to respond the same way each time, to respond differently to messages containing certain keywords and even to use machine learning to adapt their responses to fit the situation.  A developing number of hospitals, nursing homes, and even private centres, presently utilize online Chatbots for human services on their sites. These bots connect with potential patients visiting the site, helping them discover specialists, booking their appointments, and getting them access to the correct treatment.  In any case, the utilization of artificial intelligence in an industry where individualsâ€™ lives could be in question, still starts misgivings in individuals. It brings up issues about whether the task mentioned above ought to be assigned to human staff. This healthcare chatbot system will help hospitals to provide healthcare support online 24 x 7, it answers deep as well as general questions. It also helps to generate leads and automatically delivers the information of leads to sales. By asking the questions in series it helps patients by guiding what exactly he/she is looking for.  ğŸ“œ Problem Statement During the pandemic, it is more important than ever to get your regular check-ups and to continue to take prescription medications. The healthier you are, the more likely you are to recover quickly from an illness.  In this time patients or health care workers within their practice, providers are deferring elective and preventive visits, such as annual physicals. For some, it is not possible to consult online. In this case, to avoid false information, our project can be of help.  ğŸ“‡ Features Register Screen. Sign-in Screen. Generates database for user login system. Offers you a GUI Based Chatbot for patients for diagnosing. [A pragmatic Approach for Diagnosis] Reccomends an appropriate doctor to you for the following symptom. ğŸ“œ Modules Used Our program uses a number of python modules to work properly:  tkinter os webbrowser numpy pandas matplotlib ğŸ“ƒ Algorithm We have used Decision tree for our health care based chat bot.  Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.It usually mimic human thinking ability while making a decision, so it is easy to understand.  :suspect: Project Members Anushka Bansal - 500067844 - R164218014 Shreya Sharma - 500068573 - R164218070 Silvi - 500069092 - R164218072 Ishika Agrawal - 500071154 - R164218097",
      "url": "https://github.com/shreyasharma04/HealthChatbot",
      "published_date": "2022-02-21T08:27:25Z",
      "stars": 113,
      "forks": 20,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "machine learning",
        "medical",
        "healthcare",
        "Healthcare",
        "Diagnosis",
        "symptom",
        "NLP",
        "patient",
        "treatment",
        "HealthCare",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "shreyasharma04"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI-Healthcare-chatbot",
      "content": "Through a series of questions about symptoms it diagnosis the health condition of patient. ",
      "url": "https://github.com/vsharathchandra/AI-Healthcare-chatbot",
      "published_date": "2018-03-20T16:52:42Z",
      "stars": 86,
      "forks": 64,
      "language": "Python",
      "entities": [
        "diagnosis",
        "patient"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "vsharathchandra"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "cyclops",
      "content": "A toolkit for evaluating and monitoring AI models in clinical settings",
      "url": "https://github.com/VectorInstitute/cyclops",
      "published_date": "2022-02-21T21:15:08Z",
      "stars": 85,
      "forks": 13,
      "language": "Python",
      "entities": [
        "clinical",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "VectorInstitute"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "EEGWaveNet",
      "content": "source codes for EEGWaveNet: Multi-Scale CNN-Based Spatiotemporal Feature Extraction for EEG Seizure Detection (IEEE Transactions on Industrial Informatics)",
      "url": "https://github.com/IoBT-VISTEC/EEGWaveNet",
      "published_date": "2021-05-17T12:06:49Z",
      "stars": 57,
      "forks": 7,
      "language": "Python",
      "entities": [],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "IoBT-VISTEC"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "fedbiomed",
      "content": "A collaborative learning framework for empowering biomedical research",
      "url": "https://github.com/fedbiomed/fedbiomed",
      "published_date": "2023-05-30T06:23:36Z",
      "stars": 54,
      "forks": 10,
      "language": "Python",
      "entities": [],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "fedbiomed"
    }
  ],
  "research_papers": [
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
      "content": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
      "url": "http://arxiv.org/abs/2507.07996v1",
      "published_date": "2025-07-10",
      "entities": [
        "neural network"
      ],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07996v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
      "content": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
      "url": "http://arxiv.org/abs/2507.07984v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07984v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
      "content": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
      "url": "http://arxiv.org/abs/2507.07982v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07982v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Pierce-Birkhoff conjecture is true for splines",
      "content": "We prove the Pierce--Birkhoff conjecture for splines, i.e., continuous\npiecewise polynomials of degree $d$ in $n$ variables on a hyperplane partition\nof $\\mathbb{R}^n$, can be written as a finite lattice combination of\npolynomials. We will provide a purely existential proof, followed by a more\nin-depth analysis that yields effective bounds.",
      "url": "http://arxiv.org/abs/2507.07976v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07976v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Constraints from CMB lensing tomography with projected bispectra",
      "content": "We measure the angular power spectrum and bispectrum of the projected\noverdensity of photometric DESI luminous red galaxies, and its\ncross-correlation with maps of the Cosmic Microwave Background lensing\nconvergence from \\planck. This analysis is enabled by the use of the\n``filtered-squared bispectrum'' approach, introduced in previous work, which we\ngeneralise here to the case of cross-correlations between multiple fields. The\nprojected galaxy bispectrum is detected at very high significance (above\n$30\\sigma$ in all redshift bins), and the galaxy-galaxy-convergence bispectrum\nis detected above $5\\sigma$ in the three highest-redshift bins. We find that\nthe bispectrum is reasonably well described over a broad range of scales by a\ntree-level prediction using the linear galaxy bias measured from the power\nspectrum. We carry out the first cosmological analysis combining projected\npower spectra and bispectra under a relatively simple model, and show that the\ngalaxy bispectrum can be used in combination with the power spectrum to place a\nconstraint on the amplitude of matter fluctuations, $\\sigma_8$, an on the\nnon-relativistic matter fraction $\\Omega_m$. We find that data combinations\ninvolving the galaxy bispectrum recover constraints on these parameters that\nare in good agreement with those found from the traditional\n``2$\\times$2-point'' combination of galaxy-galaxy and galaxy-convergence power\nspectra, across all redshift bins.",
      "url": "http://arxiv.org/abs/2507.07968v1",
      "published_date": "2025-07-10",
      "entities": [
        "traditional"
      ],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07968v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Sharp estimates of quantum covering problems via a novel trace\n  inequality",
      "content": "In this paper, we prove a novel trace inequality involving two operators. As\napplications, we sharpen the one-shot achievability bound on the relative\nentropy error in a wealth of quantum covering-type problems, such as soft\ncovering, privacy amplification, convex splitting, quantum information\ndecoupling, and quantum channel simulation by removing some dimension-dependent\nfactors. Moreover, the established one-shot bounds extend to\ninfinite-dimensional separable Hilbert spaces as well. The proof techniques are\nbased on the recently developed operator layer cake theorem and an operator\nchange-of-variable argument, which are of independent interest.",
      "url": "http://arxiv.org/abs/2507.07961v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07961v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Constructing Optimal Kobon Triangle Arrangements via Table Encoding, SAT\n  Solving, and Heuristic Straightening",
      "content": "We present new methods and results for constructing optimal Kobon triangle\narrangements. First, we introduce a compact table notation for describing\narrangements of pseudolines, enabling the representation and analysis of\ncomplex cases, including symmetrical arrangements, arrangements with parallel\nlines, and arrangements with multiple-line intersection points. Building on\nthis, we provide a simple heuristic method and tools for recovering\nstraight-line arrangements from a given table, with the ability to enforce\nadditional properties such as symmetries. The tool successfully recovers\narrangements for many previously known optimal solutions. Additionally, we\ndevelop a tool that transforms the search for optimal Kobon arrangement tables\ninto a SAT problem, allowing us to leverage modern SAT solvers (specifically\nKissat) to efficiently find new solutions or to show that no other solutions\nexist (for example, confirming that no optimal solution exists in the 11-line\ncase). Using these techniques, we find new optimal Kobon arrangements for 23\nand 27 lines, along with several other new results.",
      "url": "http://arxiv.org/abs/2507.07951v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07951v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Quantum Wall States for Noise Mitigation and Eternal Purity Bounds",
      "content": "The present work analyzes state-stabilization techniques for decoupling a\nsubsystem from environmental interactions. The proposed framework uses\nanalytical and numerical tools to find an approximate decoherence-free subspace\n(DFS) with enhanced passive noise isolation. Active state-stabilizing control\non a subsystem mediating dominant environmental interactions, which we call\nwall subsystem, creates an effective quantum wall state. The proposed method\ncontrols only the wall subsystem, leaving the logical subsystem untouched. This\nsimplifies logic operations in the protected subsystem, and makes it suitable\nfor integration with other quantum information protection techniques, such as\ndynamical decoupling (DD). We demonstrated its effectiveness in enhancing the\nperformance of selective or complete DD. Under suitable conditions, our method\nmaintains system purity above a threshold for all times, achieving eternal\npurity preservation. Theoretical analysis links this behavior to the asymptotic\nspectrum of the Hamiltonian when the control gain grows unbounded.",
      "url": "http://arxiv.org/abs/2507.07944v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07944v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "A Randomized Rounding Approach for DAG Edge Deletion",
      "content": "In the DAG Edge Deletion problem, we are given an edge-weighted directed\nacyclic graph and a parameter $k$, and the goal is to delete the minimum weight\nset of edges so that the resulting graph has no paths of length $k$. This\nproblem, which has applications to scheduling, was introduced in 2015 by\nKenkre, Pandit, Purohit, and Saket. They gave a $k$-approximation and showed\nthat it is UGC-Hard to approximate better than $\\lfloor 0.5k \\rfloor$ for any\nconstant $k \\ge 4$ using a work of Svensson from 2012. The approximation ratio\nwas improved to $\\frac{2}{3}(k+1)$ by Klein and Wexler in 2016.\n  In this work, we introduce a randomized rounding framework based on\ndistributions over vertex labels in $[0,1]$. The most natural distribution is\nto sample labels independently from the uniform distribution over $[0,1]$. We\nshow this leads to a $(2-\\sqrt{2})(k+1) \\approx 0.585(k+1)$-approximation. By\nusing a modified (but still independent) label distribution, we obtain a\n$0.549(k+1)$-approximation for the problem, as well as show that no independent\ndistribution over labels can improve our analysis to below $0.542(k+1)$.\nFinally, we show a $0.5(k+1)$-approximation for bipartite graphs and for\ninstances with structured LP solutions. Whether this ratio can be obtained in\ngeneral is open.",
      "url": "http://arxiv.org/abs/2507.07943v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07943v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Late Fusion Multi-task Learning for Semiparametric Inference with\n  Nuisance Parameters",
      "content": "In the age of large and heterogeneous datasets, the integration of\ninformation from diverse sources is essential to improve parameter estimation.\nMulti-task learning offers a powerful approach by enabling simultaneous\nlearning across related tasks. In this work, we introduce a late fusion\nframework for multi-task learning with semiparametric models that involve\ninfinite-dimensional nuisance parameters, focusing on applications such as\nheterogeneous treatment effect estimation across multiple data sources,\nincluding electronic health records from different hospitals or clinical trial\ndata. Our framework is two-step: first, initial double machine-learning\nestimators are obtained through individual task learning; second, these\nestimators are adaptively aggregated to exploit task similarities while\nremaining robust to task-specific differences. In particular, the framework\navoids individual level data sharing, preserving privacy. Additionally, we\npropose a novel multi-task learning method for nuisance parameter estimation,\nwhich further enhances parameter estimation when nuisance parameters exhibit\nsimilarity across tasks. We establish theoretical guarantees for the method,\ndemonstrating faster convergence rates compared to individual task learning\nwhen tasks share similar parametric components. Extensive simulations and real\ndata applications complement the theoretical findings of our work while\nhighlight the effectiveness of our framework even in moderate sample sizes.",
      "url": "http://arxiv.org/abs/2507.07941v1",
      "published_date": "2025-07-10",
      "entities": [
        "clinical",
        "treatment"
      ],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07941v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
      "content": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
      "url": "http://arxiv.org/abs/2507.07957v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07957v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Intraseasonal Equatorial Kelvin and Rossby Waves in Modern AI-ML Models",
      "content": "We examine the structure of large-scale convectively coupled Kelvin and\nRossby waves in a suite of modern AI-ML models. In particular, multiple runs of\nPanguWeather, GraphCast, FourCastNet and Aurora are performed to assess the\nstructure of the aforementioned waves. Wavenumber-frequency diagrams of zonal\nwinds from all models show a clear signature of Rossby and Kelvin waves with\nequivalent depths that are in accord with observations and reanalysis.\nComposites of Kelvin waves show correct lower and upper troposphere horizontal\nconvergence patterns, vertical tilts in temperature, humidity and vertical\nvelocity as well as the phase relation between temperature and vertical\nvelocity anomalies. Though, differences between models are notable such as\nsmaller vertical tilts and incorrect surface temperature anomalies in GraphCast\nand relatively weak convergent flows in PanguWeather. The models had much more\ndifficulty with Rossby waves; while the horizontal gyres were captured, the\nvertical structure of temperature and divergence was incorrect. Apart from\nunexpected tilts in various fields, the temperature anomaly was inconsistent\nwith the nature of the vertical velocity in all four models. Curiously,\nmoisture and vertical velocity anomalies were much closer to observations.\nFurther, only two models (GraphCast and FourCastNet) captured the simultaneous\ngeneration of deep vertical motion with moisture anomalies. In all, while the\nrepresentation of these large-scale waves is encouraging, issues with the\nstructure of Rossby waves and especially the inconsistency among fields require\nfurther investigation.",
      "url": "http://arxiv.org/abs/2507.07952v1",
      "published_date": "2025-07-10",
      "entities": [
        "ML",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07952v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Multimodal Framework for Explainable Autonomous Driving: Integrating\n  Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency",
      "content": "Autonomous vehicles (AVs) are poised to redefine transportation by enhancing\nroad safety, minimizing human error, and optimizing traffic efficiency. The\nsuccess of AVs depends on their ability to interpret complex, dynamic\nenvironments through diverse data sources, including video streams, sensor\nmeasurements, and contextual textual information. However, seamlessly\nintegrating these multimodal inputs and ensuring transparency in AI-driven\ndecisions remain formidable challenges. This study introduces a novel\nmultimodal framework that synergistically combines video, sensor, and textual\ndata to predict driving actions while generating human-readable explanations,\nfostering trust and regulatory compliance. By leveraging VideoMAE for\nspatiotemporal video analysis, a custom sensor fusion module for real-time data\nprocessing, and BERT for textual comprehension, our approach achieves robust\ndecision-making and interpretable outputs. Evaluated on the BDD-X (21113\nsamples) and nuScenes (1000 scenes) datasets, our model reduces training loss\nfrom 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy\nof 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming\nstate-of-the-art methods. Ablation studies confirm the critical role of each\nmodality, while qualitative analyses and human evaluations highlight the\nmodel's ability to produce contextually rich, user-friendly explanations. These\nadvancements underscore the transformative potential of multimodal integration\nand explainability in building safe, transparent, and trustworthy AV systems,\npaving the way for broader societal adoption of autonomous driving\ntechnologies.",
      "url": "http://arxiv.org/abs/2507.07938v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07938v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Working with AI: Measuring the Occupational Implications of Generative\n  AI",
      "content": "Given the rapid adoption of generative AI and its potential to impact a wide\nrange of tasks, understanding the effects of AI on the economy is one of\nsociety's most important questions. In this work, we take a step toward that\ngoal by analyzing the work activities people do with AI, how successfully and\nbroadly those activities are done, and combine that with data on what\noccupations do those activities. We analyze a dataset of 200k anonymized and\nprivacy-scrubbed conversations between users and Microsoft Bing Copilot, a\npublicly available generative AI system. We find the most common work\nactivities people seek AI assistance for involve gathering information and\nwriting, while the most common activities that AI itself is performing are\nproviding information and assistance, writing, teaching, and advising.\nCombining these activity classifications with measurements of task success and\nscope of impact, we compute an AI applicability score for each occupation. We\nfind the highest AI applicability scores for knowledge work occupation groups\nsuch as computer and mathematical, and office and administrative support, as\nwell as occupations such as sales whose work activities involve providing and\ncommunicating information. Additionally, we characterize the types of work\nactivities performed most successfully, how wage and education correlate with\nAI applicability, and how real-world usage compares to predictions of\noccupational AI impact.",
      "url": "http://arxiv.org/abs/2507.07935v1",
      "published_date": "2025-07-10",
      "entities": [
        "Microsoft",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07935v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Meek Models Shall Inherit the Earth",
      "content": "The past decade has seen incredible scaling of AI systems by a few companies,\nleading to inequality in AI model performance. This paper argues that, contrary\nto prevailing intuition, the diminishing returns to compute scaling will lead\nto a convergence of AI model capabilities. In other words, meek models (those\nwith limited computation budget) shall inherit the earth, approaching the\nperformance level of the best models overall. We develop a model illustrating\nthat under a fixed-distribution next-token objective, the marginal capability\nreturns to raw compute shrink substantially. Given current scaling practices,\nwe argue that these diminishing returns are strong enough that even companies\nthat can scale their models exponentially faster than other organizations will\neventually have little advantage in capabilities. As part of our argument, we\ngive several reasons that proxies like training loss differences capture\nimportant capability measures using evidence from benchmark data and\ntheoretical performance models. In addition, we analyze empirical data on the\ncapability difference of AI models over time. Finally, in light of the\nincreasing ability of meek models, we argue that AI strategy and policy require\nreexamination, and we outline the areas this shift will affect.",
      "url": "http://arxiv.org/abs/2507.07931v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07931v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training",
      "content": "Background: Public speaking is a vital professional skill, yet it remains a\nsource of significant anxiety for many individuals. Traditional training relies\nheavily on expert coaching, but recent advances in AI has led to novel types of\ncommercial automated public speaking feedback tools. However, most research has\nfocused on prototypes rather than commercial applications, and little is known\nabout how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and\ndesign of commercial AI-based public speaking training tools and to propose\nguidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus\ngroups with public speaking experts. Participants discussed their views on\ncurrent commercial tools, their potential integration into traditional\ncoaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in\nhandling repetitive, technical aspects of training, allowing coaches to focus\non higher-level skills. However they found key issues in current tools,\nemphasising the need for personalised, understandable, carefully selected\nfeedback and clear instructional design. Overall, they supported a hybrid model\ncombining traditional coaching with AI-supported exercises.",
      "url": "http://arxiv.org/abs/2507.07930v1",
      "published_date": "2025-07-10",
      "entities": [
        "Traditional",
        "traditional",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07930v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Agentic Retrieval of Topics and Insights from Earnings Calls",
      "content": "Tracking the strategic focus of companies through topics in their earnings\ncalls is a key task in financial analysis. However, as industries evolve,\ntraditional topic modeling techniques struggle to dynamically capture emerging\ntopics and their relationships. In this work, we propose an LLM-agent driven\napproach to discover and retrieve emerging topics from quarterly earnings\ncalls. We propose an LLM-agent to extract topics from documents, structure them\ninto a hierarchical ontology, and establish relationships between new and\nexisting topics through a topic ontology. We demonstrate the use of extracted\ntopics to infer company-level insights and emerging trends over time. We\nevaluate our approach by measuring ontology coherence, topic evolution\naccuracy, and its ability to surface emerging financial trends.",
      "url": "http://arxiv.org/abs/2507.07906v1",
      "published_date": "2025-07-10",
      "entities": [
        "traditional"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07906v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "MIRA: A Novel Framework for Fusing Modalities in Medical RAG",
      "content": "Multimodal Large Language Models (MLLMs) have significantly advanced\nAI-assisted medical diagnosis, but they often generate factually inconsistent\nresponses that deviate from established medical knowledge. Retrieval-Augmented\nGeneration (RAG) enhances factual accuracy by integrating external sources, but\nit presents two key challenges. First, insufficient retrieval can miss critical\ninformation, whereas excessive retrieval can introduce irrelevant or misleading\ncontent, disrupting model output. Second, even when the model initially\nprovides correct answers, over-reliance on retrieved data can lead to factual\nerrors. To address these issues, we introduce the Multimodal Intelligent\nRetrieval and Augmentation (MIRA) framework, designed to optimize factual\naccuracy in MLLM. MIRA consists of two key components: (1) a calibrated\nRethinking and Rearrangement module that dynamically adjusts the number of\nretrieved contexts to manage factual risk, and (2) A medical RAG framework\nintegrating image embeddings and a medical knowledge base with a query-rewrite\nmodule for efficient multimodal reasoning. This enables the model to\neffectively integrate both its inherent knowledge and external references. Our\nevaluation of publicly available medical VQA and report generation benchmarks\ndemonstrates that MIRA substantially enhances factual accuracy and overall\nperformance, achieving new state-of-the-art results. Code is released at\nhttps://github.com/mbzuai-oryx/MIRA.",
      "url": "http://arxiv.org/abs/2507.07902v1",
      "published_date": "2025-07-10",
      "entities": [
        "medical",
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07902v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "The Trust Fabric: Decentralized Interoperability and Economic\n  Coordination for the Agentic Web",
      "content": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.",
      "url": "http://arxiv.org/abs/2507.07901v1",
      "published_date": "2025-07-10",
      "entities": [
        "healthcare",
        "MIT",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07901v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Can AI-predicted complexes teach machine learning to compute drug\n  binding affinity?",
      "content": "We evaluate the feasibility of using co-folding models for synthetic data\naugmentation in training machine learning-based scoring functions (MLSFs) for\nbinding affinity prediction. Our results show that performance gains depend\ncritically on the structural quality of augmented data. In light of this, we\nestablished simple heuristics for identifying high-quality co-folding\npredictions without reference structures, enabling them to substitute for\nexperimental structures in MLSF training. Our study informs future data\naugmentation strategies based on co-folding models.",
      "url": "http://arxiv.org/abs/2507.07882v1",
      "published_date": "2025-07-10",
      "entities": [
        "machine learning"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07882v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Impact of Pretraining Word Co-occurrence on Compositional Generalization\n  in Multimodal Models",
      "content": "CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.",
      "url": "http://arxiv.org/abs/2507.08000v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.08000v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
      "content": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
      "url": "http://arxiv.org/abs/2507.07999v1",
      "published_date": "2025-07-10",
      "entities": [
        "OpenAI"
      ],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07999v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search",
      "content": "According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.",
      "url": "http://arxiv.org/abs/2507.07995v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07995v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
      "content": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
      "url": "http://arxiv.org/abs/2507.07996v1",
      "published_date": "2025-07-10",
      "entities": [
        "neural network"
      ],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07996v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection",
      "content": "Keypoint detection, integral to modern machine perception, faces challenges\nin few-shot learning, particularly when source data from the same distribution\nas the query is unavailable. This gap is addressed by leveraging sketches, a\npopular form of human expression, providing a source-free alternative. However,\nchallenges arise in mastering cross-modal embeddings and handling user-specific\nsketch styles. Our proposed framework overcomes these hurdles with a\nprototypical setup, combined with a grid-based locator and prototypical domain\nadaptation. We also demonstrate success in few-shot convergence across novel\nkeypoints and classes through extensive experiments.",
      "url": "http://arxiv.org/abs/2507.07994v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07994v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "EXPO: Stable Reinforcement Learning with Expressive Policies",
      "content": "We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.",
      "url": "http://arxiv.org/abs/2507.07986v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07986v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is\n  Why",
      "content": "Contrastive vision-language models like CLIP are used for a large variety of\napplications, such as zero-shot classification or as vision encoder for\nmulti-modal models. Despite their popularity, their representations show major\nlimitations. For instance, CLIP models learn bag-of-words representations and,\nas a consequence, fail to distinguish whether an image is of \"a yellow\nsubmarine and a blue bus\" or \"a blue submarine and a yellow bus\". Previous\nattempts to fix this issue added hard negatives during training or modified the\narchitecture, but failed to resolve the problem in its entirety. We suspect\nthat the missing insights to solve the binding problem for CLIP are hidden in\nthe arguably most important part of learning algorithms: the data. In this\nwork, we fill this gap by rigorously identifying the influence of data\nproperties on CLIP's ability to learn binding using a synthetic dataset. We\nfind that common properties of natural data such as low attribute density,\nincomplete captions, and the saliency bias, a tendency of human captioners to\ndescribe the object that is \"most salient\" to them have a detrimental effect on\nbinding performance. In contrast to common belief, we find that neither scaling\nthe batch size, i.e., implicitly adding more hard negatives, nor explicitly\ncreating hard negatives enables CLIP to learn reliable binding. Only when the\ndata expresses our identified data properties CLIP learns almost perfect\nbinding.",
      "url": "http://arxiv.org/abs/2507.07985v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07985v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
      "content": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
      "url": "http://arxiv.org/abs/2507.07982v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07982v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Why is Your Language Model a Poor Implicit Reward Model?",
      "content": "Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Towards a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.",
      "url": "http://arxiv.org/abs/2507.07981v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07981v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Reinforcement Learning with Action Chunking",
      "content": "We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.",
      "url": "http://arxiv.org/abs/2507.07969v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07969v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Impact of Pretraining Word Co-occurrence on Compositional Generalization\n  in Multimodal Models",
      "content": "CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.",
      "url": "http://arxiv.org/abs/2507.08000v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.08000v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Multigranular Evaluation for Brain Visual Decoding",
      "content": "Existing evaluation protocols for brain visual decoding predominantly rely on\ncoarse metrics that obscure inter-model differences, lack neuroscientific\nfoundation, and fail to capture fine-grained visual distinctions. To address\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\nframework that jointly quantifies structural fidelity, inferential alignment,\nand contextual coherence between decoded and ground truth images. For the\nstructural level, we introduce a hierarchical suite of segmentation-based\nmetrics, including foreground, semantic, instance, and component masks,\nanchored in granularity-aware correspondence across mask structures. For the\nsemantic level, we extract structured scene representations encompassing\nobjects, attributes, and relationships using multimodal large language models,\nenabling detailed, scalable, and context-rich comparisons with ground-truth\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\nstimulus-neuroimaging datasets within this unified evaluation framework.\nTogether, these criteria provide a more discriminative, interpretable, and\ncomprehensive foundation for measuring brain visual decoding methods.",
      "url": "http://arxiv.org/abs/2507.07993v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07993v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Automating Expert-Level Medical Reasoning Evaluation of Large Language\n  Models",
      "content": "As large language models (LLMs) become increasingly integrated into clinical\ndecision-making, ensuring transparent and trustworthy reasoning is essential.\nHowever, existing evaluation strategies of LLMs' medical reasoning capability\neither suffer from unsatisfactory assessment or poor scalability, and a\nrigorous benchmark remains lacking. To address this, we introduce\nMedThink-Bench, a benchmark designed for rigorous, explainable, and scalable\nassessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging\nquestions across ten medical domains, each annotated with expert-crafted\nstep-by-step rationales. Building on this, we propose LLM-w-Ref, a novel\nevaluation framework that leverages fine-grained rationales and LLM-as-a-Judge\nmechanisms to assess intermediate reasoning with expert-level fidelity while\nmaintaining scalability. Experiments show that LLM-w-Ref exhibits a strong\npositive correlation with expert judgments. Benchmarking twelve\nstate-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can\nsurpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,\nMedThink-Bench offers a foundational tool for evaluating LLMs' medical\nreasoning, advancing their safe and responsible deployment in clinical\npractice.",
      "url": "http://arxiv.org/abs/2507.07988v1",
      "published_date": "2025-07-10",
      "entities": [
        "clinical",
        "medical",
        "OpenAI"
      ],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07988v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
      "content": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
      "url": "http://arxiv.org/abs/2507.07984v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07984v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Martian World Models: Controllable Video Synthesis with Physically\n  Accurate 3D Reconstructions",
      "content": "Synthesizing realistic Martian landscape videos is crucial for mission\nrehearsal and robotic simulation. However, this task poses unique challenges\ndue to the scarcity of high-quality Martian data and the significant domain gap\nbetween Martian and terrestrial imagery. To address these challenges, we\npropose a holistic solution composed of two key components: 1) A data curation\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\nenvironments from real stereo navigation images, sourced from NASA's Planetary\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\nMartian terrain video generator, MarsGen, which synthesizes novel videos\nvisually realistic and geometrically consistent with the 3D structure encoded\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\nacquisition dates, enabling the generation of physically accurate 3D surface\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\nsynthesizes videos conditioned on an initial image frame and, optionally,\ncamera trajectories or textual prompts, allowing for video generation in novel\nenvironments. Experimental results show that our approach outperforms video\nsynthesis models trained on terrestrial datasets, achieving superior visual\nfidelity and 3D structural consistency.",
      "url": "http://arxiv.org/abs/2507.07978v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07978v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
      "content": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
      "url": "http://arxiv.org/abs/2507.07957v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07957v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement\n  and Entropy-aware Alignment",
      "content": "While Vision-Language Models (VLMs) have shown promising progress in general\nmultimodal tasks, they often struggle in industrial anomaly detection and\nreasoning, particularly in delivering interpretable explanations and\ngeneralizing to unseen categories. This limitation stems from the inherently\ndomain-specific nature of anomaly detection, which hinders the applicability of\nexisting VLMs in industrial scenarios that require precise, structured, and\ncontext-aware analysis. To address these challenges, we propose SAGE, a\nVLM-based framework that enhances anomaly reasoning through Self-Guided Fact\nEnhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE\nintegrates domain-specific knowledge into visual reasoning via fact extraction\nand fusion, while E-DPO aligns model outputs with expert preferences using\nentropy-aware optimization. Additionally, we introduce AD-PL, a\npreference-optimized dataset tailored for industrial anomaly reasoning,\nconsisting of 28,415 question-answering instances with expert-ranked responses.\nTo evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation\n(MLE), a quantitative framework analyzing model logic and consistency. SAGE\ndemonstrates superior performance on industrial anomaly datasets under\nzero-shot and one-shot settings. The code, model and dataset are available at\nhttps://github.com/amoreZgx1n/SAGE.",
      "url": "http://arxiv.org/abs/2507.07939v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07939v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Multimodal Framework for Explainable Autonomous Driving: Integrating\n  Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency",
      "content": "Autonomous vehicles (AVs) are poised to redefine transportation by enhancing\nroad safety, minimizing human error, and optimizing traffic efficiency. The\nsuccess of AVs depends on their ability to interpret complex, dynamic\nenvironments through diverse data sources, including video streams, sensor\nmeasurements, and contextual textual information. However, seamlessly\nintegrating these multimodal inputs and ensuring transparency in AI-driven\ndecisions remain formidable challenges. This study introduces a novel\nmultimodal framework that synergistically combines video, sensor, and textual\ndata to predict driving actions while generating human-readable explanations,\nfostering trust and regulatory compliance. By leveraging VideoMAE for\nspatiotemporal video analysis, a custom sensor fusion module for real-time data\nprocessing, and BERT for textual comprehension, our approach achieves robust\ndecision-making and interpretable outputs. Evaluated on the BDD-X (21113\nsamples) and nuScenes (1000 scenes) datasets, our model reduces training loss\nfrom 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy\nof 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming\nstate-of-the-art methods. Ablation studies confirm the critical role of each\nmodality, while qualitative analyses and human evaluations highlight the\nmodel's ability to produce contextually rich, user-friendly explanations. These\nadvancements underscore the transformative potential of multimodal integration\nand explainability in building safe, transparent, and trustworthy AV systems,\npaving the way for broader societal adoption of autonomous driving\ntechnologies.",
      "url": "http://arxiv.org/abs/2507.07938v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07938v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Hydrodynamic Insight Drives Multimodal Vortex-Field Dynamics via\n  Streamline Engineering",
      "content": "Since the 1970s, analogies between laser dynamics and (super)fluid systems\nhave elucidated phenomena from superconductivity to BoseEinstein condensation.\nInspired by Coullet et al.s 1989 formalization of optical vortices via\nhydrodynamic whirlpools, we here advance a hydrodynamic paradigm for\nvortex_beam propagation. By treating the Poynting_vector trajectories as energy\nstreamlines, we establish a three_dimensional map of photon trajectories and\nenergy flow. Angular_spectrum engineering in momentum space, together with the\nfluid_continuity equation, is used to sculpt these streamlines and thereby\ntailor multimodal, independently tunable propagation behaviors. The resulting\nbeams simultaneously suppress diffraction_ and OAM_induced broadening and\ninherit the diffraction_free, self_healing, self_accelerating, and self_similar\ntraits of classic structured modes, with adjustable energy_density profiles to\ncompensate loss. Optical_tweezer measurements, akin to particle_tracking\nvelocimetry of fluid dynamics, confirm that trapped microspheres trace the\nprescribed streamlines, laying the groundwork for future precision\nthree_dimensional photonic manipulation. In a free_space communication case\nstudy, streamline_engineered multimodal vortex beams deliver an\norder_of_magnitude increase in independent channels, enhanced turbulence\nresilience, and non_line_of_sight capability. This hydrodynamic framework thus\nfurnishes a versatile, experimentally verified toolkit for multimodal control\nof vortex beams and substantially broadens their application scope, while\nopening new avenues for fluid dynamics simulations using optical analogues.",
      "url": "http://arxiv.org/abs/2507.07928v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07928v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "The Potential of Olfactory Stimuli in Stress Reduction through Virtual\n  Reality",
      "content": "Immersive virtual reality (VR) is a promising tool for stress reduction and\nrelaxation, traditionally relying on visual and auditory stimuli. This study\nexamines the role of olfactory stimuli in enhancing these effects, using a\nrandomized within-subject design. Thirty participants aged 18-60 experienced VR\nscenarios simulating a calming seaside environment, with sessions lasting 45\nminutes, in two conditions: with and without a \"Beach\" essential oil scent\n(Yankee Candle) administered via diffuser. Stress and relaxation were assessed\nthrough self-reported surveys and physiological measures, specifically\nECG-based heart rate variability (HRV). Results showed no significant\ndifference in self-reported relaxation scores (p=0.371) between conditions, but\nHRV analysis revealed a significant stress reduction (p=0.002) with olfactory\ninput, with HF increasing 108% from the Math Stress Test to the scented\nrelaxation condition, compared to 44% without scent. Additionally, 71.4% of\nparticipants expressed willingness to use olfactory-enhanced VR for relaxation,\nsuggesting practical appeal. These findings indicate that olfactory stimuli may\nenhance relaxation subconsciously, underscoring the importance of multisensory\nintegration in VR. Future work could explore personalized scents and long-term\neffects to optimize VR- based interventions for emotional and physical\nwell-being.",
      "url": "http://arxiv.org/abs/2507.07911v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07911v1"
    }
  ]
}