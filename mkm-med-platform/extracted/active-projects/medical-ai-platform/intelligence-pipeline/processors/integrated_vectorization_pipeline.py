import json
import time
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from datetime import datetime
from google.cloud import storage
from google.cloud import bigquery

class IntegratedVectorizationPipeline:
    def __init__(self):
        print("ğŸ”§ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_db = []
        print("âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # GCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.gcp_bucket_name = "mkm-lab-intelligence"  # ì‹¤ì œ GCS ë²„í‚·ëª…
        self.gcp_bq_dataset = "intelligence"           # ì‹¤ì œ BigQuery ë°ì´í„°ì…‹ëª…
        self.gcp_bq_table = "vectorized_papers"        # ì‹¤ì œ BigQuery í…Œì´ë¸”ëª…
        self.storage_client = storage.Client()
        self.bigquery_client = bigquery.Client()

    def load_collected_data(self, filename="enhanced_collected_data.json"):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(filename, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filename}")
            return data
        except FileNotFoundError:
            print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
            return {}
    
    def process_and_vectorize(self, data_items, data_type):
        """ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„°í™”"""
        if not data_items:
            print(f"[{data_type}] ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_items = len(data_items)
        processed_count = 0
        start_time = time.time()
        
        print(f"\n[{data_type}] ë²¡í„°í™” ì‹œì‘ - ì´ {total_items}ê±´")
        
        for item in data_items:
            processed_count += 1
            
            # í…ìŠ¤íŠ¸ ì¤€ë¹„
            title = item.get('title', '')
            content = item.get('content', '')
            description = item.get('description', '')
            
            # í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
            full_text = f"{title} {content} {description}".strip()
            if not full_text:
                continue
            
            # ìš”ì•½ ìƒì„± (í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ì—ë§Œ)
            try:
                if len(full_text) > 100:
                    summary = self.summarizer(full_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']
                else:
                    summary = full_text
            except Exception as e:
                print(f"    âš ï¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
                summary = full_text[:200] + "..." if len(full_text) > 200 else full_text
            
            # ë²¡í„° ìƒì„±
            try:
                vector = self.model.encode(summary)
            except Exception as e:
                print(f"    âš ï¸ ë²¡í„° ìƒì„± ì‹¤íŒ¨: {e}")
                continue
            
            # í†µí•© ë°ì´í„° êµ¬ì¡° ìƒì„±
            vector_item = {
                "source_type": item.get("source_type", data_type),
                "source_name": item.get("source_name", ""),
                "title": title,
                "summary": summary,
                "content": content,
                "url": item.get("url", ""),
                "published_date": item.get("published_date", ""),
                "entities": item.get("entities", []),
                "keywords": item.get("keywords", []),
                "vector": vector.tolist(),
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                "nct_id": item.get("nct_id", ""),
                "phase": item.get("phase", ""),
                "status": item.get("status", ""),
                "sponsor": item.get("sponsor", ""),
                "funding_round": item.get("funding_round", ""),
                "funding_amount": item.get("funding_amount", ""),
                "company_name": item.get("company_name", ""),
                "stars": item.get("stars", 0),
                "forks": item.get("forks", 0),
                "language": item.get("language", ""),
                "owner": item.get("owner", ""),
                "arxiv_id": item.get("arxiv_id", ""),
                
                # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
                "processed_date": datetime.now().isoformat(),
                "vector_dimension": len(vector)
            }
            
            self.vector_db.append(vector_item)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            elapsed = time.time() - start_time
            avg_per_item = elapsed / processed_count
            est_total = avg_per_item * total_items
            
            print(f"  ì§„í–‰ë¥ : {processed_count/total_items*100:.1f}% | {processed_count}/{total_items}ê±´ | ì˜ˆìƒ ì†Œìš”: {est_total/60:.1f}ë¶„")
            print(f"    â†’ {title[:50]}...")
        
        print(f"[{data_type}] ë²¡í„°í™” ì™„ë£Œ - {processed_count}ê±´ ì²˜ë¦¬ë¨")
    
    def run_integrated_vectorization(self):
        """í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ
        print("\nğŸ“‚ 1ë‹¨ê³„: ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ")
        collected_data = self.load_collected_data()
        
        if not collected_data:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ìˆ˜ì§‘ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # 2. ê° ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬
        print("\nğŸ”§ 2ë‹¨ê³„: ë°ì´í„° íƒ€ì…ë³„ ë²¡í„°í™”")
        
        # GitHub í”„ë¡œì íŠ¸ ì²˜ë¦¬
        if "github_projects" in collected_data:
            self.process_and_vectorize(collected_data["github_projects"], "GitHub í”„ë¡œì íŠ¸")
        
        # ì—°êµ¬ ë…¼ë¬¸ ì²˜ë¦¬
        if "research_papers" in collected_data:
            self.process_and_vectorize(collected_data["research_papers"], "ì—°êµ¬ ë…¼ë¬¸")
        
        # ë‰´ìŠ¤ ì²˜ë¦¬
        if "news" in collected_data:
            self.process_and_vectorize(collected_data["news"], "ë‰´ìŠ¤")
        
        # ì„ìƒì‹œí—˜ ì²˜ë¦¬
        if "clinical_trials" in collected_data:
            self.process_and_vectorize(collected_data["clinical_trials"], "ì„ìƒì‹œí—˜")
        
        # 3. í†µê³„ ìƒì„±
        print("\nğŸ“Š 3ë‹¨ê³„: í†µê³„ ìƒì„±")
        stats = self.generate_statistics()
        
        # 4. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        self.save_results(stats)
        
        total_time = time.time() - start_time
        print(f"\nâœ… í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"   ì´ ë²¡í„°í™”ëœ ë°ì´í„°: {len(self.vector_db)}ê±´")
        print(f"   ì €ì¥ íŒŒì¼: integrated_intelligence_database.json")
        
        return self.vector_db
    
    def generate_statistics(self):
        """ë²¡í„°í™”ëœ ë°ì´í„° í†µê³„ ìƒì„±"""
        stats = {
            "total_items": len(self.vector_db),
            "source_type_distribution": {},
            "source_name_distribution": {},
            "processing_date": datetime.now().isoformat(),
            "vector_dimension": 0
        }
        
        for item in self.vector_db:
            # ì†ŒìŠ¤ íƒ€ì…ë³„ ë¶„í¬
            source_type = item.get("source_type", "unknown")
            stats["source_type_distribution"][source_type] = stats["source_type_distribution"].get(source_type, 0) + 1
            
            # ì†ŒìŠ¤ ì´ë¦„ë³„ ë¶„í¬
            source_name = item.get("source_name", "unknown")
            stats["source_name_distribution"][source_name] = stats["source_name_distribution"].get(source_name, 0) + 1
            
            # ë²¡í„° ì°¨ì› (ì²« ë²ˆì§¸ í•­ëª©ì—ì„œ ê°€ì ¸ì˜´)
            if stats["vector_dimension"] == 0:
                stats["vector_dimension"] = item.get("vector_dimension", 0)
        
        print("ğŸ“Š í†µê³„ ì •ë³´:")
        print(f"   ì´ ë°ì´í„°: {stats['total_items']}ê±´")
        print(f"   ë²¡í„° ì°¨ì›: {stats['vector_dimension']}ì°¨ì›")
        print("   ì†ŒìŠ¤ íƒ€ì…ë³„ ë¶„í¬:")
        for source_type, count in stats["source_type_distribution"].items():
            print(f"     {source_type}: {count}ê±´")
        
        return stats
    
    def save_results(self, stats):
        """ê²°ê³¼ ì €ì¥ ë° GCP ì—…ë¡œë“œ"""
        # ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        with open("integrated_intelligence_database.json", "w", encoding="utf-8") as f:
            json.dump(self.vector_db, f, ensure_ascii=False, indent=2)
        
        # í†µê³„ ì •ë³´ ì €ì¥
        with open("intelligence_database_stats.json", "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print("ğŸ’¾ ì €ì¥ ì™„ë£Œ:")
        print("   - integrated_intelligence_database.json (ë©”ì¸ ë°ì´í„°)")
        print("   - intelligence_database_stats.json (í†µê³„ ì •ë³´)")

        # GCP Cloud Storage ì—…ë¡œë“œ
        try:
            self.upload_to_gcs("integrated_intelligence_database.json", "intelligence/integrated_intelligence_database.json")
            self.upload_to_gcs("intelligence_database_stats.json", "intelligence/intelligence_database_stats.json")
        except Exception as e:
            print(f"âŒ GCS ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")

        # BigQuery ì ì¬
        try:
            self.upload_to_bigquery(self.vector_db)
        except Exception as e:
            print(f"âŒ BigQuery ì ì¬ ì˜¤ë¥˜: {e}")

    # GCP ì—°ë™ í•¨ìˆ˜ ì¶”ê°€
    def upload_to_gcs(self, local_path, gcs_path):
        from google.cloud import storage
        bucket = self.storage_client.bucket(self.gcp_bucket_name)
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_path)
        print(f"â˜ï¸ GCS ì—…ë¡œë“œ ì™„ë£Œ: gs://{self.gcp_bucket_name}/{gcs_path}")

    def upload_to_bigquery(self, data):
        from google.cloud import bigquery
        table_id = f"{self.bigquery_client.project}.{self.gcp_bq_dataset}.{self.gcp_bq_table}"
        errors = self.bigquery_client.insert_rows_json(table_id, data)
        if errors:
            print(f"âŒ BigQuery ì ì¬ ì˜¤ë¥˜: {errors}")
        else:
            print(f"âœ… BigQuery ì ì¬ ì™„ë£Œ: {table_id}")

if __name__ == "__main__":
    pipeline = IntegratedVectorizationPipeline()
    results = pipeline.run_integrated_vectorization()