import requests
import json
import time
import re
from datetime import datetime, timedelta
from bs4port BeautifulSoup
from urllib.parse import urljoin, urlparse
import feedparser
import xml.etree.ElementTree as ET
import arxiv
import scholarly

class KnowledgeEnhancedCollectors:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
          User-Agent': 'Mozilla/5.0 (Windows NT10 Win64; x64) AppleWebKit/537.36      })
        
        # ì§€ì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì •ì˜
        self.medical_keywords = 
          rPPG heart rate variability",
          facial analysis health",
         voice analysis stress,12constitution types",
      traditional medicine AI,       biometric signal processing,emotion recognition healthcare"
        ]
        
        self.ai_keywords = [
  persona classification",
           image generation health",
     multimodal emotion recognition",
    federated learning healthcare,     privacy-preserving AI",
         explainable AI medical"
        ]
        
        self.psychology_keywords = [
      personality biometric correlation",
          stress response patterns,emotion physiology",
   wellness psychology",
          health behavior patterns"
        ]
        
        print("ğŸ§  ì§€ì‹ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def collect_medical_research_papers(self):
      í•™ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ (PubMed API)"
        print(ğŸ¥ ì˜í•™ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_papers = []
        
        for keyword in self.medical_keywords:
            print(f"  í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # PubMed API í˜¸ì¶œ
                base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
                
                # ê²€ìƒ‰
                search_url = f{base_url}esearch.fcgi"
                search_params = {
                  db                  term               retmax                 retmode": "json",
                    sort": "relevance"
                }
                
                search_response = self.session.get(search_url, params=search_params)
                if search_response.status_code == 200:
                    search_data = search_response.json()
                    pmids = search_data.get('esearchresult,[object Object]}).get('idlist', [])
                    
                    # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                    for pmid in pmids[:10]:  # ìƒìœ„ 10ê°œë§Œ
                        try:
                            fetch_url = f"[object Object]base_url}efetch.fcgi"
                            fetch_params = {
                              db             id                  retmode                   }
                            
                            fetch_response = self.session.get(fetch_url, params=fetch_params)
                            if fetch_response.status_code == 200:
                                paper_data = self.parse_pubmed_xml(fetch_response.text, keyword)
                                if paper_data:
                                    all_papers.append(paper_data)
                                    print(f   â†’ {paper_data['title'][:50]}...")
                        
                        except Exception as e:
                            print(f    âš ï¸ ë…¼ë¬¸ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                            continue
                
                time.sleep(1)  # API ì œí•œ ì¤€ìˆ˜
                
            except Exception as e:
                print(f âš ï¸ [object Object]keyword} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        print(f"  ì´ [object Object]len(all_papers)}ê±´ì˜ ì˜í•™ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_papers
    
    def collect_ai_research_papers(self):
      AI ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ (arXiv API)"
        print(ğŸ¤– AI ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_papers = []
        
        for keyword in self.ai_keywords:
            print(f"  í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # arXiv ê²€ìƒ‰
                search = arxiv.Search(
                    query=keyword,
                    max_results=20,
                    sort_by=arxiv.SortCriterion.Relevance
                )
                
                for result in search.results():
                    paper_data = {
                        source_type:                  source_name": "arXiv",
                     title": result.title,
                       content": result.summary,
                    url: result.entry_id,
                       published_date": result.published.strftime("%Y-%m-%d"),
                        authors": [author.name for author in result.authors],
                   categories": result.categories,
                   keywords": [keyword],
                        arxiv_id: result.entry_id.split('/')[-1]
                    }
                    
                    all_papers.append(paper_data)
                    print(f"    â†’[object Object]result.title[:50]}...")
                
                time.sleep(1)  # API ì œí•œ ì¤€ìˆ˜
                
            except Exception as e:
                print(f âš ï¸ [object Object]keyword} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        print(f"  ì´ [object Object]len(all_papers)}ê±´ì˜ AI ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_papers
    
    def collect_psychology_research(self):
    ì‹¬ë¦¬í•™ ì—°êµ¬ìˆ˜ì§‘ (Google Scholar)"
        print("ğŸ§  ì‹¬ë¦¬í•™ ì—°êµ¬ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_papers = []
        
        for keyword in self.psychology_keywords:
            print(f"  í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # Google Scholar ê²€ìƒ‰ (ì œí•œì )
                search_query = scholarly.search_pubs(keyword)
                
                for i, result in enumerate(search_query):
                    if i >= 10:  # ìƒìœ„ 10ê°œë§Œ
                        break
                    
                    paper_data = {
                      source_type: sychology_research",
                      source_name:Google Scholar                title": result.get('title', ''),
                       content": result.get('abstract', ''),
                       url: result.get('url', ''),
                       published_date": result.get('year', ''),
                       authors": result.get('author', []),
                  citations": result.get('citations', 0),
                   keywords": [keyword]
                    }
                    
                    all_papers.append(paper_data)
                    print(f   â†’ {paper_data['title'][:50]}...")
                
                time.sleep(2)  # API ì œí•œ ì¤€ìˆ˜
                
            except Exception as e:
                print(f âš ï¸ [object Object]keyword} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        print(f"  ì´ [object Object]len(all_papers)}ê±´ì˜ ì‹¬ë¦¬í•™ ì—°êµ¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_papers
    
    def collect_patent_data(self):
       ë°ì´í„° ìˆ˜ì§‘"
        print("ğŸ“‹ íŠ¹í—ˆ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        patent_keywords =       biometric signal analysis",
            health monitoring system,          emotion detection device",
       personalized health recommendation",
          wearable health technology"
        ]
        
        all_patents = []
        
        for keyword in patent_keywords:
            print(f"  í‚¤ì›Œë“œ: {keyword}")
            
            try:
                # USPTO API í˜¸ì¶œ (ì˜ˆì‹œ)
                # ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” ì ì ˆí•œ íŠ¹í—ˆ ë°ì´í„°ë² ì´ìŠ¤ API ì‚¬ìš©
                patent_data = {
                  source_type": "patent",
                    source_name": "USPTO",
                   title: fPatent related to {keyword}",
               content": f"Patent description for {keyword}",
                   url": f"https://patents.google.com/?q={keyword}",
                   published_date:datetime.now().strftime("%Y-%m-%d"),
               keywords": [keyword],
                    patent_number": f"US{int(time.time())}"
                }
                
                all_patents.append(patent_data)
                print(f"    â†’ {patent_data['title']}")
                
            except Exception as e:
                print(f âš ï¸ [object Object]keyword} íŠ¹í—ˆ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        print(f"  ì´ [object Object]len(all_patents)}ê±´ì˜ íŠ¹í—ˆ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return all_patents
    
    def parse_pubmed_xml(self, xml_content, keyword):
      PubMed XML íŒŒì‹±"""
        try:
            root = ET.fromstring(xml_content)
            
            # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
            article = root.find('.//PubmedArticle')
            if article is None:
                return None
            
            # ì œëª©
            title_elem = article.find('.//ArticleTitle')
            title = title_elem.text if title_elem is not None else ""
            
            # ì´ˆë¡
            abstract_elem = article.find('.//AbstractText')
            abstract = abstract_elem.text if abstract_elem is not None else ""
            
            # ì €ì
            authors = []
            author_list = article.findall('.//Author)        for author in author_list:
                last_name = author.find('LastName)             first_name = author.find('ForeName)                if last_name is not None and first_name is not None:
                    authors.append(f{first_name.text} {last_name.text}")
            
            # ë°œí–‰ì¼
            pub_date = article.find('.//PubDate')
            year = pub_date.find('Year').text if pub_date is not None and pub_date.find('Year') is not None else ""
            
            # PMID
            pmid_elem = article.find('.//PMID')
            pmid = pmid_elem.text if pmid_elem is not None else ""
            
            return[object Object]
              source_type":medical_research,
              source_name": "PubMed,
              titlee,
        content": abstract,
               url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/,
               published_date": year,
                authors": authors,
            pmidd,
           keywords": [keyword]
            }
            
        except Exception as e:
            print(f   âš ï¸ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def run_complete_knowledge_collection(self):
        ì²´ ì§€ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"
        print("ğŸš€ ì§€ì‹ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘)
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. ì˜í•™ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘
        medical_papers = self.collect_medical_research_papers()
        
        # 2. AI ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì§‘
        ai_papers = self.collect_ai_research_papers()
        
        # 3. ì‹¬ë¦¬í•™ ì—°êµ¬ ìˆ˜ì§‘
        psychology_papers = self.collect_psychology_research()
        
        # 4. íŠ¹í—ˆ ë°ì´í„° ìˆ˜ì§‘
        patent_data = self.collect_patent_data()
        
        # 5. í†µí•© ê²°ê³¼ ì €ì¥
        all_knowledge_data = {
           medical_research": medical_papers,
           ai_research": ai_papers,
         psychology_research": psychology_papers,
        patent_data": patent_data,
            collection_date:datetime.now().isoformat(),
            total_items": len(medical_papers) + len(ai_papers) + len(psychology_papers) + len(patent_data)
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open("knowledge_enhanced_data.json,w, encoding="utf-8") as f:
            json.dump(all_knowledge_data, f, ensure_ascii=False, indent=2)
        
        total_time = time.time() - start_time
        
        print(f"\nâœ… ì§€ì‹ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!)
        print(f   ì´ ì†Œìš”ì‹œê°„: [object Object]total_time/60}ë¶„)
        print(f"   ì˜í•™ ë…¼ë¬¸: {len(medical_papers)}ê±´)
        print(f  AI ë…¼ë¬¸: {len(ai_papers)}ê±´)
        print(f"   ì‹¬ë¦¬í•™ ì—°êµ¬: {len(psychology_papers)}ê±´)
        print(f"   íŠ¹í—ˆ ë°ì´í„°: [object Object]len(patent_data)}ê±´)
        print(f"   ì´ ë°ì´í„°: {all_knowledge_data['total_items']}ê±´)
        print(f"   ì €ì¥ íŒŒì¼: knowledge_enhanced_data.json")
        
        return all_knowledge_data

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__:
    collector = KnowledgeEnhancedCollectors()
    knowledge_data = collector.run_complete_knowledge_collection() 