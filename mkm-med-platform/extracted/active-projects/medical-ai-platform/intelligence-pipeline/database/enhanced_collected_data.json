{
  "news": [],
  "clinical_trials": [],
  "github_projects": [
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "funNLP",
      "content": "中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT & ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目、brat rapid annotation tool: 序列标注工具、大规模中文知识图谱数据：1.4亿实体、数据增强在机器翻译及其他nlp任务中的应用及效果、allennlp阅读理解:支持多种数据和模型、PDF表格数据提取工具 、 Graphbrain：AI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断、简历自动筛选系统、基于命名实体识别的简历自动摘要、中文语言理解测评基准，包括代表性的数据集&基准模型&语料库&排行榜、树洞 OCR 文字识别 、从包含表格的扫描图片中识别表格和文字、语声迁移、Python口语自然语言处理工具集(英文)、 similarity：相似度计算工具包，java编写、海量中文预训练ALBERT模型 、Transformers 2.0 、基于大规模音频数据集Audioset的音频增强 、Poplar：网页版自然语言标注工具、图片文字去除，可用于漫画翻译 、186种语言的数字叫法库、Amazon发布基于知识的人-人开放领域对话数据集 、中文文本纠错模块代码、繁简体转换 、 Python实现的多种文本可读性评价指标、类似于人名/地名/组织机构名的命名体识别数据集 、东南大学《知识图谱》研究生课程(资料)、. 英文拼写检查库 、 wwsearch是企业微信后台自研的全文检索引擎、CHAMELEON：深度学习新闻推荐系统元架构 、 8篇论文梳理BERT相关模型进展与反思、DocSearch：免费文档搜索引擎、 LIDA：轻量交互式对话标注工具 、aili - the fastest in-memory index in the East 东半球最快并发索引 、知识图谱车音工作项目、自然语言生成资源大全 、中日韩分词库mecab的Python接口库、中文文本摘要/关键词提取、汉字字符特征提取器 (featurizer)，提取汉字的特征（发音特征、字形特征）用做深度学习的特征、中文生成任务基准测评 、中文缩写数据集、中文任务基准测评 - 代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜、PySS3：面向可解释AI的SS3文本分类器机器可视化工具 、中文NLP数据集列表、COPE - 格律诗编辑程序、doccano：基于网页的开源协同多语言文本标注工具 、PreNLP：自然语言预处理库、简单的简历解析器，用来从简历中提取关键信息、用于中文闲聊的GPT2模型：GPT2-chitchat、基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)、(Colab)抽象文本摘要实现集锦(教程 、词语拼音数据、高效模糊搜索工具、NLP数据增广资源集、微软对话机器人框架 、 GitHub Typo Corpus：大规模GitHub多语言拼写错误/语法错误数据集、TextCluster：短文本聚类预处理模块 Short text cluster、面向语音识别的中文文本规范化、BLINK：最先进的实体链接库、BertPunc：基于BERT的最先进标点修复模型、Tokenizer：快速、可定制的文本词条化库、中文语言理解测评基准，包括代表性的数据集、基准(预训练)模型、语料库、排行榜、spaCy 医学文本挖掘与信息提取 、 NLP任务示例项目代码集、 python拼写检查库、chatbot-list - 行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍、语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)、 用138GB语料训练的法文RoBERTa预训练语言模型 、BERT-NER-Pytorch：三种不同模式的BERT中文NER实验、无道词典 - 有道词典的命令行版本，支持英汉互查和在线查询、2019年NLP亮点回顾、 Chinese medical dialogue data 中文医疗对话数据集 、最好的汉字数字(中文数字)-阿拉伯数字转换工具、 基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧、awesome-nlp-sentiment-analysis - 情感分析、情绪原因识别、评价对象和评价词抽取、LineFlow：面向所有深度学习框架的NLP数据高效加载器、中文医学NLP公开资源整理 、MedQuAD：(英文)医学问答数据集、将自然语言数字串解析转换为整数和浮点数、Transfer Learning in Natural Language Processing (NLP) 、面向语音识别的中文/英文发音辞典、Tokenizers：注重性能与多功能性的最先进分词器、CLUENER 细粒度命名实体识别 Fine Grained Named Entity Recognition、 基于BERT的中文命名实体识别、中文谣言数据库、NLP数据集/基准任务大列表、nlp相关的一些论文及代码, 包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflow 、Python文本挖掘/NLP实战示例、 Blackstone：面向非结构化法律文本的spaCy pipeline和NLP模型通过同义词替换实现文本“变脸” 、中文 预训练 ELECTREA 模型: 基于对抗学习 pretrain Chinese Model 、albert-chinese-ner - 用预训练语言模型ALBERT做中文NER 、基于GPT2的特定主题文本生成/文本增广、开源预训练语言模型合集、多语言句向量包、编码、标记和实现：一种可控高效的文本生成方法、 英文脏话大列表 、attnvis：GPT2、BERT等transformer语言模型注意力交互可视化、CoVoST：Facebook发布的多语种语音-文本翻译语料库，包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文、Jiagu自然语言处理工具 - 以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能、用unet实现对文档表格的自动检测，表格重建、NLP事件提取文献资源列表 、 金融领域自然语言处理研究资源大列表、CLUEDatasetSearch - 中英文NLP数据集：搜索所有中文NLP数据集，附常用英文NLP数据集 、medical_NER - 中文医学知识图谱命名实体识别 、(哈佛)讲因果推理的免费书、知识图谱相关学习资料/数据集/工具资源大列表、Forte：灵活强大的自然语言处理pipeline工具集 、Python字符串相似性算法库、PyLaia：面向手写文档分析的深度学习工具包、TextFooler：针对文本分类/推理的对抗文本生成模块、Haystack：灵活、强大的可扩展问答(QA)框架、中文关键短语抽取工具",
      "url": "https://github.com/fighting41love/funNLP",
      "published_date": "2018-08-21T11:20:39Z",
      "stars": 74748,
      "forks": 14912,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "medical",
        "Machine Learning",
        "nlp",
        "NLP",
        "ML"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "fighting41love"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "py-gpt",
      "content": "Desktop AI Assistant powered by o1, o3, GPT-4, Gemini, Claude, Ollama, DeepSeek, Grok, Bielik, chat, vision, voice control, image generation and analysis, agents, command execution, file upload/download, speech synthesis and recognition, access to Web, memory, presets, assistants, plugins, and more. Linux, Windows, Mac",
      "url": "https://github.com/szczyglis-dev/py-gpt",
      "published_date": "2023-04-09T23:48:06Z",
      "stars": 1107,
      "forks": 215,
      "language": "Python",
      "entities": [
        "Desktop AI",
        "AI"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "szczyglis-dev"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "my-voice-analysis",
      "content": "My-Voice Analysis is a Python library for the analysis of voice (simultaneous speech, high entropy) without the need of a transcription. It breaks utterances and detects syllable boundaries, fundamental frequency contours, and formants.",
      "url": "https://github.com/Shahabks/my-voice-analysis",
      "published_date": "2018-11-29T17:05:36Z",
      "stars": 321,
      "forks": 92,
      "language": "Python",
      "entities": [
        "Voice Analysis"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "Shahabks"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Parkinson-Disease-Prediction",
      "content": "Introduction  Parkinson’s Disease is the second most prevalent neurodegenerative disorder after Alzheimer’s, affecting more than 10 million people worldwide. Parkinson’s is characterized primarily by the deterioration of motor and cognitive ability. There is no single test which can be administered for diagnosis. Instead, doctors must perform a careful clinical analysis of the patient’s medical history. Unfortunately, this method of diagnosis is highly inaccurate. A study from the National Institute of Neurological Disorders finds that early diagnosis (having symptoms for 5 years or less) is only 53% accurate. This is not much better than random guessing, but an early diagnosis is critical to effective treatment. Because of these difficulties, I investigate a machine learning approach to accurately diagnose Parkinson’s, using a dataset of various speech features (a non-invasive yet characteristic tool) from the University of Oxford. Why speech features? Speech is very predictive and characteristic of Parkinson’s disease; almost every Parkinson’s patient experiences severe vocal degradation (inability to produce sustained phonations, tremor, hoarseness), so it makes sense to use voice to diagnose the disease. Voice analysis gives the added benefit of being non-invasive, inexpensive, and very easy to extract clinically. Background  Parkinson's Disease  Parkinson’s is a progressive neurodegenerative condition resulting from the death of the dopamine containing cells of the substantia nigra (which plays an important role in movement). Symptoms include: “frozen” facial features, bradykinesia (slowness of movement), akinesia (impairment of voluntary movement), tremor, and voice impairment. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. Performance Metrics  TP = true positive, FP = false positive, TN = true negative, FN = false negative Accuracy: (TP+TN)/(P+N) Matthews Correlation Coefficient: 1=perfect, 0=random, -1=completely inaccurate Algorithms Employed  Logistic Regression (LR): Uses the sigmoid logistic equation with weights (coefficient values) and biases (constants) to model the probability of a certain class for binary classification. An output of 1 represents one class, and an output of 0 represents the other. Training the model will learn the optimal weights and biases. Linear Discriminant Analysis (LDA): Assumes that the data is Gaussian and each feature has the same variance. LDA estimates the mean and variance for each class from the training data, and then uses properties of statistics (Bayes theorem , Gaussian distribution, etc) to compute the probability of a particular instance belonging to a given class. The class with the largest probability is the prediction. k Nearest Neighbors (KNN): Makes predictions about the validation set using the entire training set. KNN makes a prediction about a new instance by searching through the entire set to find the k “closest” instances. “Closeness” is determined using a proximity measurement (Euclidean) across all features. The class that the majority of the k closest instances belong to is the class that the model predicts the new instance to be. Decision Tree (DT): Represented by a binary tree, where each root node represents an input variable and a split point, and each leaf node contains an output used to make a prediction. Neural Network (NN): Models the way the human brain makes decisions. Each neuron takes in 1+ inputs, and then uses an activation function to process the input with weights and biases to produce an output. Neurons can be arranged into layers, and multiple layers can form a network to model complex decisions. Training the network involves using the training instances to optimize the weights and biases. Naive Bayes (NB): Simplifies the calculation of probabilities by assuming that all features are independent of one another (a strong but effective assumption). Employs Bayes Theorem to calculate the probabilities that the instance to be predicted is in each class, then finds the class with the highest probability. Gradient Boost (GB): Generally used when seeking a model with very high predictive performance. Used to reduce bias and variance (“error”) by combining multiple “weak learners” (not very good models) to create a “strong learner” (high performance model). Involves 3 elements: a loss function (error function) to be optimized, a weak learner (decision tree) to make predictions, and an additive model to add trees to minimize the loss function. Gradient descent is used to minimize error after adding each tree (one by one). Engineering Goal  Produce a machine learning model to diagnose Parkinson’s disease given various features of a patient’s speech with at least 90% accuracy and/or a Matthews Correlation Coefficient of at least 0.9. Compare various algorithms and parameters to determine the best model for predicting Parkinson’s.  Dataset Description  Source: the University of Oxford 195 instances (147 subjects with Parkinson’s, 48 without Parkinson’s) 22 features (elements that are possibly characteristic of Parkinson’s, such as frequency, pitch, amplitude / period of the sound wave) 1 label (1 for Parkinson’s, 0 for no Parkinson’s) Project Pipeline  pipeline  Summary of Procedure  Split the Oxford Parkinson’s Dataset into two parts: one for training, one for validation (evaluate how well the model performs) Train each of the following algorithms with the training set: Logistic Regression, Linear Discriminant Analysis, k Nearest Neighbors, Decision Tree, Neural Network, Naive Bayes, Gradient Boost Evaluate results using the validation set Repeat for the following training set to validation set splits: 80% training / 20% validation, 75% / 25%, and 70% / 30% Repeat for a rescaled version of the dataset (scale all the numbers in the dataset to a range from 0 to 1: this helps to reduce the effect of outliers) Conduct 5 trials and average the results Data  a_o  a_r  m_o  m_r  Data Analysis  In general, the models tended to perform the best (both in terms of accuracy and Matthews Correlation Coefficient) on the rescaled dataset with a 75-25 train-test split. The two highest performing algorithms, k Nearest Neighbors and the Neural Network, both achieved an accuracy of 98%. The NN achieved a MCC of 0.96, while KNN achieved a MCC of 0.94. These figures outperform most existing literature and significantly outperform current methods of diagnosis. Conclusion and Significance  These robust results suggest that a machine learning approach can indeed be implemented to significantly improve diagnosis methods of Parkinson’s disease. Given the necessity of early diagnosis for effective treatment, my machine learning models provide a very promising alternative to the current, rather ineffective method of diagnosis. Current methods of early diagnosis are only 53% accurate, while my machine learning model produces 98% accuracy. This 45% increase is critical because an accurate, early diagnosis is needed to effectively treat the disease. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. With an earlier diagnosis, much of this degradation could have been slowed or treated. My results are very significant because Parkinson’s affects over 10 million people worldwide who could benefit greatly from an early, accurate diagnosis. Not only is my machine learning approach more accurate in terms of diagnostic accuracy, it is also more scalable, less expensive, and therefore more accessible to people who might not have access to established medical facilities and professionals. The diagnosis is also much simpler, requiring only a 10-15 second voice recording and producing an immediate diagnosis. Future Research  Given more time and resources, I would investigate the following: Create a mobile application which would allow the user to record his/her voice, extract the necessary vocal features, and feed it into my machine learning model to diagnose Parkinson’s. Use larger datasets in conjunction with the University of Oxford dataset. Tune and improve my models even further to achieve even better results. Investigate different structures and types of neural networks. Construct a novel algorithm specifically suited for the prediction of Parkinson’s. Generalize my findings and algorithms for all types of dementia disorders, such as Alzheimer’s. References  Bind, Shubham. \"A Survey of Machine Learning Based Approaches for Parkinson Disease Prediction.\" International Journal of Computer Science and Information Technologies 6 (2015): n. pag. International Journal of Computer Science and Information Technologies. 2015. Web. 8 Mar. 2017. Brooks, Megan. \"Diagnosing Parkinson's Disease Still Challenging.\" Medscape Medical News. National Institute of Neurological Disorders, 31 July 2014. Web. 20 Mar. 2017. Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) Hashmi, Sumaiya F. \"A Machine Learning Approach to Diagnosis of Parkinson’s Disease.\"Claremont Colleges Scholarship. Claremont College, 2013. Web. 10 Mar. 2017. Karplus, Abraham. \"Machine Learning Algorithms for Cancer Diagnosis.\" Machine Learning Algorithms for Cancer Diagnosis (n.d.): n. pag. Mar. 2012. Web. 20 Mar. 2017. Little, Max. \"Parkinsons Data Set.\" UCI Machine Learning Repository. University of Oxford, 26 June 2008. Web. 20 Feb. 2017. Ozcift, Akin, and Arif Gulten. \"Classifier Ensemble Construction with Rotation Forest to Improve Medical Diagnosis Performance of Machine Learning Algorithms.\" Computer Methods and Programs in Biomedicine 104.3 (2011): 443-51. Semantic Scholar. 2011. Web. 15 Mar. 2017. \"Parkinson’s Disease Dementia.\" UCI MIND. N.p., 19 Oct. 2015. Web. 17 Feb. 2017. Salvatore, C., A. Cerasa, I. Castiglioni, F. Gallivanone, A. Augimeri, M. Lopez, G. Arabia, M. Morelli, M.c. Gilardi, and A. Quattrone. \"Machine Learning on Brain MRI Data for Differential Diagnosis of Parkinson's Disease and Progressive Supranuclear Palsy.\"Journal of Neuroscience Methods 222 (2014): 230-37. 2014. Web. 18 Mar. 2017. Shahbakhi, Mohammad, Danial Taheri Far, and Ehsan Tahami. \"Speech Analysis for Diagnosis of Parkinson’s Disease Using Genetic Algorithm and Support Vector Machine.\"Journal of Biomedical Science and Engineering 07.04 (2014): 147-56. Scientific Research. July 2014. Web. 2 Mar. 2017. \"Speech and Communication.\" Speech and Communication. Parkinson's Disease Foundation, n.d. Web. 22 Mar. 2017. Sriram, Tarigoppula V. S., M. Venkateswara Rao, G. V. Satya Narayana, and D. S. V. G. K. Kaladhar. \"Diagnosis of Parkinson Disease Using Machine Learning and Data Mining Systems from Voice Dataset.\" SpringerLink. Springer, Cham, 01 Jan. 1970. Web. 17 Mar. 2017.",
      "url": "https://github.com/Aastha2104/Parkinson-Disease-Prediction",
      "published_date": "2021-01-16T07:02:34Z",
      "stars": 162,
      "forks": 31,
      "language": "Python",
      "entities": [
        "machine learning",
        "medical",
        "Data Mining Systems",
        "Machine Learning",
        "Medscape Medical",
        "Voice analysis",
        "diagnosis",
        "Diagnosis",
        "Improve Medical",
        "Neural Network",
        "clinical",
        "patient",
        "Medical",
        "who",
        "Information Technologies",
        "treatment",
        "disease",
        "Disease"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "Aastha2104"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "VoiceLab",
      "content": "Automated Reproducible Acoustical Analysis",
      "url": "https://github.com/Voice-Lab/VoiceLab",
      "published_date": "2020-02-12T20:25:21Z",
      "stars": 157,
      "forks": 19,
      "language": "Python",
      "entities": [],
      "keywords": [
        "voice analysis"
      ],
      "owner": "Voice-Lab"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "torch-nansypp",
      "content": "NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis",
      "url": "https://github.com/revsic/torch-nansypp",
      "published_date": "2022-12-08T00:52:51Z",
      "stars": 147,
      "forks": 11,
      "language": "Python",
      "entities": [],
      "keywords": [
        "voice analysis"
      ],
      "owner": "revsic"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "hume-python-sdk",
      "content": "Python client for Hume AI",
      "url": "https://github.com/HumeAI/hume-python-sdk",
      "published_date": "2022-08-03T16:18:48Z",
      "stars": 126,
      "forks": 34,
      "language": "Python",
      "entities": [
        "AI",
        "Hume AI"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "HumeAI"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "PyBot-A-ChatBot-For-Answering-Python-Queries-Using-NLP",
      "content": "Pybot can change the way learners try to learn python programming language in a more interactive way. This chatbot will try to solve or provide answer to almost every python related issues or queries that the user is asking for. We are implementing NLP for improving the efficiency of the chatbot. We will include voice feature for more interactivity to the user. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation. NLTK has been called “a wonderful tool for teaching and working in, computational linguistics using Python,” and “an amazing library to play with natural language.The main issue with text data is that it is all in text format (strings). However, the Machine learning algorithms need some sort of numerical feature vector in order to perform the task. So before we start with any NLP project we need to pre-process it to make it ideal for working. Converting the entire text into uppercase or lowercase, so that the algorithm does not treat the same words in different cases as different Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.Removing Noise i.e everything that isn’t in a standard number or letter.Removing Stop words. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”. A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.",
      "url": "https://github.com/abhishek305/PyBot-A-ChatBot-For-Answering-Python-Queries-Using-NLP",
      "published_date": "2019-04-27T17:42:21Z",
      "stars": 92,
      "forks": 37,
      "language": "Python",
      "entities": [
        "Machine learning",
        "NLP"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "abhishek305"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "opensauce-python",
      "content": "Voice analysis software (Python port of VoiceSauce)",
      "url": "https://github.com/voicesauce/opensauce-python",
      "published_date": "2014-03-21T18:02:24Z",
      "stars": 59,
      "forks": 16,
      "language": "Python",
      "entities": [
        "Voice analysis"
      ],
      "keywords": [
        "voice analysis"
      ],
      "owner": "voicesauce"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "python_wizard",
      "content": "Command line LPC analysis tool to generate bitstreams for the Texas Instruments TMS5220 chip",
      "url": "https://github.com/ptwz/python_wizard",
      "published_date": "2017-10-08T09:05:46Z",
      "stars": 45,
      "forks": 14,
      "language": "Python",
      "entities": [],
      "keywords": [
        "voice analysis"
      ],
      "owner": "ptwz"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "face_recognition",
      "content": "The world's simplest facial recognition api for Python and the command line",
      "url": "https://github.com/ageitgey/face_recognition",
      "published_date": "2017-03-03T21:52:39Z",
      "stars": 55050,
      "forks": 13624,
      "language": "Python",
      "entities": [
        "facial recognition"
      ],
      "keywords": [
        "facial recognition"
      ],
      "owner": "ageitgey"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "deepface",
      "content": "A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python",
      "url": "https://github.com/serengil/deepface",
      "published_date": "2020-02-08T20:42:28Z",
      "stars": 19670,
      "forks": 2664,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "serengil"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "fawkes",
      "content": "Fawkes, privacy preserving tool against facial recognition systems. More info at https://sandlab.cs.uchicago.edu/fawkes",
      "url": "https://github.com/Shawn-Shan/fawkes",
      "published_date": "2020-05-18T00:16:49Z",
      "stars": 5382,
      "forks": 495,
      "language": "Python",
      "entities": [
        "facial recognition"
      ],
      "keywords": [
        "facial recognition"
      ],
      "owner": "Shawn-Shan"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "facenet-pytorch",
      "content": "Pretrained Pytorch face detection (MTCNN) and facial recognition (InceptionResnet) models",
      "url": "https://github.com/timesler/facenet-pytorch",
      "published_date": "2019-05-25T01:29:24Z",
      "stars": 4929,
      "forks": 993,
      "language": "Python",
      "entities": [
        "facial recognition"
      ],
      "keywords": [
        "facial recognition"
      ],
      "owner": "timesler"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Facial-Expression-Recognition.Pytorch",
      "content": "A CNN based pytorch implementation on facial expression recognition (FER2013 and CK+), achieving 73.112% (state-of-the-art) in FER2013 and 94.64% in CK+ dataset",
      "url": "https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch",
      "published_date": "2018-07-15T06:29:38Z",
      "stars": 1889,
      "forks": 563,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "WuJie1010"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "HRNet-Facial-Landmark-Detection",
      "content": "This is an official implementation of facial landmark detection for our TPAMI paper \"Deep High-Resolution Representation Learning for Visual Recognition\". https://arxiv.org/abs/1908.07919",
      "url": "https://github.com/HRNet/HRNet-Facial-Landmark-Detection",
      "published_date": "2019-04-09T13:25:44Z",
      "stars": 1101,
      "forks": 268,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "HRNet"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "EmoPy",
      "content": "A deep neural net toolkit for emotion analysis via Facial Expression Recognition (FER)",
      "url": "https://github.com/thoughtworksarts/EmoPy",
      "published_date": "2017-12-20T02:19:22Z",
      "stars": 959,
      "forks": 264,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "thoughtworksarts"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "FacialExpressionRecognition",
      "content": "人脸识别之表情识别项目相关源码",
      "url": "https://github.com/luanshiyinyang/FacialExpressionRecognition",
      "published_date": "2019-06-18T08:16:47Z",
      "stars": 888,
      "forks": 176,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "luanshiyinyang"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "facial-expression-recognition",
      "content": null,
      "url": "https://github.com/rondinellimorais/facial-expression-recognition",
      "published_date": "2022-04-04T23:56:37Z",
      "stars": 853,
      "forks": 82,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "rondinellimorais"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Facial-Expression-Recognition",
      "content": "Facial-Expression-Recognition in TensorFlow. Detecting faces in video and recognize the expression(emotion).",
      "url": "https://github.com/xionghc/Facial-Expression-Recognition",
      "published_date": "2017-07-17T10:48:03Z",
      "stars": 665,
      "forks": 193,
      "language": "Python",
      "entities": [],
      "keywords": [
        "facial recognition"
      ],
      "owner": "xionghc"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Tongue_diagnosis_Sysytem",
      "content": "基于深度学习的舌象诊断系统 ",
      "url": "https://github.com/NinjaRabbitOvO/Tongue_diagnosis_Sysytem",
      "published_date": "2022-03-25T12:04:13Z",
      "stars": 53,
      "forks": 17,
      "language": null,
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "NinjaRabbitOvO"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Tongue_diagnosis",
      "content": "基于深度学习的舌苔检测毕设 留档",
      "url": "https://github.com/812411838/Tongue_diagnosis",
      "published_date": "2023-02-17T05:43:51Z",
      "stars": 40,
      "forks": 4,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "812411838"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "TongueDiagnosis",
      "content": "舌诊系统，诊断舌头症状，给出食疗方案。",
      "url": "https://github.com/IronSpiderMan/TongueDiagnosis",
      "published_date": "2023-02-11T08:59:38Z",
      "stars": 31,
      "forks": 10,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "IronSpiderMan"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "tongue-diagnosis",
      "content": "Tongue :stuck_out_tongue: diagnose by CNN, VGG-16, Res-50 models and a web application demo.",
      "url": "https://github.com/YaoxiangLi/tongue-diagnosis",
      "published_date": "2018-03-19T06:57:39Z",
      "stars": 12,
      "forks": 1,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "YaoxiangLi"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "SelectorNet",
      "content": "The project implementation of the paper \"A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Diagnosis\".",
      "url": "https://github.com/cshan-github/SelectorNet",
      "published_date": "2023-08-15T15:21:10Z",
      "stars": 11,
      "forks": 0,
      "language": "Python",
      "entities": [
        "TCM",
        "Diagnosis"
      ],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "cshan-github"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "chinese-medicine-tongue-diagnosis",
      "content": "中医舌诊系统 - 一个基于AI的中医舌诊分析和知识库系统",
      "url": "https://github.com/invisible30/chinese-medicine-tongue-diagnosis",
      "published_date": "2025-05-02T03:19:06Z",
      "stars": 10,
      "forks": 4,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "invisible30"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "tongueDiagnosis",
      "content": null,
      "url": "https://github.com/zhiyingproject/tongueDiagnosis",
      "published_date": "2021-06-24T19:42:32Z",
      "stars": 5,
      "forks": 1,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "zhiyingproject"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "TongueDiagnosis",
      "content": null,
      "url": "https://github.com/Sondermmu/TongueDiagnosis",
      "published_date": "2025-05-26T07:00:14Z",
      "stars": 3,
      "forks": 0,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "Sondermmu"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Tongue-Image-Analysis-for-Covid-19-Diagnosis-and-Disease-Detection",
      "content": null,
      "url": "https://github.com/varsharamesh82/Tongue-Image-Analysis-for-Covid-19-Diagnosis-and-Disease-Detection",
      "published_date": "2022-04-04T07:50:21Z",
      "stars": 2,
      "forks": 0,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "varsharamesh82"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "tongue-diagnosis",
      "content": null,
      "url": "https://github.com/ray-SDJ/tongue-diagnosis",
      "published_date": "2024-07-29T23:11:11Z",
      "stars": 1,
      "forks": 0,
      "language": "Python",
      "entities": [],
      "keywords": [
        "tongue diagnosis"
      ],
      "owner": "ray-SDJ"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI_Hospital",
      "content": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis",
      "url": "https://github.com/LibertFan/AI_Hospital",
      "published_date": "2024-02-21T02:22:39Z",
      "stars": 168,
      "forks": 23,
      "language": "Python",
      "entities": [
        "Clinical",
        "Diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "LibertFan"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Image_Recognition_WebGUI",
      "content": "✨基于 3D 卷积神经网络(CNN)的阿尔兹海默智能诊断 Web 应用 Alzheimer's Intelligent Diagnosis Web Application based on 3D Convolutional Neural Network and the ADNI Dataset ✨ (with README in English) 🚩：图像识别可视化界面，快速部署深度学习模型为网页应用，Web预测系统，决策支持系统(DSS)，图像识别前端网页，图像识别Demo展示-Pywebio。AI人工智能图像识别-Pytorch；nii医学影像处理；ADNI数据集。100%纯Python代码，轻量化，易复现 ",
      "url": "https://github.com/bytesc/Image_Recognition_WebGUI",
      "published_date": "2023-04-26T03:16:44Z",
      "stars": 139,
      "forks": 9,
      "language": "Python",
      "entities": [
        "Neural Network",
        "Diagnosis"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "bytesc"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "HealthChatbot",
      "content": "🤖 HealthCare ChatBot Major -1 (4th year - 7th semester)  Health Care Chat-Bot is a Healthcare Domain Chatbot to simulate the predictions of a General Physician.  ChatBot can be described as software that can chat with people using artificial intelligence. These software are used to perform tasks such as quickly responding to users, informing them, helping to purchase products and providing better service to customers. We have made a healthcare based chatbot.  The three main areas where chatbots can be used are diagnostics, patient engagement outside medical facilities, and mental health. In our major we are working on diagnostic.  📃 Brief A chatbot is an artificially intelligent creature which can converse with humans. This could be text-based, or a spoken conversation. In our project we will be using Python as it is currently the most popular language for creating an AI chatbot. In the middle of AI chatbot, architecture is the Natural Language Processing (NLP) layer.  This project aims to build an user-friendly healthcare chatbot which facilitates the job of a healthcare provider and helps improve their performance by interacting with users in a human-like way.  Through chatbots one can communicate with text or voice interface and get reply through artificial intelligence  Typically, a chat bot will communicate with a real person. Chat bots are used in applications such as E-commerce customer service, Call centres, Internet gaming,etc.  Chatbots are programs built to automatically engage with received messages. Chatbots can be programmed to respond the same way each time, to respond differently to messages containing certain keywords and even to use machine learning to adapt their responses to fit the situation.  A developing number of hospitals, nursing homes, and even private centres, presently utilize online Chatbots for human services on their sites. These bots connect with potential patients visiting the site, helping them discover specialists, booking their appointments, and getting them access to the correct treatment.  In any case, the utilization of artificial intelligence in an industry where individuals’ lives could be in question, still starts misgivings in individuals. It brings up issues about whether the task mentioned above ought to be assigned to human staff. This healthcare chatbot system will help hospitals to provide healthcare support online 24 x 7, it answers deep as well as general questions. It also helps to generate leads and automatically delivers the information of leads to sales. By asking the questions in series it helps patients by guiding what exactly he/she is looking for.  📜 Problem Statement During the pandemic, it is more important than ever to get your regular check-ups and to continue to take prescription medications. The healthier you are, the more likely you are to recover quickly from an illness.  In this time patients or health care workers within their practice, providers are deferring elective and preventive visits, such as annual physicals. For some, it is not possible to consult online. In this case, to avoid false information, our project can be of help.  📇 Features Register Screen. Sign-in Screen. Generates database for user login system. Offers you a GUI Based Chatbot for patients for diagnosing. [A pragmatic Approach for Diagnosis] Reccomends an appropriate doctor to you for the following symptom. 📜 Modules Used Our program uses a number of python modules to work properly:  tkinter os webbrowser numpy pandas matplotlib 📃 Algorithm We have used Decision tree for our health care based chat bot.  Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.It usually mimic human thinking ability while making a decision, so it is easy to understand.  :suspect: Project Members Anushka Bansal - 500067844 - R164218014 Shreya Sharma - 500068573 - R164218070 Silvi - 500069092 - R164218072 Ishika Agrawal - 500071154 - R164218097",
      "url": "https://github.com/shreyasharma04/HealthChatbot",
      "published_date": "2022-02-21T08:27:25Z",
      "stars": 113,
      "forks": 20,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "machine learning",
        "medical",
        "healthcare",
        "Healthcare",
        "Diagnosis",
        "symptom",
        "NLP",
        "patient",
        "treatment",
        "HealthCare",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "shreyasharma04"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Image-Recognition-system",
      "content": "✨基于 3D 卷积神经网络(CNN)的阿尔兹海默智能诊断 Web 应用  Alzheimer's Intelligent Diagnosis Web Application based on 3D Convolutional Neural Network and the ADNI Dataset ✨ 🚩(with README in English) 📌含在线demo：医学影像识别系统，图像识别可视化界面，OCR，快速部署深度学习模型为网页应用，Web预测系统，决策支持系统(DSS)，图像识别前端网页，图像识别Demo展示-Pywebio。AI人工智能图像识别-Pytorch；nii医学影像处理；ADNI数据集。100%纯Python代码，轻量化，易复现 ",
      "url": "https://github.com/bytesc/Image-Recognition-system",
      "published_date": "2023-06-13T09:22:54Z",
      "stars": 105,
      "forks": 13,
      "language": "Python",
      "entities": [
        "Neural Network",
        "Diagnosis"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "bytesc"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "SLIViT",
      "content": "An AI framework for clinical diagnosis of 3D biomedical scans",
      "url": "https://github.com/cozygene/SLIViT",
      "published_date": "2024-10-05T05:24:56Z",
      "stars": 101,
      "forks": 27,
      "language": "Python",
      "entities": [
        "clinical",
        "An AI",
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "cozygene"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "nmed2024",
      "content": "AI-based differential diagnosis of dementia etiologies on multimodal data",
      "url": "https://github.com/vkola-lab/nmed2024",
      "published_date": "2024-05-21T12:24:52Z",
      "stars": 89,
      "forks": 16,
      "language": "Python",
      "entities": [
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "vkola-lab"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI-Healthcare-chatbot",
      "content": "Through a series of questions about symptoms it diagnosis the health condition of patient. ",
      "url": "https://github.com/vsharathchandra/AI-Healthcare-chatbot",
      "published_date": "2018-03-20T16:52:42Z",
      "stars": 86,
      "forks": 64,
      "language": "Python",
      "entities": [
        "diagnosis",
        "patient"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "vsharathchandra"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AIOsp-Fault-Diagnosis",
      "content": null,
      "url": "https://github.com/OS-ABC/AIOsp-Fault-Diagnosis",
      "published_date": "2021-03-07T08:55:33Z",
      "stars": 49,
      "forks": 19,
      "language": "Python",
      "entities": [],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "OS-ABC"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "picai_baseline",
      "content": "Baseline AI models for 3D csPCa detection/diagnosis in bpMRI",
      "url": "https://github.com/DIAGNijmegen/picai_baseline",
      "published_date": "2022-06-14T16:26:24Z",
      "stars": 49,
      "forks": 28,
      "language": "Python",
      "entities": [
        "Baseline AI",
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "DIAGNijmegen"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI4Eyes",
      "content": "Multi-level diagnosis of cataract from anterior images via deep learning",
      "url": "https://github.com/wuxianjia1996/AI4Eyes",
      "published_date": "2022-05-16T08:41:59Z",
      "stars": 46,
      "forks": 0,
      "language": "Python",
      "entities": [
        "diagnosis",
        "deep learning"
      ],
      "keywords": [
        "AI diagnosis"
      ],
      "owner": "wuxianjia1996"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "funNLP",
      "content": "中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT & ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目、brat rapid annotation tool: 序列标注工具、大规模中文知识图谱数据：1.4亿实体、数据增强在机器翻译及其他nlp任务中的应用及效果、allennlp阅读理解:支持多种数据和模型、PDF表格数据提取工具 、 Graphbrain：AI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断、简历自动筛选系统、基于命名实体识别的简历自动摘要、中文语言理解测评基准，包括代表性的数据集&基准模型&语料库&排行榜、树洞 OCR 文字识别 、从包含表格的扫描图片中识别表格和文字、语声迁移、Python口语自然语言处理工具集(英文)、 similarity：相似度计算工具包，java编写、海量中文预训练ALBERT模型 、Transformers 2.0 、基于大规模音频数据集Audioset的音频增强 、Poplar：网页版自然语言标注工具、图片文字去除，可用于漫画翻译 、186种语言的数字叫法库、Amazon发布基于知识的人-人开放领域对话数据集 、中文文本纠错模块代码、繁简体转换 、 Python实现的多种文本可读性评价指标、类似于人名/地名/组织机构名的命名体识别数据集 、东南大学《知识图谱》研究生课程(资料)、. 英文拼写检查库 、 wwsearch是企业微信后台自研的全文检索引擎、CHAMELEON：深度学习新闻推荐系统元架构 、 8篇论文梳理BERT相关模型进展与反思、DocSearch：免费文档搜索引擎、 LIDA：轻量交互式对话标注工具 、aili - the fastest in-memory index in the East 东半球最快并发索引 、知识图谱车音工作项目、自然语言生成资源大全 、中日韩分词库mecab的Python接口库、中文文本摘要/关键词提取、汉字字符特征提取器 (featurizer)，提取汉字的特征（发音特征、字形特征）用做深度学习的特征、中文生成任务基准测评 、中文缩写数据集、中文任务基准测评 - 代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜、PySS3：面向可解释AI的SS3文本分类器机器可视化工具 、中文NLP数据集列表、COPE - 格律诗编辑程序、doccano：基于网页的开源协同多语言文本标注工具 、PreNLP：自然语言预处理库、简单的简历解析器，用来从简历中提取关键信息、用于中文闲聊的GPT2模型：GPT2-chitchat、基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)、(Colab)抽象文本摘要实现集锦(教程 、词语拼音数据、高效模糊搜索工具、NLP数据增广资源集、微软对话机器人框架 、 GitHub Typo Corpus：大规模GitHub多语言拼写错误/语法错误数据集、TextCluster：短文本聚类预处理模块 Short text cluster、面向语音识别的中文文本规范化、BLINK：最先进的实体链接库、BertPunc：基于BERT的最先进标点修复模型、Tokenizer：快速、可定制的文本词条化库、中文语言理解测评基准，包括代表性的数据集、基准(预训练)模型、语料库、排行榜、spaCy 医学文本挖掘与信息提取 、 NLP任务示例项目代码集、 python拼写检查库、chatbot-list - 行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍、语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)、 用138GB语料训练的法文RoBERTa预训练语言模型 、BERT-NER-Pytorch：三种不同模式的BERT中文NER实验、无道词典 - 有道词典的命令行版本，支持英汉互查和在线查询、2019年NLP亮点回顾、 Chinese medical dialogue data 中文医疗对话数据集 、最好的汉字数字(中文数字)-阿拉伯数字转换工具、 基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧、awesome-nlp-sentiment-analysis - 情感分析、情绪原因识别、评价对象和评价词抽取、LineFlow：面向所有深度学习框架的NLP数据高效加载器、中文医学NLP公开资源整理 、MedQuAD：(英文)医学问答数据集、将自然语言数字串解析转换为整数和浮点数、Transfer Learning in Natural Language Processing (NLP) 、面向语音识别的中文/英文发音辞典、Tokenizers：注重性能与多功能性的最先进分词器、CLUENER 细粒度命名实体识别 Fine Grained Named Entity Recognition、 基于BERT的中文命名实体识别、中文谣言数据库、NLP数据集/基准任务大列表、nlp相关的一些论文及代码, 包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflow 、Python文本挖掘/NLP实战示例、 Blackstone：面向非结构化法律文本的spaCy pipeline和NLP模型通过同义词替换实现文本“变脸” 、中文 预训练 ELECTREA 模型: 基于对抗学习 pretrain Chinese Model 、albert-chinese-ner - 用预训练语言模型ALBERT做中文NER 、基于GPT2的特定主题文本生成/文本增广、开源预训练语言模型合集、多语言句向量包、编码、标记和实现：一种可控高效的文本生成方法、 英文脏话大列表 、attnvis：GPT2、BERT等transformer语言模型注意力交互可视化、CoVoST：Facebook发布的多语种语音-文本翻译语料库，包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文、Jiagu自然语言处理工具 - 以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能、用unet实现对文档表格的自动检测，表格重建、NLP事件提取文献资源列表 、 金融领域自然语言处理研究资源大列表、CLUEDatasetSearch - 中英文NLP数据集：搜索所有中文NLP数据集，附常用英文NLP数据集 、medical_NER - 中文医学知识图谱命名实体识别 、(哈佛)讲因果推理的免费书、知识图谱相关学习资料/数据集/工具资源大列表、Forte：灵活强大的自然语言处理pipeline工具集 、Python字符串相似性算法库、PyLaia：面向手写文档分析的深度学习工具包、TextFooler：针对文本分类/推理的对抗文本生成模块、Haystack：灵活、强大的可扩展问答(QA)框架、中文关键短语抽取工具",
      "url": "https://github.com/fighting41love/funNLP",
      "published_date": "2018-08-21T11:20:39Z",
      "stars": 74748,
      "forks": 14912,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "medical",
        "Machine Learning",
        "nlp",
        "NLP",
        "ML"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "fighting41love"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "torchio",
      "content": "Medical imaging processing for AI applications.",
      "url": "https://github.com/TorchIO-project/torchio",
      "published_date": "2019-11-26T09:10:09Z",
      "stars": 2235,
      "forks": 248,
      "language": "Python",
      "entities": [
        "Medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "TorchIO-project"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "nitrain",
      "content": "Train AI models efficiently on medical images using any framework",
      "url": "https://github.com/nitrain/nitrain",
      "published_date": "2017-03-01T02:42:12Z",
      "stars": 1871,
      "forks": 303,
      "language": "Python",
      "entities": [
        "medical",
        "Train AI",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "nitrain"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "paperai",
      "content": "📄 🤖 AI for medical and scientific papers",
      "url": "https://github.com/neuml/paperai",
      "published_date": "2020-07-21T18:33:30Z",
      "stars": 1442,
      "forks": 112,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "neuml"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AnkiAIUtils",
      "content": "AI-powered tools to enhance Anki flashcards with explanations, mnemonics, illustrations, and adaptive learning for medical school and beyond",
      "url": "https://github.com/thiswillbeyourgithub/AnkiAIUtils",
      "published_date": "2024-03-29T14:40:31Z",
      "stars": 763,
      "forks": 26,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "thiswillbeyourgithub"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "INSIGHT",
      "content": "INSIGHT is an autonomous AI that can do medical research!",
      "url": "https://github.com/oneil512/INSIGHT",
      "published_date": "2023-04-08T15:20:14Z",
      "stars": 409,
      "forks": 58,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "oneil512"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "annotateai",
      "content": "📝 Automatically annotate papers using LLMs ",
      "url": "https://github.com/neuml/annotateai",
      "published_date": "2024-12-12T15:51:02Z",
      "stars": 332,
      "forks": 33,
      "language": "Python",
      "entities": [],
      "keywords": [
        "medical AI"
      ],
      "owner": "neuml"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "hi-ml",
      "content": "HI-ML toolbox for deep learning for medical imaging and Azure integration",
      "url": "https://github.com/microsoft/hi-ml",
      "published_date": "2021-07-01T11:03:52Z",
      "stars": 291,
      "forks": 59,
      "language": "Python",
      "entities": [
        "medical",
        "deep learning",
        "ML"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "microsoft"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AIDoctor",
      "content": "AIDoctor training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining, Supervised Finetuning, RLHF(Reward Modeling and Reinforcement Learning) and DPO(Direct Preferenc…",
      "url": "https://github.com/Jerry-XDL/AIDoctor",
      "published_date": "2025-02-23T08:17:56Z",
      "stars": 274,
      "forks": 22,
      "language": "Python",
      "entities": [
        "medical"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "Jerry-XDL"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "kaapana",
      "content": "Kaapana is an open source toolkit for state of the art platform provisioning in the field of medical data analysis. The applications comprise AI-based workflows and federated learning scenarios with a focus on radiological and radiotherapeutic imaging. The name Kaapana comes from the Hawaiian word kaʻāpana, meaning \"distributor\" or \"part\".",
      "url": "https://github.com/kaapana/kaapana",
      "published_date": "2020-08-06T09:28:22Z",
      "stars": 213,
      "forks": 51,
      "language": "Python",
      "entities": [
        "medical",
        "AI"
      ],
      "keywords": [
        "medical AI"
      ],
      "owner": "kaapana"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "MONAI",
      "content": "AI Toolkit for Healthcare Imaging",
      "url": "https://github.com/Project-MONAI/MONAI",
      "published_date": "2019-10-11T16:41:38Z",
      "stars": 6596,
      "forks": 1217,
      "language": "Python",
      "entities": [
        "Healthcare",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "Project-MONAI"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
      "content": "A comprehensive list of Deep Learning / Artificial Intelligence and Machine Learning tutorials - rapidly expanding into areas of AI/Deep Learning / Machine Vision / NLP and industry specific areas such as Climate / Energy, Automotives, Retail, Pharma, Medicine, Healthcare, Policy, Ethics and more.",
      "url": "https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
      "published_date": "2017-07-13T19:46:01Z",
      "stars": 3883,
      "forks": 1633,
      "language": "Python",
      "entities": [
        "Machine Learning",
        "Deep Learning",
        "Healthcare",
        "Medicine",
        "NLP",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "TarrySingh"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "hi-ml",
      "content": "HI-ML toolbox for deep learning for medical imaging and Azure integration",
      "url": "https://github.com/microsoft/hi-ml",
      "published_date": "2021-07-01T11:03:52Z",
      "stars": 291,
      "forks": 59,
      "language": "Python",
      "entities": [
        "medical",
        "deep learning",
        "ML"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "microsoft"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "fuse-med-ml",
      "content": "A python framework accelerating ML based discovery in the medical field by encouraging code reuse. Batteries included :)",
      "url": "https://github.com/BiomedSciAI/fuse-med-ml",
      "published_date": "2021-06-23T12:10:42Z",
      "stars": 148,
      "forks": 36,
      "language": "Python",
      "entities": [
        "medical",
        "ML"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "BiomedSciAI"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "medicare_locator",
      "content": "🏥Medicare Locator - Open source starter pack for developers to build contextual chatbots and AI assistants in healthcare",
      "url": "https://github.com/RasaHQ/medicare_locator",
      "published_date": "2018-11-21T14:47:45Z",
      "stars": 147,
      "forks": 136,
      "language": "Python",
      "entities": [
        "healthcare",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "RasaHQ"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "HealthChatbot",
      "content": "🤖 HealthCare ChatBot Major -1 (4th year - 7th semester)  Health Care Chat-Bot is a Healthcare Domain Chatbot to simulate the predictions of a General Physician.  ChatBot can be described as software that can chat with people using artificial intelligence. These software are used to perform tasks such as quickly responding to users, informing them, helping to purchase products and providing better service to customers. We have made a healthcare based chatbot.  The three main areas where chatbots can be used are diagnostics, patient engagement outside medical facilities, and mental health. In our major we are working on diagnostic.  📃 Brief A chatbot is an artificially intelligent creature which can converse with humans. This could be text-based, or a spoken conversation. In our project we will be using Python as it is currently the most popular language for creating an AI chatbot. In the middle of AI chatbot, architecture is the Natural Language Processing (NLP) layer.  This project aims to build an user-friendly healthcare chatbot which facilitates the job of a healthcare provider and helps improve their performance by interacting with users in a human-like way.  Through chatbots one can communicate with text or voice interface and get reply through artificial intelligence  Typically, a chat bot will communicate with a real person. Chat bots are used in applications such as E-commerce customer service, Call centres, Internet gaming,etc.  Chatbots are programs built to automatically engage with received messages. Chatbots can be programmed to respond the same way each time, to respond differently to messages containing certain keywords and even to use machine learning to adapt their responses to fit the situation.  A developing number of hospitals, nursing homes, and even private centres, presently utilize online Chatbots for human services on their sites. These bots connect with potential patients visiting the site, helping them discover specialists, booking their appointments, and getting them access to the correct treatment.  In any case, the utilization of artificial intelligence in an industry where individuals’ lives could be in question, still starts misgivings in individuals. It brings up issues about whether the task mentioned above ought to be assigned to human staff. This healthcare chatbot system will help hospitals to provide healthcare support online 24 x 7, it answers deep as well as general questions. It also helps to generate leads and automatically delivers the information of leads to sales. By asking the questions in series it helps patients by guiding what exactly he/she is looking for.  📜 Problem Statement During the pandemic, it is more important than ever to get your regular check-ups and to continue to take prescription medications. The healthier you are, the more likely you are to recover quickly from an illness.  In this time patients or health care workers within their practice, providers are deferring elective and preventive visits, such as annual physicals. For some, it is not possible to consult online. In this case, to avoid false information, our project can be of help.  📇 Features Register Screen. Sign-in Screen. Generates database for user login system. Offers you a GUI Based Chatbot for patients for diagnosing. [A pragmatic Approach for Diagnosis] Reccomends an appropriate doctor to you for the following symptom. 📜 Modules Used Our program uses a number of python modules to work properly:  tkinter os webbrowser numpy pandas matplotlib 📃 Algorithm We have used Decision tree for our health care based chat bot.  Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.It usually mimic human thinking ability while making a decision, so it is easy to understand.  :suspect: Project Members Anushka Bansal - 500067844 - R164218014 Shreya Sharma - 500068573 - R164218070 Silvi - 500069092 - R164218072 Ishika Agrawal - 500071154 - R164218097",
      "url": "https://github.com/shreyasharma04/HealthChatbot",
      "published_date": "2022-02-21T08:27:25Z",
      "stars": 113,
      "forks": 20,
      "language": "Python",
      "entities": [
        "Natural Language Processing",
        "machine learning",
        "medical",
        "healthcare",
        "Healthcare",
        "Diagnosis",
        "symptom",
        "NLP",
        "patient",
        "treatment",
        "HealthCare",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "shreyasharma04"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "AI-Healthcare-chatbot",
      "content": "Through a series of questions about symptoms it diagnosis the health condition of patient. ",
      "url": "https://github.com/vsharathchandra/AI-Healthcare-chatbot",
      "published_date": "2018-03-20T16:52:42Z",
      "stars": 86,
      "forks": 64,
      "language": "Python",
      "entities": [
        "diagnosis",
        "patient"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "vsharathchandra"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "cyclops",
      "content": "A toolkit for evaluating and monitoring AI models in clinical settings",
      "url": "https://github.com/VectorInstitute/cyclops",
      "published_date": "2022-02-21T21:15:08Z",
      "stars": 85,
      "forks": 13,
      "language": "Python",
      "entities": [
        "clinical",
        "AI"
      ],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "VectorInstitute"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "EEGWaveNet",
      "content": "source codes for EEGWaveNet: Multi-Scale CNN-Based Spatiotemporal Feature Extraction for EEG Seizure Detection (IEEE Transactions on Industrial Informatics)",
      "url": "https://github.com/IoBT-VISTEC/EEGWaveNet",
      "published_date": "2021-05-17T12:06:49Z",
      "stars": 57,
      "forks": 7,
      "language": "Python",
      "entities": [],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "IoBT-VISTEC"
    },
    {
      "source_type": "github",
      "source_name": "GitHub",
      "title": "fedbiomed",
      "content": "A collaborative learning framework for empowering biomedical research",
      "url": "https://github.com/fedbiomed/fedbiomed",
      "published_date": "2023-05-30T06:23:36Z",
      "stars": 54,
      "forks": 10,
      "language": "Python",
      "entities": [],
      "keywords": [
        "healthcare AI"
      ],
      "owner": "fedbiomed"
    }
  ],
  "research_papers": [
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
      "content": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
      "url": "http://arxiv.org/abs/2507.07996v1",
      "published_date": "2025-07-10",
      "entities": [
        "neural network"
      ],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07996v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
      "content": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
      "url": "http://arxiv.org/abs/2507.07984v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07984v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
      "content": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
      "url": "http://arxiv.org/abs/2507.07982v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07982v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Pierce-Birkhoff conjecture is true for splines",
      "content": "We prove the Pierce--Birkhoff conjecture for splines, i.e., continuous\npiecewise polynomials of degree $d$ in $n$ variables on a hyperplane partition\nof $\\mathbb{R}^n$, can be written as a finite lattice combination of\npolynomials. We will provide a purely existential proof, followed by a more\nin-depth analysis that yields effective bounds.",
      "url": "http://arxiv.org/abs/2507.07976v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07976v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Constraints from CMB lensing tomography with projected bispectra",
      "content": "We measure the angular power spectrum and bispectrum of the projected\noverdensity of photometric DESI luminous red galaxies, and its\ncross-correlation with maps of the Cosmic Microwave Background lensing\nconvergence from \\planck. This analysis is enabled by the use of the\n``filtered-squared bispectrum'' approach, introduced in previous work, which we\ngeneralise here to the case of cross-correlations between multiple fields. The\nprojected galaxy bispectrum is detected at very high significance (above\n$30\\sigma$ in all redshift bins), and the galaxy-galaxy-convergence bispectrum\nis detected above $5\\sigma$ in the three highest-redshift bins. We find that\nthe bispectrum is reasonably well described over a broad range of scales by a\ntree-level prediction using the linear galaxy bias measured from the power\nspectrum. We carry out the first cosmological analysis combining projected\npower spectra and bispectra under a relatively simple model, and show that the\ngalaxy bispectrum can be used in combination with the power spectrum to place a\nconstraint on the amplitude of matter fluctuations, $\\sigma_8$, an on the\nnon-relativistic matter fraction $\\Omega_m$. We find that data combinations\ninvolving the galaxy bispectrum recover constraints on these parameters that\nare in good agreement with those found from the traditional\n``2$\\times$2-point'' combination of galaxy-galaxy and galaxy-convergence power\nspectra, across all redshift bins.",
      "url": "http://arxiv.org/abs/2507.07968v1",
      "published_date": "2025-07-10",
      "entities": [
        "traditional"
      ],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07968v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Sharp estimates of quantum covering problems via a novel trace\n  inequality",
      "content": "In this paper, we prove a novel trace inequality involving two operators. As\napplications, we sharpen the one-shot achievability bound on the relative\nentropy error in a wealth of quantum covering-type problems, such as soft\ncovering, privacy amplification, convex splitting, quantum information\ndecoupling, and quantum channel simulation by removing some dimension-dependent\nfactors. Moreover, the established one-shot bounds extend to\ninfinite-dimensional separable Hilbert spaces as well. The proof techniques are\nbased on the recently developed operator layer cake theorem and an operator\nchange-of-variable argument, which are of independent interest.",
      "url": "http://arxiv.org/abs/2507.07961v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07961v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Constructing Optimal Kobon Triangle Arrangements via Table Encoding, SAT\n  Solving, and Heuristic Straightening",
      "content": "We present new methods and results for constructing optimal Kobon triangle\narrangements. First, we introduce a compact table notation for describing\narrangements of pseudolines, enabling the representation and analysis of\ncomplex cases, including symmetrical arrangements, arrangements with parallel\nlines, and arrangements with multiple-line intersection points. Building on\nthis, we provide a simple heuristic method and tools for recovering\nstraight-line arrangements from a given table, with the ability to enforce\nadditional properties such as symmetries. The tool successfully recovers\narrangements for many previously known optimal solutions. Additionally, we\ndevelop a tool that transforms the search for optimal Kobon arrangement tables\ninto a SAT problem, allowing us to leverage modern SAT solvers (specifically\nKissat) to efficiently find new solutions or to show that no other solutions\nexist (for example, confirming that no optimal solution exists in the 11-line\ncase). Using these techniques, we find new optimal Kobon arrangements for 23\nand 27 lines, along with several other new results.",
      "url": "http://arxiv.org/abs/2507.07951v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07951v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Quantum Wall States for Noise Mitigation and Eternal Purity Bounds",
      "content": "The present work analyzes state-stabilization techniques for decoupling a\nsubsystem from environmental interactions. The proposed framework uses\nanalytical and numerical tools to find an approximate decoherence-free subspace\n(DFS) with enhanced passive noise isolation. Active state-stabilizing control\non a subsystem mediating dominant environmental interactions, which we call\nwall subsystem, creates an effective quantum wall state. The proposed method\ncontrols only the wall subsystem, leaving the logical subsystem untouched. This\nsimplifies logic operations in the protected subsystem, and makes it suitable\nfor integration with other quantum information protection techniques, such as\ndynamical decoupling (DD). We demonstrated its effectiveness in enhancing the\nperformance of selective or complete DD. Under suitable conditions, our method\nmaintains system purity above a threshold for all times, achieving eternal\npurity preservation. Theoretical analysis links this behavior to the asymptotic\nspectrum of the Hamiltonian when the control gain grows unbounded.",
      "url": "http://arxiv.org/abs/2507.07944v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07944v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "A Randomized Rounding Approach for DAG Edge Deletion",
      "content": "In the DAG Edge Deletion problem, we are given an edge-weighted directed\nacyclic graph and a parameter $k$, and the goal is to delete the minimum weight\nset of edges so that the resulting graph has no paths of length $k$. This\nproblem, which has applications to scheduling, was introduced in 2015 by\nKenkre, Pandit, Purohit, and Saket. They gave a $k$-approximation and showed\nthat it is UGC-Hard to approximate better than $\\lfloor 0.5k \\rfloor$ for any\nconstant $k \\ge 4$ using a work of Svensson from 2012. The approximation ratio\nwas improved to $\\frac{2}{3}(k+1)$ by Klein and Wexler in 2016.\n  In this work, we introduce a randomized rounding framework based on\ndistributions over vertex labels in $[0,1]$. The most natural distribution is\nto sample labels independently from the uniform distribution over $[0,1]$. We\nshow this leads to a $(2-\\sqrt{2})(k+1) \\approx 0.585(k+1)$-approximation. By\nusing a modified (but still independent) label distribution, we obtain a\n$0.549(k+1)$-approximation for the problem, as well as show that no independent\ndistribution over labels can improve our analysis to below $0.542(k+1)$.\nFinally, we show a $0.5(k+1)$-approximation for bipartite graphs and for\ninstances with structured LP solutions. Whether this ratio can be obtained in\ngeneral is open.",
      "url": "http://arxiv.org/abs/2507.07943v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07943v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Late Fusion Multi-task Learning for Semiparametric Inference with\n  Nuisance Parameters",
      "content": "In the age of large and heterogeneous datasets, the integration of\ninformation from diverse sources is essential to improve parameter estimation.\nMulti-task learning offers a powerful approach by enabling simultaneous\nlearning across related tasks. In this work, we introduce a late fusion\nframework for multi-task learning with semiparametric models that involve\ninfinite-dimensional nuisance parameters, focusing on applications such as\nheterogeneous treatment effect estimation across multiple data sources,\nincluding electronic health records from different hospitals or clinical trial\ndata. Our framework is two-step: first, initial double machine-learning\nestimators are obtained through individual task learning; second, these\nestimators are adaptively aggregated to exploit task similarities while\nremaining robust to task-specific differences. In particular, the framework\navoids individual level data sharing, preserving privacy. Additionally, we\npropose a novel multi-task learning method for nuisance parameter estimation,\nwhich further enhances parameter estimation when nuisance parameters exhibit\nsimilarity across tasks. We establish theoretical guarantees for the method,\ndemonstrating faster convergence rates compared to individual task learning\nwhen tasks share similar parametric components. Extensive simulations and real\ndata applications complement the theoretical findings of our work while\nhighlight the effectiveness of our framework even in moderate sample sizes.",
      "url": "http://arxiv.org/abs/2507.07941v1",
      "published_date": "2025-07-10",
      "entities": [
        "clinical",
        "treatment"
      ],
      "keywords": [
        "voice analysis health"
      ],
      "arxiv_id": "2507.07941v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
      "content": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
      "url": "http://arxiv.org/abs/2507.07957v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07957v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Intraseasonal Equatorial Kelvin and Rossby Waves in Modern AI-ML Models",
      "content": "We examine the structure of large-scale convectively coupled Kelvin and\nRossby waves in a suite of modern AI-ML models. In particular, multiple runs of\nPanguWeather, GraphCast, FourCastNet and Aurora are performed to assess the\nstructure of the aforementioned waves. Wavenumber-frequency diagrams of zonal\nwinds from all models show a clear signature of Rossby and Kelvin waves with\nequivalent depths that are in accord with observations and reanalysis.\nComposites of Kelvin waves show correct lower and upper troposphere horizontal\nconvergence patterns, vertical tilts in temperature, humidity and vertical\nvelocity as well as the phase relation between temperature and vertical\nvelocity anomalies. Though, differences between models are notable such as\nsmaller vertical tilts and incorrect surface temperature anomalies in GraphCast\nand relatively weak convergent flows in PanguWeather. The models had much more\ndifficulty with Rossby waves; while the horizontal gyres were captured, the\nvertical structure of temperature and divergence was incorrect. Apart from\nunexpected tilts in various fields, the temperature anomaly was inconsistent\nwith the nature of the vertical velocity in all four models. Curiously,\nmoisture and vertical velocity anomalies were much closer to observations.\nFurther, only two models (GraphCast and FourCastNet) captured the simultaneous\ngeneration of deep vertical motion with moisture anomalies. In all, while the\nrepresentation of these large-scale waves is encouraging, issues with the\nstructure of Rossby waves and especially the inconsistency among fields require\nfurther investigation.",
      "url": "http://arxiv.org/abs/2507.07952v1",
      "published_date": "2025-07-10",
      "entities": [
        "ML",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07952v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Multimodal Framework for Explainable Autonomous Driving: Integrating\n  Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency",
      "content": "Autonomous vehicles (AVs) are poised to redefine transportation by enhancing\nroad safety, minimizing human error, and optimizing traffic efficiency. The\nsuccess of AVs depends on their ability to interpret complex, dynamic\nenvironments through diverse data sources, including video streams, sensor\nmeasurements, and contextual textual information. However, seamlessly\nintegrating these multimodal inputs and ensuring transparency in AI-driven\ndecisions remain formidable challenges. This study introduces a novel\nmultimodal framework that synergistically combines video, sensor, and textual\ndata to predict driving actions while generating human-readable explanations,\nfostering trust and regulatory compliance. By leveraging VideoMAE for\nspatiotemporal video analysis, a custom sensor fusion module for real-time data\nprocessing, and BERT for textual comprehension, our approach achieves robust\ndecision-making and interpretable outputs. Evaluated on the BDD-X (21113\nsamples) and nuScenes (1000 scenes) datasets, our model reduces training loss\nfrom 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy\nof 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming\nstate-of-the-art methods. Ablation studies confirm the critical role of each\nmodality, while qualitative analyses and human evaluations highlight the\nmodel's ability to produce contextually rich, user-friendly explanations. These\nadvancements underscore the transformative potential of multimodal integration\nand explainability in building safe, transparent, and trustworthy AV systems,\npaving the way for broader societal adoption of autonomous driving\ntechnologies.",
      "url": "http://arxiv.org/abs/2507.07938v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07938v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Working with AI: Measuring the Occupational Implications of Generative\n  AI",
      "content": "Given the rapid adoption of generative AI and its potential to impact a wide\nrange of tasks, understanding the effects of AI on the economy is one of\nsociety's most important questions. In this work, we take a step toward that\ngoal by analyzing the work activities people do with AI, how successfully and\nbroadly those activities are done, and combine that with data on what\noccupations do those activities. We analyze a dataset of 200k anonymized and\nprivacy-scrubbed conversations between users and Microsoft Bing Copilot, a\npublicly available generative AI system. We find the most common work\nactivities people seek AI assistance for involve gathering information and\nwriting, while the most common activities that AI itself is performing are\nproviding information and assistance, writing, teaching, and advising.\nCombining these activity classifications with measurements of task success and\nscope of impact, we compute an AI applicability score for each occupation. We\nfind the highest AI applicability scores for knowledge work occupation groups\nsuch as computer and mathematical, and office and administrative support, as\nwell as occupations such as sales whose work activities involve providing and\ncommunicating information. Additionally, we characterize the types of work\nactivities performed most successfully, how wage and education correlate with\nAI applicability, and how real-world usage compares to predictions of\noccupational AI impact.",
      "url": "http://arxiv.org/abs/2507.07935v1",
      "published_date": "2025-07-10",
      "entities": [
        "Microsoft",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07935v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Meek Models Shall Inherit the Earth",
      "content": "The past decade has seen incredible scaling of AI systems by a few companies,\nleading to inequality in AI model performance. This paper argues that, contrary\nto prevailing intuition, the diminishing returns to compute scaling will lead\nto a convergence of AI model capabilities. In other words, meek models (those\nwith limited computation budget) shall inherit the earth, approaching the\nperformance level of the best models overall. We develop a model illustrating\nthat under a fixed-distribution next-token objective, the marginal capability\nreturns to raw compute shrink substantially. Given current scaling practices,\nwe argue that these diminishing returns are strong enough that even companies\nthat can scale their models exponentially faster than other organizations will\neventually have little advantage in capabilities. As part of our argument, we\ngive several reasons that proxies like training loss differences capture\nimportant capability measures using evidence from benchmark data and\ntheoretical performance models. In addition, we analyze empirical data on the\ncapability difference of AI models over time. Finally, in light of the\nincreasing ability of meek models, we argue that AI strategy and policy require\nreexamination, and we outline the areas this shift will affect.",
      "url": "http://arxiv.org/abs/2507.07931v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07931v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training",
      "content": "Background: Public speaking is a vital professional skill, yet it remains a\nsource of significant anxiety for many individuals. Traditional training relies\nheavily on expert coaching, but recent advances in AI has led to novel types of\ncommercial automated public speaking feedback tools. However, most research has\nfocused on prototypes rather than commercial applications, and little is known\nabout how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and\ndesign of commercial AI-based public speaking training tools and to propose\nguidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus\ngroups with public speaking experts. Participants discussed their views on\ncurrent commercial tools, their potential integration into traditional\ncoaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in\nhandling repetitive, technical aspects of training, allowing coaches to focus\non higher-level skills. However they found key issues in current tools,\nemphasising the need for personalised, understandable, carefully selected\nfeedback and clear instructional design. Overall, they supported a hybrid model\ncombining traditional coaching with AI-supported exercises.",
      "url": "http://arxiv.org/abs/2507.07930v1",
      "published_date": "2025-07-10",
      "entities": [
        "Traditional",
        "traditional",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07930v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Agentic Retrieval of Topics and Insights from Earnings Calls",
      "content": "Tracking the strategic focus of companies through topics in their earnings\ncalls is a key task in financial analysis. However, as industries evolve,\ntraditional topic modeling techniques struggle to dynamically capture emerging\ntopics and their relationships. In this work, we propose an LLM-agent driven\napproach to discover and retrieve emerging topics from quarterly earnings\ncalls. We propose an LLM-agent to extract topics from documents, structure them\ninto a hierarchical ontology, and establish relationships between new and\nexisting topics through a topic ontology. We demonstrate the use of extracted\ntopics to infer company-level insights and emerging trends over time. We\nevaluate our approach by measuring ontology coherence, topic evolution\naccuracy, and its ability to surface emerging financial trends.",
      "url": "http://arxiv.org/abs/2507.07906v1",
      "published_date": "2025-07-10",
      "entities": [
        "traditional"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07906v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "MIRA: A Novel Framework for Fusing Modalities in Medical RAG",
      "content": "Multimodal Large Language Models (MLLMs) have significantly advanced\nAI-assisted medical diagnosis, but they often generate factually inconsistent\nresponses that deviate from established medical knowledge. Retrieval-Augmented\nGeneration (RAG) enhances factual accuracy by integrating external sources, but\nit presents two key challenges. First, insufficient retrieval can miss critical\ninformation, whereas excessive retrieval can introduce irrelevant or misleading\ncontent, disrupting model output. Second, even when the model initially\nprovides correct answers, over-reliance on retrieved data can lead to factual\nerrors. To address these issues, we introduce the Multimodal Intelligent\nRetrieval and Augmentation (MIRA) framework, designed to optimize factual\naccuracy in MLLM. MIRA consists of two key components: (1) a calibrated\nRethinking and Rearrangement module that dynamically adjusts the number of\nretrieved contexts to manage factual risk, and (2) A medical RAG framework\nintegrating image embeddings and a medical knowledge base with a query-rewrite\nmodule for efficient multimodal reasoning. This enables the model to\neffectively integrate both its inherent knowledge and external references. Our\nevaluation of publicly available medical VQA and report generation benchmarks\ndemonstrates that MIRA substantially enhances factual accuracy and overall\nperformance, achieving new state-of-the-art results. Code is released at\nhttps://github.com/mbzuai-oryx/MIRA.",
      "url": "http://arxiv.org/abs/2507.07902v1",
      "published_date": "2025-07-10",
      "entities": [
        "medical",
        "diagnosis",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07902v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "The Trust Fabric: Decentralized Interoperability and Economic\n  Coordination for the Agentic Web",
      "content": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.",
      "url": "http://arxiv.org/abs/2507.07901v1",
      "published_date": "2025-07-10",
      "entities": [
        "healthcare",
        "MIT",
        "AI"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07901v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Can AI-predicted complexes teach machine learning to compute drug\n  binding affinity?",
      "content": "We evaluate the feasibility of using co-folding models for synthetic data\naugmentation in training machine learning-based scoring functions (MLSFs) for\nbinding affinity prediction. Our results show that performance gains depend\ncritically on the structural quality of augmented data. In light of this, we\nestablished simple heuristics for identifying high-quality co-folding\npredictions without reference structures, enabling them to substitute for\nexperimental structures in MLSF training. Our study informs future data\naugmentation strategies based on co-folding models.",
      "url": "http://arxiv.org/abs/2507.07882v1",
      "published_date": "2025-07-10",
      "entities": [
        "machine learning"
      ],
      "keywords": [
        "facial diagnosis AI"
      ],
      "arxiv_id": "2507.07882v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Impact of Pretraining Word Co-occurrence on Compositional Generalization\n  in Multimodal Models",
      "content": "CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.",
      "url": "http://arxiv.org/abs/2507.08000v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.08000v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
      "content": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
      "url": "http://arxiv.org/abs/2507.07999v1",
      "published_date": "2025-07-10",
      "entities": [
        "OpenAI"
      ],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07999v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search",
      "content": "According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.",
      "url": "http://arxiv.org/abs/2507.07995v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07995v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
      "content": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
      "url": "http://arxiv.org/abs/2507.07996v1",
      "published_date": "2025-07-10",
      "entities": [
        "neural network"
      ],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07996v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection",
      "content": "Keypoint detection, integral to modern machine perception, faces challenges\nin few-shot learning, particularly when source data from the same distribution\nas the query is unavailable. This gap is addressed by leveraging sketches, a\npopular form of human expression, providing a source-free alternative. However,\nchallenges arise in mastering cross-modal embeddings and handling user-specific\nsketch styles. Our proposed framework overcomes these hurdles with a\nprototypical setup, combined with a grid-based locator and prototypical domain\nadaptation. We also demonstrate success in few-shot convergence across novel\nkeypoints and classes through extensive experiments.",
      "url": "http://arxiv.org/abs/2507.07994v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07994v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "EXPO: Stable Reinforcement Learning with Expressive Policies",
      "content": "We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.",
      "url": "http://arxiv.org/abs/2507.07986v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07986v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is\n  Why",
      "content": "Contrastive vision-language models like CLIP are used for a large variety of\napplications, such as zero-shot classification or as vision encoder for\nmulti-modal models. Despite their popularity, their representations show major\nlimitations. For instance, CLIP models learn bag-of-words representations and,\nas a consequence, fail to distinguish whether an image is of \"a yellow\nsubmarine and a blue bus\" or \"a blue submarine and a yellow bus\". Previous\nattempts to fix this issue added hard negatives during training or modified the\narchitecture, but failed to resolve the problem in its entirety. We suspect\nthat the missing insights to solve the binding problem for CLIP are hidden in\nthe arguably most important part of learning algorithms: the data. In this\nwork, we fill this gap by rigorously identifying the influence of data\nproperties on CLIP's ability to learn binding using a synthetic dataset. We\nfind that common properties of natural data such as low attribute density,\nincomplete captions, and the saliency bias, a tendency of human captioners to\ndescribe the object that is \"most salient\" to them have a detrimental effect on\nbinding performance. In contrast to common belief, we find that neither scaling\nthe batch size, i.e., implicitly adding more hard negatives, nor explicitly\ncreating hard negatives enables CLIP to learn reliable binding. Only when the\ndata expresses our identified data properties CLIP learns almost perfect\nbinding.",
      "url": "http://arxiv.org/abs/2507.07985v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07985v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
      "content": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
      "url": "http://arxiv.org/abs/2507.07982v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07982v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Why is Your Language Model a Poor Implicit Reward Model?",
      "content": "Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Towards a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.",
      "url": "http://arxiv.org/abs/2507.07981v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07981v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Reinforcement Learning with Action Chunking",
      "content": "We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.",
      "url": "http://arxiv.org/abs/2507.07969v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "tongue diagnosis deep learning"
      ],
      "arxiv_id": "2507.07969v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Impact of Pretraining Word Co-occurrence on Compositional Generalization\n  in Multimodal Models",
      "content": "CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.",
      "url": "http://arxiv.org/abs/2507.08000v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.08000v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Multigranular Evaluation for Brain Visual Decoding",
      "content": "Existing evaluation protocols for brain visual decoding predominantly rely on\ncoarse metrics that obscure inter-model differences, lack neuroscientific\nfoundation, and fail to capture fine-grained visual distinctions. To address\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\nframework that jointly quantifies structural fidelity, inferential alignment,\nand contextual coherence between decoded and ground truth images. For the\nstructural level, we introduce a hierarchical suite of segmentation-based\nmetrics, including foreground, semantic, instance, and component masks,\nanchored in granularity-aware correspondence across mask structures. For the\nsemantic level, we extract structured scene representations encompassing\nobjects, attributes, and relationships using multimodal large language models,\nenabling detailed, scalable, and context-rich comparisons with ground-truth\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\nstimulus-neuroimaging datasets within this unified evaluation framework.\nTogether, these criteria provide a more discriminative, interpretable, and\ncomprehensive foundation for measuring brain visual decoding methods.",
      "url": "http://arxiv.org/abs/2507.07993v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07993v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Automating Expert-Level Medical Reasoning Evaluation of Large Language\n  Models",
      "content": "As large language models (LLMs) become increasingly integrated into clinical\ndecision-making, ensuring transparent and trustworthy reasoning is essential.\nHowever, existing evaluation strategies of LLMs' medical reasoning capability\neither suffer from unsatisfactory assessment or poor scalability, and a\nrigorous benchmark remains lacking. To address this, we introduce\nMedThink-Bench, a benchmark designed for rigorous, explainable, and scalable\nassessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging\nquestions across ten medical domains, each annotated with expert-crafted\nstep-by-step rationales. Building on this, we propose LLM-w-Ref, a novel\nevaluation framework that leverages fine-grained rationales and LLM-as-a-Judge\nmechanisms to assess intermediate reasoning with expert-level fidelity while\nmaintaining scalability. Experiments show that LLM-w-Ref exhibits a strong\npositive correlation with expert judgments. Benchmarking twelve\nstate-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can\nsurpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,\nMedThink-Bench offers a foundational tool for evaluating LLMs' medical\nreasoning, advancing their safe and responsible deployment in clinical\npractice.",
      "url": "http://arxiv.org/abs/2507.07988v1",
      "published_date": "2025-07-10",
      "entities": [
        "clinical",
        "medical",
        "OpenAI"
      ],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07988v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
      "content": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
      "url": "http://arxiv.org/abs/2507.07984v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07984v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Martian World Models: Controllable Video Synthesis with Physically\n  Accurate 3D Reconstructions",
      "content": "Synthesizing realistic Martian landscape videos is crucial for mission\nrehearsal and robotic simulation. However, this task poses unique challenges\ndue to the scarcity of high-quality Martian data and the significant domain gap\nbetween Martian and terrestrial imagery. To address these challenges, we\npropose a holistic solution composed of two key components: 1) A data curation\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\nenvironments from real stereo navigation images, sourced from NASA's Planetary\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\nMartian terrain video generator, MarsGen, which synthesizes novel videos\nvisually realistic and geometrically consistent with the 3D structure encoded\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\nacquisition dates, enabling the generation of physically accurate 3D surface\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\nsynthesizes videos conditioned on an initial image frame and, optionally,\ncamera trajectories or textual prompts, allowing for video generation in novel\nenvironments. Experimental results show that our approach outperforms video\nsynthesis models trained on terrestrial datasets, achieving superior visual\nfidelity and 3D structural consistency.",
      "url": "http://arxiv.org/abs/2507.07978v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07978v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
      "content": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
      "url": "http://arxiv.org/abs/2507.07957v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07957v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement\n  and Entropy-aware Alignment",
      "content": "While Vision-Language Models (VLMs) have shown promising progress in general\nmultimodal tasks, they often struggle in industrial anomaly detection and\nreasoning, particularly in delivering interpretable explanations and\ngeneralizing to unseen categories. This limitation stems from the inherently\ndomain-specific nature of anomaly detection, which hinders the applicability of\nexisting VLMs in industrial scenarios that require precise, structured, and\ncontext-aware analysis. To address these challenges, we propose SAGE, a\nVLM-based framework that enhances anomaly reasoning through Self-Guided Fact\nEnhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE\nintegrates domain-specific knowledge into visual reasoning via fact extraction\nand fusion, while E-DPO aligns model outputs with expert preferences using\nentropy-aware optimization. Additionally, we introduce AD-PL, a\npreference-optimized dataset tailored for industrial anomaly reasoning,\nconsisting of 28,415 question-answering instances with expert-ranked responses.\nTo evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation\n(MLE), a quantitative framework analyzing model logic and consistency. SAGE\ndemonstrates superior performance on industrial anomaly datasets under\nzero-shot and one-shot settings. The code, model and dataset are available at\nhttps://github.com/amoreZgx1n/SAGE.",
      "url": "http://arxiv.org/abs/2507.07939v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07939v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Multimodal Framework for Explainable Autonomous Driving: Integrating\n  Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency",
      "content": "Autonomous vehicles (AVs) are poised to redefine transportation by enhancing\nroad safety, minimizing human error, and optimizing traffic efficiency. The\nsuccess of AVs depends on their ability to interpret complex, dynamic\nenvironments through diverse data sources, including video streams, sensor\nmeasurements, and contextual textual information. However, seamlessly\nintegrating these multimodal inputs and ensuring transparency in AI-driven\ndecisions remain formidable challenges. This study introduces a novel\nmultimodal framework that synergistically combines video, sensor, and textual\ndata to predict driving actions while generating human-readable explanations,\nfostering trust and regulatory compliance. By leveraging VideoMAE for\nspatiotemporal video analysis, a custom sensor fusion module for real-time data\nprocessing, and BERT for textual comprehension, our approach achieves robust\ndecision-making and interpretable outputs. Evaluated on the BDD-X (21113\nsamples) and nuScenes (1000 scenes) datasets, our model reduces training loss\nfrom 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy\nof 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming\nstate-of-the-art methods. Ablation studies confirm the critical role of each\nmodality, while qualitative analyses and human evaluations highlight the\nmodel's ability to produce contextually rich, user-friendly explanations. These\nadvancements underscore the transformative potential of multimodal integration\nand explainability in building safe, transparent, and trustworthy AV systems,\npaving the way for broader societal adoption of autonomous driving\ntechnologies.",
      "url": "http://arxiv.org/abs/2507.07938v1",
      "published_date": "2025-07-10",
      "entities": [
        "AI"
      ],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07938v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "Hydrodynamic Insight Drives Multimodal Vortex-Field Dynamics via\n  Streamline Engineering",
      "content": "Since the 1970s, analogies between laser dynamics and (super)fluid systems\nhave elucidated phenomena from superconductivity to BoseEinstein condensation.\nInspired by Coullet et al.s 1989 formalization of optical vortices via\nhydrodynamic whirlpools, we here advance a hydrodynamic paradigm for\nvortex_beam propagation. By treating the Poynting_vector trajectories as energy\nstreamlines, we establish a three_dimensional map of photon trajectories and\nenergy flow. Angular_spectrum engineering in momentum space, together with the\nfluid_continuity equation, is used to sculpt these streamlines and thereby\ntailor multimodal, independently tunable propagation behaviors. The resulting\nbeams simultaneously suppress diffraction_ and OAM_induced broadening and\ninherit the diffraction_free, self_healing, self_accelerating, and self_similar\ntraits of classic structured modes, with adjustable energy_density profiles to\ncompensate loss. Optical_tweezer measurements, akin to particle_tracking\nvelocimetry of fluid dynamics, confirm that trapped microspheres trace the\nprescribed streamlines, laying the groundwork for future precision\nthree_dimensional photonic manipulation. In a free_space communication case\nstudy, streamline_engineered multimodal vortex beams deliver an\norder_of_magnitude increase in independent channels, enhanced turbulence\nresilience, and non_line_of_sight capability. This hydrodynamic framework thus\nfurnishes a versatile, experimentally verified toolkit for multimodal control\nof vortex beams and substantially broadens their application scope, while\nopening new avenues for fluid dynamics simulations using optical analogues.",
      "url": "http://arxiv.org/abs/2507.07928v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07928v1"
    },
    {
      "source_type": "research_paper",
      "source_name": "arXiv",
      "title": "The Potential of Olfactory Stimuli in Stress Reduction through Virtual\n  Reality",
      "content": "Immersive virtual reality (VR) is a promising tool for stress reduction and\nrelaxation, traditionally relying on visual and auditory stimuli. This study\nexamines the role of olfactory stimuli in enhancing these effects, using a\nrandomized within-subject design. Thirty participants aged 18-60 experienced VR\nscenarios simulating a calming seaside environment, with sessions lasting 45\nminutes, in two conditions: with and without a \"Beach\" essential oil scent\n(Yankee Candle) administered via diffuser. Stress and relaxation were assessed\nthrough self-reported surveys and physiological measures, specifically\nECG-based heart rate variability (HRV). Results showed no significant\ndifference in self-reported relaxation scores (p=0.371) between conditions, but\nHRV analysis revealed a significant stress reduction (p=0.002) with olfactory\ninput, with HF increasing 108% from the Math Stress Test to the scented\nrelaxation condition, compared to 44% without scent. Additionally, 71.4% of\nparticipants expressed willingness to use olfactory-enhanced VR for relaxation,\nsuggesting practical appeal. These findings indicate that olfactory stimuli may\nenhance relaxation subconsciously, underscoring the importance of multisensory\nintegration in VR. Future work could explore personalized scents and long-term\neffects to optimize VR- based interventions for emotional and physical\nwell-being.",
      "url": "http://arxiv.org/abs/2507.07911v1",
      "published_date": "2025-07-10",
      "entities": [],
      "keywords": [
        "multimodal medical diagnosis"
      ],
      "arxiv_id": "2507.07911v1"
    }
  ]
}