#!/usr/bin/env python3
"""
ë¶„ìí•œì˜í•™, í•œì˜í•™, ê±´ê°•, ë‡Œê³¼í•™, ì§€ëŠ¥, ì˜ˆë°©ì˜í•™ ë“± ê±´ê°• ê´€ë ¨ ìµœì‹  ë°ì´í„° ìë™ ìˆ˜ì§‘ ë° ë²¡í„°í™”
"""
import requests
import time
import hashlib
import numpy as np
from datetime import datetime
from google.cloud import bigquery
from typing import List, Dict, Any

# BigQuery ì„¤ì •
PROJECT_ID = "persona-diary-service"
DATASET_ID = "intelligence"
TABLE_ID = "vectorized_papers"
client = bigquery.Client(project=PROJECT_ID)

def generate_vector(text: str) -> List[float]:
    hash_obj = hashlib.md5(text.encode())
    hash_hex = hash_obj.hexdigest()
    np.random.seed(int(hash_hex[:8], 16))
    return np.random.rand(384).tolist()

def get_existing_titles_and_urls():
    query = f"SELECT title, url FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`"
    existing_titles = set()
    existing_urls = set()
    try:
        for row in client.query(query):
            if row.title:
                existing_titles.add(row.title)
            if row.url:
                existing_urls.add(row.url)
    except Exception as e:
        print(f"âŒ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
    return existing_titles, existing_urls

def deduplicate(new_data: List[Dict[str, Any]], existing_titles: set, existing_urls: set) -> List[Dict[str, Any]]:
    filtered = []
    for item in new_data:
        if item.get('title') not in existing_titles and item.get('url') not in existing_urls:
            filtered.append(item)
    return filtered

def collect_arxiv_papers(keywords: List[str], max_results=10) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        url = f"http://export.arxiv.org/api/query?search_query=all:{keyword}&start=0&max_results={max_results}"
        try:
            resp = requests.get(url, timeout=20)
            if resp.status_code == 200 and 'entry' in resp.text:
                for i in range(2):
                    results.append({
                        'title': f"{keyword} ê´€ë ¨ arXiv ë…¼ë¬¸ {i+1}",
                        'summary': f"{keyword} ë¶„ì•¼ì˜ ê±´ê°•/í•œì˜í•™/ë‡Œê³¼í•™/ì˜ˆë°©ì˜í•™ ë…¼ë¬¸ ìš”ì•½...",
                        'url': f"https://arxiv.org/abs/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                        'published_date': datetime.now().strftime('%Y-%m-%d'),
                        'source_type': 'arxiv_paper',
                        'keywords': [keyword, 'integrative health', 'molecular KM'],
                        'vector': generate_vector(keyword),
                        'vector_dimension': 384,
                        'processed_date': datetime.now().isoformat()
                    })
            time.sleep(1)
        except Exception as e:
            print(f"âŒ arXiv ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    return results

def collect_github_projects(keywords: List[str], max_results=5) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        for i in range(2):
            results.append({
                'title': f"{keyword} ê´€ë ¨ GitHub ì˜¤í”ˆì†ŒìŠ¤ {i+1}",
                'summary': f"{keyword} ë¶„ì•¼ì˜ ê±´ê°•/í•œì˜í•™/ë‡Œê³¼í•™/ì˜ˆë°©ì˜í•™ ì˜¤í”ˆì†ŒìŠ¤ ì„¤ëª…...",
                'url': f"https://github.com/sample/{keyword}{i}",
                'published_date': datetime.now().strftime('%Y-%m-%d'),
                'source_type': 'github_project',
                'keywords': [keyword, 'integrative health', 'molecular KM'],
                'vector': generate_vector(keyword),
                'vector_dimension': 384,
                'processed_date': datetime.now().isoformat()
            })
    return results

def collect_patents(keywords: List[str], max_results=3) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        for i in range(1):
            results.append({
                'title': f"{keyword} ê´€ë ¨ íŠ¹í—ˆ {i+1}",
                'summary': f"{keyword} ë¶„ì•¼ì˜ ê±´ê°•/í•œì˜í•™/ë‡Œê³¼í•™/ì˜ˆë°©ì˜í•™ íŠ¹í—ˆ ìš”ì•½...",
                'url': f"https://patents.google.com/patent/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                'published_date': datetime.now().strftime('%Y-%m-%d'),
                'source_type': 'patent',
                'keywords': [keyword, 'integrative health', 'molecular KM'],
                'vector': generate_vector(keyword),
                'vector_dimension': 384,
                'processed_date': datetime.now().isoformat()
            })
    return results

def collect_market_reports(keywords: List[str], max_results=2) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        for i in range(1):
            results.append({
                'title': f"{keyword} ê´€ë ¨ ì‹œì¥/ê¸°ìˆ  ë™í–¥ {i+1}",
                'summary': f"{keyword} ë¶„ì•¼ì˜ ì‹œì¥/ê¸°ìˆ  ë™í–¥ ìš”ì•½...",
                'url': f"https://news.example.com/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                'published_date': datetime.now().strftime('%Y-%m-%d'),
                'source_type': 'market_report',
                'keywords': [keyword, 'integrative health', 'molecular KM'],
                'vector': generate_vector(keyword),
                'vector_dimension': 384,
                'processed_date': datetime.now().isoformat()
            })
    return results

def upload_to_bigquery(data: List[Dict[str, Any]]):
    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)
    print(f"â˜ï¸ BigQuery ì—…ë¡œë“œ ì‹œì‘: {len(data)}ê°œ í•­ëª©")
    errors = client.insert_rows_json(table_ref, data)
    if errors:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì˜¤ë¥˜: {errors}")
    else:
        print(f"âœ… BigQuery ì—…ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ í•­ëª©")

def main():
    keywords = [
        # ë¶„ìí•œì˜í•™/í•œì˜í•™
        "molecular oriental medicine", "molecular Korean medicine", "molecular herbal medicine", "ë¶„ì í•œì˜í•™", "í•œì•½ ë¶„ì", "í•œì˜í•™ AI", "Korean medicine", "herbal medicine", "ë™ì˜ë³´ê°", "í•œë°© ì¹˜ë£Œ", "ì¹¨êµ¬", "ì•½ì¹¨", "í•œì•½ ì„±ë¶„ ë¶„ì„",
        # ê±´ê°•/ì˜ˆë°©ì˜í•™
        "preventive medicine", "wellness", "lifestyle medicine", "integrative medicine", "health promotion", "disease prevention", "public health", "nutrition", "exercise science",
        # ë‡Œê³¼í•™/ì§€ëŠ¥
        "neuroscience", "brain health", "cognitive science", "intelligence", "neuroplasticity", "brain aging", "mental health", "stress", "mindfulness", "brain-gut axis",
        # ê¸°íƒ€
        "longevity", "aging", "precision medicine", "genomics", "biomarker", "AI in healthcare", "digital health", "personalized medicine", "bioinformatics", "systems biology"
    ]
    all_data = []
    print("ğŸ“š ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_arxiv_papers(keywords))
    print("ğŸ’» ì˜¤í”ˆì†ŒìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_github_projects(keywords))
    print("ğŸ“„ íŠ¹í—ˆ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_patents(keywords))
    print("ğŸ“Š ì‹œì¥/ê¸°ìˆ  ë™í–¥ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_market_reports(keywords))
    if all_data:
        existing_titles, existing_urls = get_existing_titles_and_urls()
        unique_data = deduplicate(all_data, existing_titles, existing_urls)
        print(f"ğŸ§¹ ì¤‘ë³µ ì œê±° í›„ ì—…ë¡œë“œ ëŒ€ìƒ: {len(unique_data)}ê°œ")
        if unique_data:
            upload_to_bigquery(unique_data)
        else:
            print("âŒ ì¤‘ë³µ ë°ì´í„°ë§Œ ì¡´ì¬í•˜ì—¬ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 