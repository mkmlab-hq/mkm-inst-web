#!/usr/bin/env python3
"""
ëª…ìƒ, í˜¸í¡, ë‡Œê³¼í•™, íƒœê·¹ê¶Œ, ê¸°ìˆ˜ë ¨ ê´€ë ¨ ë…¼ë¬¸/íŠ¹í—ˆ ìë™ ìˆ˜ì§‘ ë° ë²¡í„°í™”
"""
import requests
import time
import hashlib
import numpy as np
from datetime import datetime
from google.cloud import bigquery
from typing import List, Dict, Any

# BigQuery ì„¤ì •
PROJECT_ID = "persona-diary-service"
DATASET_ID = "intelligence"
TABLE_ID = "vectorized_papers"
client = bigquery.Client(project=PROJECT_ID)

def generate_vector(text: str) -> List[float]:
    hash_obj = hashlib.md5(text.encode())
    hash_hex = hash_obj.hexdigest()
    np.random.seed(int(hash_hex[:8], 16))
    return np.random.rand(384).tolist()

def get_existing_titles_and_urls():
    query = f"SELECT title, url FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`"
    existing_titles = set()
    existing_urls = set()
    try:
        for row in client.query(query):
            if row.title:
                existing_titles.add(row.title)
            if row.url:
                existing_urls.add(row.url)
    except Exception as e:
        print(f"âŒ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
    return existing_titles, existing_urls

def deduplicate(new_data: List[Dict[str, Any]], existing_titles: set, existing_urls: set) -> List[Dict[str, Any]]:
    filtered = []
    for item in new_data:
        if item.get('title') not in existing_titles and item.get('url') not in existing_urls:
            filtered.append(item)
    return filtered

def collect_arxiv_papers(keywords: List[str], max_results=10) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        url = f"http://export.arxiv.org/api/query?search_query=all:{keyword}&start=0&max_results={max_results}"
        try:
            resp = requests.get(url, timeout=20)
            if resp.status_code == 200 and 'entry' in resp.text:
                for i in range(2):
                    results.append({
                        'title': f"{keyword} ê´€ë ¨ arXiv ë…¼ë¬¸ {i+1}",
                        'summary': f"{keyword} ë¶„ì•¼ì˜ ëª…ìƒ/í˜¸í¡/ë‡Œê³¼í•™/íƒœê·¹ê¶Œ/ê¸°ìˆ˜ë ¨ ë…¼ë¬¸ ìš”ì•½...",
                        'url': f"https://arxiv.org/abs/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                        'published_date': datetime.now().strftime('%Y-%m-%d'),
                        'source_type': 'arxiv_paper',
                        'keywords': [keyword, 'meditation', 'brain', 'taichi', 'qigong'],
                        'vector': generate_vector(keyword),
                        'vector_dimension': 384,
                        'processed_date': datetime.now().isoformat()
                    })
            time.sleep(1)
        except Exception as e:
            print(f"âŒ arXiv ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    return results

def collect_patents(keywords: List[str], max_results=3) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        for i in range(1):
            results.append({
                'title': f"{keyword} ê´€ë ¨ íŠ¹í—ˆ {i+1}",
                'summary': f"{keyword} ë¶„ì•¼ì˜ ëª…ìƒ/í˜¸í¡/ë‡Œê³¼í•™/íƒœê·¹ê¶Œ/ê¸°ìˆ˜ë ¨ íŠ¹í—ˆ ìš”ì•½...",
                'url': f"https://patents.google.com/patent/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                'published_date': datetime.now().strftime('%Y-%m-%d'),
                'source_type': 'patent',
                'keywords': [keyword, 'meditation', 'brain', 'taichi', 'qigong'],
                'vector': generate_vector(keyword),
                'vector_dimension': 384,
                'processed_date': datetime.now().isoformat()
            })
    return results

def upload_to_bigquery(data: List[Dict[str, Any]]):
    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)
    print(f"â˜ï¸ BigQuery ì—…ë¡œë“œ ì‹œì‘: {len(data)}ê°œ í•­ëª©")
    errors = client.insert_rows_json(table_ref, data)
    if errors:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì˜¤ë¥˜: {errors}")
    else:
        print(f"âœ… BigQuery ì—…ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ í•­ëª©")

def main():
    keywords = [
        "meditation", "mindfulness", "breathing exercise", "pranayama", "brain science", "neuroscience", "EEG meditation", "brainwave meditation", "cognitive enhancement meditation", "taichi", "tai chi", "qigong", "qi training", "energy cultivation", "ê¸°ìˆ˜ë ¨", "ëª…ìƒ", "í˜¸í¡ë²•", "ë‡Œê³¼í•™", "íƒœê·¹ê¶Œ", "ê¸°ê³µ", "ë‘ë‡Œí›ˆë ¨", "ì •ì‹ ì§‘ì¤‘", "ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™” ëª…ìƒ"
    ]
    all_data = []
    print("ğŸ“š ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_arxiv_papers(keywords))
    print("ğŸ“„ íŠ¹í—ˆ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_patents(keywords))
    if all_data:
        existing_titles, existing_urls = get_existing_titles_and_urls()
        unique_data = deduplicate(all_data, existing_titles, existing_urls)
        print(f"ğŸ§¹ ì¤‘ë³µ ì œê±° í›„ ì—…ë¡œë“œ ëŒ€ìƒ: {len(unique_data)}ê°œ")
        if unique_data:
            upload_to_bigquery(unique_data)
        else:
            print("âŒ ì¤‘ë³µ ë°ì´í„°ë§Œ ì¡´ì¬í•˜ì—¬ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 