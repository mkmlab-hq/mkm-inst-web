#!/usr/bin/env python3
"""
ì˜ë£Œ, ê²Œì„, ì˜¨ë¼ì¸ë§ˆì¼€íŒ… ë¶„ì•¼ì˜ ìµœì‹  AI ë…¼ë¬¸/íŠ¹í—ˆ/ì˜¤í”ˆì†ŒìŠ¤ ìë™ ìˆ˜ì§‘ ë° ë²¡í„°í™”
"""
import requests
import json
import time
import hashlib
import numpy as np
from datetime import datetime
from google.cloud import bigquery
from typing import List, Dict, Any

# BigQuery ì„¤ì •
PROJECT_ID = "persona-diary-service"
DATASET_ID = "intelligence"
TABLE_ID = "vectorized_papers"

client = bigquery.Client(project=PROJECT_ID)

def generate_vector(text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ (ê°„ë‹¨ ì‹œë®¬ë ˆì´ì…˜)"""
    hash_obj = hashlib.md5(text.encode())
    hash_hex = hash_obj.hexdigest()
    np.random.seed(int(hash_hex[:8], 16))
    return np.random.rand(384).tolist()

# 1. ë…¼ë¬¸/íŠ¹í—ˆ/ì˜¤í”ˆì†ŒìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ìƒ˜í”Œ: arXiv, GitHub, Google Patents)
def collect_arxiv_papers(keywords: List[str], max_results=10) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        url = f"http://export.arxiv.org/api/query?search_query=all:{keyword}&start=0&max_results={max_results}"
        try:
            resp = requests.get(url, timeout=20)
            if resp.status_code == 200 and 'entry' in resp.text:
                # ì‹¤ì œë¡œëŠ” XML íŒŒì‹± í•„ìš”, ì—¬ê¸°ì„  ìƒ˜í”Œ ë°ì´í„° ìƒì„±
                for i in range(2):
                    results.append({
                        'title': f"{keyword} ê´€ë ¨ arXiv ë…¼ë¬¸ {i+1}",
                        'summary': f"{keyword} ë¶„ì•¼ì˜ ìµœì‹  AI ë…¼ë¬¸ ìš”ì•½...",
                        'url': f"https://arxiv.org/abs/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                        'published_date': datetime.now().strftime('%Y-%m-%d'),
                        'source_type': 'arxiv_paper',
                        'keywords': [keyword, 'AI'],
                        'vector': generate_vector(keyword),
                        'vector_dimension': 384,
                        'processed_date': datetime.now().isoformat()
                    })
            time.sleep(1)
        except Exception as e:
            print(f"âŒ arXiv ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    return results

def collect_github_projects(keywords: List[str], max_results=5) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        # ì‹¤ì œë¡œëŠ” GitHub API ì‚¬ìš©, ì—¬ê¸°ì„  ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        for i in range(2):
            results.append({
                'title': f"{keyword} ê´€ë ¨ GitHub ì˜¤í”ˆì†ŒìŠ¤ {i+1}",
                'summary': f"{keyword} ë¶„ì•¼ì˜ AI ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ ì„¤ëª…...",
                'url': f"https://github.com/sample/{keyword}{i}",
                'published_date': datetime.now().strftime('%Y-%m-%d'),
                'source_type': 'github_project',
                'keywords': [keyword, 'AI'],
                'vector': generate_vector(keyword),
                'vector_dimension': 384,
                'processed_date': datetime.now().isoformat()
            })
    return results

def collect_patents(keywords: List[str], max_results=3) -> List[Dict[str, Any]]:
    results = []
    for keyword in keywords:
        # ì‹¤ì œë¡œëŠ” íŠ¹í—ˆ API ì‚¬ìš©, ì—¬ê¸°ì„  ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        for i in range(1):
            results.append({
                'title': f"{keyword} ê´€ë ¨ íŠ¹í—ˆ {i+1}",
                'summary': f"{keyword} ë¶„ì•¼ì˜ AI íŠ¹í—ˆ ìš”ì•½...",
                'url': f"https://patents.google.com/patent/{hashlib.md5((keyword+str(i)).encode()).hexdigest()[:8]}",
                'published_date': datetime.now().strftime('%Y-%m-%d'),
                'source_type': 'patent',
                'keywords': [keyword, 'AI'],
                'vector': generate_vector(keyword),
                'vector_dimension': 384,
                'processed_date': datetime.now().isoformat()
            })
    return results

def upload_to_bigquery(data: List[Dict[str, Any]]):
    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)
    # 'domain' í•„ë“œ ì œê±°
    for item in data:
        if 'domain' in item:
            del item['domain']
    print(f"â˜ï¸ BigQuery ì—…ë¡œë“œ ì‹œì‘: {len(data)}ê°œ í•­ëª©")
    errors = client.insert_rows_json(table_ref, data)
    if errors:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì˜¤ë¥˜: {errors}")
    else:
        print(f"âœ… BigQuery ì—…ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ í•­ëª©")

def main():
    # ë¶„ì•¼ë³„ í‚¤ì›Œë“œ
    healthcare_keywords = ["medical AI", "healthcare LLM", "AI diagnosis", "bioGPT", "digital health"]
    game_keywords = ["game AI", "procedural content generation", "AI NPC", "reinforcement learning game"]
    marketing_keywords = ["AI marketing", "AI ad", "AI content generation", "AI CRM", "marketing automation"]

    all_data = []
    print("ğŸ“š ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_arxiv_papers(healthcare_keywords + game_keywords + marketing_keywords))
    print("ğŸ’» ì˜¤í”ˆì†ŒìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_github_projects(healthcare_keywords + game_keywords + marketing_keywords))
    print("ğŸ“„ íŠ¹í—ˆ ìˆ˜ì§‘ ì¤‘...")
    all_data.extend(collect_patents(healthcare_keywords + game_keywords + marketing_keywords))

    if all_data:
        upload_to_bigquery(all_data)
    else:
        print("âŒ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 